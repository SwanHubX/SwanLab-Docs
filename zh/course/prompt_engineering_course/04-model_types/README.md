# 模型选择

提示词工程应用于开发和优化提示词（Prompt），提高大语言模型的处理复杂任务场景的能力，比如特定任务领域的问答或者算术推理的能力。而不同的模型由于能力不同以及适用领域的不同，构建提示词需要参考模型类型，从而提高模型处理复杂任务的能力，本节介绍了不同类型的大语言模型能力，包括Base模型、Instruct模型以及Reasoning模型，并补充每个类型的模型适用任务领域的差异，帮助用户了解构建提示词的时候需要使用哪种类型的模型并提供指导建议。


## Base模型

大模型通常包含预训练、微调、强化学习等训练阶段，其中Base模型是预训练后，没有经过特定领域微调的大语言模型。它通过海量通用数据，比如网页文本、书籍、论文、百科知识、代码等进行无监督或者半监督训练，学会了大量语言结构、逻辑规则以及世界知识，具备基础的语言理解和生成能力。

Base模型的核心特点在于其 “通用性” 与 “可塑性”。一方面，它能处理多种自然语言任务，如文本生成、问答、翻译；另一方面，由于未被特定任务 “固化”，可灵活适配不同领域需求。其出色的上下文理解能力，能精准捕捉文本中的语义关联与逻辑线索，即使面对长段落或复杂语境，也能输出连贯、合理的内容。同时，base 模型具备较强的泛化能力，可基于训练中积累的知识，快速适应新场景或任务，降低 “过拟合” 风险。

## Instruct模型

Instruct 模型，即指令微调模型（Instruction-Tuned Model），是在基础预训练模型（Base Model）之上，通过特定优化技术使其更擅长理解和执行人类指令的大型语言模型。其核心在于**指令微调（Instruction Tuning** 这一关键步骤：使用包含 “指令 - 响应” 对的数据集对 Base 模型进行有监督微调，让模型学习如何根据明确的用户指令生成高质量回复。例如，在训练数据中会有类似这样的样本：

```Plain
指令：“用简洁的语言解释区块链技术。”
目标响应：“区块链是一种去中心化的分布式账本技术……”
```

这种训练方式使 Instruct 模型具备了更强的任务泛化能力，能够将在训练中学习到的指令理解模式应用到未见过的指令类型上，而非仅仅依赖预训练阶段学到的语言模式。

相比于Base 模型主要擅长文本续写等无明确目标的生成任务，输出容易偏离用户需求，以重复文本、其他特殊字符等形式生成回答，Instruct模型会严格聚焦于遵循用户指令回答问题；同时通过指令微调数据集中的高质量示例，Instruct 模型学会了在不确定时保持谨慎，减少生成无根据信息的概率。例如，在回答历史问题时，Base 模型可能编造不存在的事件细节，而 Instruct 模型更倾向于承认不确定性或提供查证建议。Instruct 模型经过优化，回复更符合人类对话习惯，包括适当的礼貌表达、分步解释复杂问题等。

简单来说，Instruct模型是基于Base基础上将指令部分的新特征添加到模型的认知中，通用问答领域内，不需要Base模型添加过多的约束，就能够回答大多数问题，也就是**擅长直接执行指令**，而经过专有领域数据集微调后的模型甚至可以直接询问，比如医疗领域微调模型。但是Instruct模型通常对训练数据的质量要求较高，即便少量的数据也可以达到很好的微调效果，但往往成本要求较高；同时Base 模型的输出更加自由，对于一些开放式的文本生成任务，如创意写作、故事续写等，可能更能发挥其优势，因为它不会受到指令的过多限制，能够产生更具多样性的内容。

Instruct 模型经指令微调，擅长理解并执行明确指令，适用于问答、邮件生成、翻译等指令驱动任务，以及智能客服、教育辅助等需贴合用户意图的场景，能精准输出实用结果。Base 模型是未微调的基础模型，语言通用性强、生成灵活，适用于二次开发（如领域微调底座）、开放式创作（如小说续写）及语言规律研究等场景，是创新和定制化的起点，二者分工互补，共同支撑大模型应用生态。

基于上述优势，我们可以用Instruct做下面的一些任务：

1. 指令问答
2. 情感分类
3. 聊天对话
4. 代码生成
5. 数学计算
6. 微调模型评估

举这些例子并不代表模型只能做这些任务，只不过这些任务具有代表性，可以让用户更好的掌握提示词的用法。


## Reasoning模型

在前文我们讲述，大模型通常包含预训练、微调等阶段，当然，所谓的强化学习更多指的是人类偏好对齐，并不是推理型模型的训练原理，除了这些训练方式，我们还见证了诸如 RAG（生成增强检索）和代码助手等专用应用的兴起。在2025年，大模型的发展更趋近于注重针对特定领域和应用的优化，推理型模型的开发正是这些专业化的集大成者。

🤔那么什么是Reasoning（推理型）模型呢？

如果你从事人工智能或机器学习工作，可能会对那些模糊且争议不断的定义有所了解。「推理型模型」这一术语也不例外。最终，某人会在论文中正式定义它，但很快就会在下一篇文章中重新定义，如此循环。

在本文中，我们将`推理`定义为回答那些**需要复杂、多步骤生成并包含中间步骤的问题的过程**。例如，像`法国的首都是哪里？`这样的事实性问题不涉及推理。相比之下，像`如果一列火车以 60 英里每小时的速度行驶，行驶 3 小时，它能走多远？`这样的问题则需要一些简单的推理。比如，它需要认识到距离、速度和时间之间的关系，然后得出答案。

在我们定义了推理型模型后，接下来可以进入更有趣的部分：如何构建和改进大语言模型以应对推理任务。然而，在深入技术细节之前，重要的是要考虑在什么情况下实际上需要推理型模型。

🧐 我们什么时候需要推理型模型？

推理型模型的设计目的是擅长解决一些复杂任务，如解谜、进阶数学问题和具有挑战性的编程任务。然而，对于像总结、翻译或基于知识的问题回答这些简单任务，它们并不是必需的。实际上，使用推理型模型处理所有任务可能会导致低效且成本较高。例如，推理型模型通常使用成本较高，生成的回答更冗长，并且有时由于“过度思考”而更容易出错。在这里，有一个简单的原则：根据任务选择合适的工具（或大语言模型类型）。

事实上，在Instruct模型对应的数学计算示例中，我们举了一个思维链的例子，其实更应该归于`推理`一类，因为它其实是在鼓励模型生成中间推理步骤，不直接跳到最终答案，这类方法我们称为**测试时计算扩展[1]**。

推理时扩展并不需要模型经过训练，但是如果要实现推理效果，直接对模型进行训练也是一种方法，往往有多种方法，比如RL（强化学习）、RL+SFT（强化学习+微调）、蒸馏等等，其中蒸馏是利用微调技术将知识从教师模型中学习到，而教师模型通常是具有高质量推理过程的模型。

总而言之，Reasoning模型更多实现的是更具有挑战性、推理思考的任务，模型采用Qwen3系列的模型，因为它本身就具备思考过程。

[1].[Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)