# Qwen2.5-VLæ¨¡å‹ç›®æ ‡æ£€æµ‹ï¼ˆGroundingï¼‰ä»»åŠ¡é¢†åŸŸå¾®è°ƒæ•™ç¨‹

## ğŸ“ç®€ä»‹

â€‹	åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸï¼Œä¼—å¤šç¥ç»ç½‘ç»œæ¨¡å‹æ—©å·²å‡­å€Ÿå…¶å“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†ç²¾å‡†çš„ç›®æ ‡æ£€æµ‹ä¸ç›®æ ‡åˆ†å‰²æ•ˆæœã€‚ç„¶è€Œï¼Œéšç€å¤šæ¨¡æ€æ¨¡å‹çš„å´›èµ·ï¼Œå…¶åœ¨å›¾åƒåˆ†ææ–¹é¢å±•ç°å‡ºçš„éå‡¡èƒ½åŠ›ï¼Œä¸ºè¯¥é¢†åŸŸå¸¦æ¥äº†æ–°çš„æœºé‡ã€‚å¤šæ¨¡æ€æ¨¡å‹ä¸ä»…èƒ½å¤Ÿæ·±å…¥ç†è§£å›¾åƒå†…å®¹ï¼Œè¿˜èƒ½å°†è¿™ç§ç†è§£è½¬åŒ–ä¸ºæ–‡æœ¬å½¢å¼è¾“å‡ºï¼Œæå¤§åœ°æ‹“å±•äº†å…¶åº”ç”¨åœºæ™¯ã€‚é‰´äºæ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ‰“é€ ä¸€ä»½è¯¦å°½çš„æ•™ç¨‹ï¼ŒæŒ‡å¯¼è¯»è€…å¦‚ä½•é€šè¿‡å¯¹ä¸»æµå¤šæ¨¡æ€å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ¥å®ç°ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚ä»¥Qwen2.5-VLä¸ºä¾‹ï¼Œå‡­å€Ÿå…¶å¼ºå¤§çš„å¤šæ¨¡æ€åˆ†æèƒ½åŠ›ï¼Œæ— éœ€ä»å¤´å¼€å§‹ï¼Œåˆ©ç”¨å¤§é‡æ•°æ®è¿›è¡Œé¢„è®­ç»ƒæ¥æ„å»ºæ–°æ¨¡å‹ï¼Œä»…é€šè¿‡å¾®è°ƒå³å¯é«˜æ•ˆåœ°å®ç°ç›®æ ‡æ£€æµ‹åŠŸèƒ½ï¼Œä¸ºè¯¥é¢†åŸŸçš„å‘å±•æä¾›ä¸€ç§å…¨æ–°çš„æ€è·¯ä¸æ–¹æ³•ã€‚



## ğŸ“šé“¾æ¥èµ„æ–™

ä½œè€…ä¿¡æ¯ï¼šæƒ…æ„Ÿæœºå™¨å®éªŒå®¤ç ”ç©¶å‘˜-æé¦¨é›¨ é‚®ç®±ï¼šwind.340171@gmail.com

æ¨¡å‹åœ°å€ï¼šQwen2.5-VL-3B-Instructï¼š[huggingface](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)|[é­”æ­ç¤¾åŒºï¼ˆä¸‹é¢çš„æ¨¡å‹ä¸‹è½½ä½¿ç”¨ï¼Œè€Œä¸”æ¯”è¾ƒæ–¹ä¾¿ï¼‰](https://www.modelscope.cn/models/Qwen/Qwen2.5-VL-3B-Instruct)

æ•°æ®é›†åœ°å€ï¼šTextVQA_GroundingTask_bboxï¼š[huggingface](https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox)|[é­”æ­ç¤¾åŒº](https://www.modelscope.cn/datasets/Tina12345/textVQA_groundingtask_bbox)

ä»£ç åœ°å€ï¼š[github](https://github.com/828Tina/textvqa_grounding_task_qwen2.5-vl-ft)

å¯è§†åŒ–å·¥å…·SwanLabé¡¹ç›®åœ°å€ï¼š[SwanLabè®­ç»ƒæŒ‡æ ‡è§‚æµ‹ç»“æœæ›²çº¿å›¾](https://swanlab.cn/@LiXinYu/qwen2.5-vl-sft-grounding/overview)

> å‹æƒ…é“¾æ¥ï¼š
>
> SwanLabå®˜æ–¹æ–‡æ¡£ï¼ŒåŠ©ä½ è½»æ¾å¼€å¯æ·±åº¦å­¦ä¹ ä¹‹æ—…ã€‚
>
> 1. [æ¡†æ¶é›†æˆæ–‡æ¡£](https://docs.swanlab.cn/guide_cloud/integration/)ï¼šSwanLabå·²ç»é›†æˆTransformersã€LLaMA Factoryã€Pytorchç­‰ä¸»æµæ¡†æ¶ï¼Œå¹¶æŒç»­æ›´æ–°
> 2. [å®æˆ˜æ¡ˆä¾‹](https://docs.swanlab.cn/examples/hello_world.html)ï¼šSwanLabæä¾›äº†ä¸°å¯Œçš„æ¨¡å‹è®­ç»ƒå®æˆ˜æ•™ç¨‹ï¼ŒåŠ©åŠ›ç”¨æˆ·å¿«é€ŸæŒæ¡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„è¦ç‚¹

<img src="./qwen2_5-vl-report/swanlab-content.png" alt="SwanLabç²¾é€‰å†…å®¹" style="zoom:50%">



## ğŸ’»è®­ç»ƒä»»åŠ¡è®¾ç½®

### 1ã€è®­ç»ƒæ–¹æ³•ç®€ä»‹

<img src="./qwen2_5-vl-report/llm-ft.png" alt="LLMå¾®è°ƒæµç¨‹å›¾" style="zoom:50%">

1. **éƒ¨åˆ†å‚æ•°å¾®è°ƒ**ï¼šéƒ¨åˆ†å‚æ•°å¾®è°ƒæ˜¯ä¸€ç§åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œé’ˆå¯¹æ€§è°ƒæ•´çš„ç­–ç•¥ã€‚å®ƒä»…å¯¹æ¨¡å‹çš„ä¸€éƒ¨åˆ†å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œè€Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯

   - è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¯¹æ•´ä¸ªæ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä½¿å¾—éƒ¨åˆ†å‚æ•°å¾®è°ƒåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æ›´åŠ å¯è¡Œï¼Œä¾‹å¦‚åœ¨å•ä¸ªGPUä¸Šæˆ–åœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­ã€‚

   - å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå› ä¸ºå®ƒé™åˆ¶äº†æ¨¡å‹çš„è°ƒæ•´èŒƒå›´ï¼Œé¿å…äº†å¯¹è®­ç»ƒæ•°æ®çš„è¿‡åº¦æ‹Ÿåˆã€‚
   - `ç¼ºç‚¹`æ˜¯å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å…¨éƒ¨æ½œåŠ›ï¼Œå› ä¸ºåªæœ‰éƒ¨åˆ†å‚æ•°å¾—åˆ°äº†ä¼˜åŒ–ã€‚è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æŸäº›å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸å¦‚å…¨å‚æ•°å¾®è°ƒã€‚

2. **å…¨å‚æ•°å¾®è°ƒ**ï¼šå…¨å‚æ•°å¾®è°ƒæ˜¯ä¸€ç§ç›´æ¥ä¸”ç›´è§‚çš„æ–¹æ³•ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹æ‰€æœ‰å‚æ•°è¿›è¡Œæ›´æ–°ã€‚

   - è¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»è€Œåœ¨è®¸å¤šä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚
   - `ç¼ºç‚¹`æ˜¯è®¡ç®—æˆæœ¬é«˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å‚æ•°é‡å·¨å¤§çš„æƒ…å†µä¸‹ã€‚å…¨å‚æ•°å¾®è°ƒéœ€è¦å¤§é‡çš„GPUå†…å­˜å’Œè®¡ç®—èµ„æºï¼Œè¿™åœ¨å¤šæ¨¡å‹éƒ¨ç½²å’Œå®æ—¶åº”ç”¨ä¸­å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚



### 2ã€é€‰ç”¨æ¨¡å‹ç®€ä»‹

- Qwen2.5-vlæŠ€æœ¯æŠ¥å‘Šè®ºæ–‡åœ°å€ï¼š[[Qwen2.5-VL Technical Report](https://arxiv.org/pdf/2502.13923)]
- ä»£ç åœ°å€ï¼š[[Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)]

<img src="./qwen2_5-vl-report/qwen2_5-vl-object.jpg" alt="Qwen2.5-vlæ¨¡å‹æ•´ä½“ç»“æ„å›¾" style="zoom:50%">

â€‹	å¤šæ¨¡æ€æ¨¡å‹ä¸»è¦ç”±**è§†è§‰ç¼–ç å™¨ï¼ˆVision Encoderï¼‰ã€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å’Œå¤šæ¨¡æ€èåˆæ¨¡å—ï¼ˆConnectorï¼‰**ä¸‰å—æ„æˆï¼Œå’ŒQwen2-VLä¸€æ ·ï¼ŒQwen2.5-VLå¹¶æ²¡æœ‰å·¨å¤§çš„Connectorï¼Œä»…ç”¨ä¸€ä¸ªMLPå®Œæˆç‰¹å¾æŠ•å½±ã€‚æ‰“å°æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š

```python
### ä»£ç è¡¨ç¤º
MODEL_PATH = '/data/nvme1/weights/Qwen2_5-VL-3B-Instruct'

from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration
from qwen_vl_utils import process_vision_info

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MODEL_PATH, torch_dtype="auto", device_map="auto"
)
print(model)
```

ç»“æœå¦‚ä¸‹ï¼š

```python
Qwen2_5_VLForConditionalGeneration(
  (visual): Qwen2_5_VisionTransformerPretrainedModel(
    (patch_embed): Qwen2_5_VisionPatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2_5_VLVisionBlock(
        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)
        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)
        (attn): Qwen2_5_VLVisionSdpaAttention(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): Qwen2_5_VLMLP(
          (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)
          (up_proj): Linear(in_features=1280, out_features=3420, bias=True)
          (down_proj): Linear(in_features=3420, out_features=1280, bias=True)
          (act_fn): SiLU()
        )
      )
    )
    (merger): Qwen2_5_VLPatchMerger(
      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=2048, bias=True)
      )
    )
  )
  (model): Qwen2_5_VLModel(
    (embed_tokens): Embedding(151936, 2048)
    (layers): ModuleList(
      (0-35): 36 x Qwen2_5_VLDecoderLayer(
        (self_attn): Qwen2_5_VLSdpaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=256, bias=True)
          (v_proj): Linear(in_features=2048, out_features=256, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): Qwen2_5_VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)
          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((2048,), eps=1e-06)
    (rotary_emb): Qwen2_5_VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
)
```

â€‹	Qwen2.5-VL-3B-Instruct åŸºäº Qwen2.5 æ¶æ„ï¼Œå…¶å‚æ•°é‡è¾¾åˆ° 30 äº¿çº§åˆ«ï¼Œä¸“ä¸ºæŒ‡ä»¤å¾®è°ƒè€Œè®¾è®¡ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡æµ·é‡æ–‡æœ¬å’Œå›¾åƒæ•°æ®å­¦ä¹ é€šç”¨çš„è¯­è¨€å’Œè§†è§‰çŸ¥è¯†ï¼Œèƒ½å¤Ÿç†è§£å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼ŒåŒæ—¶å¤„ç†ä¸æ–‡æœ¬ç›¸å…³çš„å›¾åƒä¿¡æ¯ï¼Œå®ç°å¤šæ¨¡æ€äº¤äº’ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒQwen2.5-VL-3B-Instruct é’ˆå¯¹ç‰¹å®šçš„æŒ‡ä»¤ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œäººç±»çš„æŒ‡ä»¤ï¼Œå¦‚é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆã€å›¾åƒæè¿°ç­‰ã€‚å®ƒåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå±•ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå°†å›¾åƒå†…å®¹ä¸æ–‡æœ¬è¯­ä¹‰ç›¸ç»“åˆï¼Œç”Ÿæˆå‡†ç¡®ä¸”å¯Œæœ‰é€»è¾‘çš„å›ç­”ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å…·å¤‡ä¸€å®šçš„æ¨ç†èƒ½åŠ›å’Œåˆ›é€ åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æä¾›æœ‰ä»·å€¼çš„è§è§£å’Œè§£å†³æ–¹æ¡ˆã€‚

ä¸‹è½½ä»£ç ï¼š

```bash
modelscope download --model Qwen/Qwen2.5-VL-3B-Instruct  --local_dir /data/nvme1/weights/Qwen/Qwen2.5-VL-3B-Instruct
```



### 3ã€æ•°æ®é›†ç®€ä»‹

â€‹	TextVQA_GT_bbox æ˜¯ Hugging Face ä¸Šçš„ä¸€ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œä¸“æ³¨äºæ–‡æœ¬ç›¸å…³çš„è§†è§‰é—®ç­”ä»»åŠ¡ï¼Œæ¥æºäº [TextVQA](https://textvqa.org/dataset/) ï¼Œå¹¶`æä¾›ç›®æ ‡è¾¹ç•Œæ¡†`ä¿¡æ¯ã€‚è¯¥æ•°æ®é›†åŒ…å«å›¾åƒã€ä¸å›¾åƒç›¸å…³çš„é—®é¢˜ä»¥åŠå¯¹åº”çš„ç­”æ¡ˆï¼Œè¾¹ç•Œæ¡†ä¿¡æ¯å¸®åŠ©æ¨¡å‹ç²¾å‡†å®šä½å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œä»è€Œæé«˜å›ç­”é—®é¢˜çš„å‡†ç¡®æ€§ã€‚è¯¥æ•°æ®é›†é€‰æ‹©[TextVQA](https://textvqa.org/dataset/) ä¸­å•ç›®æ ‡æ£€æµ‹çš„é—®ç­”ï¼Œä¿ç•™5000ä¸ªæ ·æœ¬ä¸­çš„4370ä¸ªã€‚

â€‹	æœ¬æ¬¡æ•™ç¨‹çš„ä»»åŠ¡ç›®æ ‡æ˜¯åˆ©ç”¨é—®é¢˜å’Œç›®æ ‡è¾¹ç•Œæ¡†ä¿¡æ¯æ¥å¯¹Qwen2.5-VL-3B-Instructæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ•°æ®é›†æ ·å¼å¦‚ä¸‹ï¼š

<img src="./qwen2_5-vl-report/dataset_example.png" alt="TextVQA(åŒ…å«è¾¹æ¡†ä¿¡æ¯)æ•°æ®é›†æ ·ä¾‹" style="zoom:50%">

â€‹	è®ºæ–‡ã€Š[MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs](https://arxiv.org/pdf/2502.17422)ã€‹ä¸­ä½¿ç”¨è¯¥æ•°æ®é›†ç”¨äºç ”ç©¶MLLMçš„æ³¨æ„åŠ›æ¨¡å¼ã€‚

ä¸‹è½½ä»£ç ï¼š

```bash
modelscope download --dataset Tina12345/textVQA_groundingtask_bbox  --local_dir /data/nvme0/textvqa_bbox
```



### 4ã€è®­ç»ƒæ¡†æ¶é€‰æ‹©

<img src="./qwen2_5-vl-report/transformers.png" alt="HF Transformerså·¥å…·åŒ…logo" style="zoom:50%">

â€‹	**Hugging Face Transformers** æ˜¯ä¸€ä¸ªåŸºäº Python çš„å¼€æºåº“ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶æä¾›äº†å¤§é‡é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ BERTã€GPTã€T5ã€RoBERTaã€DistilBERT ç­‰ï¼‰ï¼Œå¹¶æ”¯æŒä½¿ç”¨ PyTorch å’Œ TensorFlow ä¸¤ç§ä¸»æµæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œæ¨¡å‹çš„å¾®è°ƒä¸éƒ¨ç½²ã€‚

â€‹	Transformers åº“çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶ç»Ÿä¸€ä¸”ç®€æ´çš„æ¥å£è®¾è®¡ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿå®ç°æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰å¤šç§ NLP ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®ƒé›†æˆäº† Hugging Face Model Hub ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ•°ä¸‡ä¸ªç¤¾åŒºè´¡çŒ®æ¨¡å‹çš„å¹³å°ï¼Œç”¨æˆ·å¯ç›´æ¥åŠ è½½å·²æœ‰æ¨¡å‹æˆ–ä¸Šä¼ è‡ªå®šä¹‰æ¨¡å‹ï¼Œä¾¿äºæ¨¡å‹å…±äº«ä¸å¤ç”¨ã€‚

â€‹	åœ¨æ€§èƒ½æ–¹é¢ï¼ŒTransformers æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€åˆ†å¸ƒå¼è®­ç»ƒä»¥åŠ ONNX å¯¼å‡ºç­‰åŠŸèƒ½ï¼Œé€‚ç”¨äºä»ç ”ç©¶åŸå‹åˆ°å·¥ä¸šçº§éƒ¨ç½²çš„å…¨æµç¨‹å¼€å‘ã€‚ç»“åˆ Datasetsã€Tokenizersã€Accelerate ç­‰é…å¥—åº“ï¼ŒHugging Face æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ NLP å¼€å‘ç”Ÿæ€ç³»ç»Ÿï¼Œæå¤§æå‡äº†æ¨¡å‹å¼€å‘ä¸å®éªŒè¿­ä»£çš„æ•ˆç‡ã€‚

å‚è€ƒææ–™ï¼šhttps://huggingface.co/docs/transformers/index



## ğŸ“œ æ•°æ®é›†å‡†å¤‡

é¦–å…ˆï¼Œè¯¥æ•°æ®é›†å¯ä»¥ä»huggingfaceä¸Šç›´æ¥ä¸‹è½½ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("jrzhang/TextVQA_GT_bbox")
```

å¦‚æœhuggingfaceæ— æ³•ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©é­”æ­ç¤¾åŒºï¼Œå°†æ•°æ®é›†ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å‘½ä»¤è¡Œä¸‹è½½ï¼š

ä»£ç ä¸‹è½½ï¼š

```python
from modelscope.msdatasets import MsDataset
ds =  MsDataset.load('Tina12345/textVQA_groundingtask_bbox', subset_name='default', split='train', cache_dir="./data")
```

å‘½ä»¤è¡Œä¸‹è½½ï¼š

```bash
modelscope download --dataset Tina12345/textVQA_groundingtask_bbox  --local_dir /data/nvme0/textvqa_bbox
```

> **âš ï¸æ³¨æ„ï¼š**
>
> ä½¿ç”¨é­”æ­ç¤¾åŒºä¸‹è½½æ•°æ®é›†ï¼Œä¼šå‡ºç°F&Açš„1ã€é­”æ­ç¤¾åŒºä¸‹è½½çš„æ•°æ®é›†ç”¨ä¸äº†çš„é—®é¢˜ï¼Œè§£ç­”åœ¨è¿™é‡ŒğŸ‘‰1ã€é­”æ­ç¤¾åŒºä¸‹è½½çš„æ•°æ®é›†ç”¨ä¸äº†

ä¸‹è½½å¥½åï¼ŒæŠŠæ•°æ®é›†ç¨åŠ æ”¹é€ ï¼Œä¿å­˜æˆjsonlæ ¼å¼çš„æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­è®­ç»ƒã€‚

åŸæ•°æ®é›†æ ¼å¼ä¸ºï¼š

```json
{
    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x681 at 0x7FA58E1DB340>, 
    'question': 'what is the name of the company on the card?', 
    'answer': ['blink', 'intergrative nutrition', 'blink', 'blink', 'blink', 'blink', 'blink', 'blink', 'blink', 'blink'], 
    'dataset_id': '36269', 
    'bbox': [712.0, 255.0, 64.0, 43.0]
}
```

æˆ‘ä»¬åªéœ€è¦å…¶ä¸­çš„imageã€questionã€bboxéƒ¨åˆ†ï¼Œå¯ä»¥å°†è¿™ä¸‰éƒ¨åˆ†ä¿å­˜ï¼Œå…¶ä¸­questionä»£è¡¨useréƒ¨åˆ†çš„æé—®ï¼Œbboxä»£è¡¨çš„æ˜¯assistantéƒ¨åˆ†çš„å›ç­”ï¼Œæˆ‘å‚è€ƒäº†[swiftçš„æ•°æ®æ ¼å¼:query-responseæ ¼å¼](https://github.com/modelscope/ms-swift/blob/main/docs/source/Customization/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86.md)ã€‚

<img src="./qwen2_5-vl-report/swift-data-format.png" alt="swiftæ•°æ®é›†æ ¼å¼" style="zoom:50%">

åŸæ•°æ®é›†çš„bboxä¸º[x1,y1,w,h]çš„æ ¼å¼ï¼Œè®­ç»ƒä¿å­˜çš„bboxä¿®æ”¹æˆ[x1,y1,x2,y2]çš„æ ¼å¼ï¼Œæœ€ç»ˆä¿å­˜æˆä»¥ä¸‹æ ¼å¼ï¼š

```json
{"image": ["./data/test/003001.jpg"], "query": "what is written on the ghost?", "response": "{\"bbox_2d\": [460, 635, 513, 669]}"}
```

å…¶ä¸­éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œqwenå¯¹äºgroundingçš„è®­ç»ƒä»»åŠ¡æœ‰ç›¸åº”çš„æ¨¡æ¿ï¼Œé“¾æ¥åœ¨è¿™ğŸ‘‰[Qwen2.5-vl-finetune](https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-finetune/README.md)ï¼Œå› æ­¤ä¸Šè¿°çš„"{\"bbox_2d\": [460, 635, 513, 669]}"å…¶å®æ˜¯å‚è€ƒäº†å®˜æ–¹çš„**Grounding Example**ï¼Œ

![](./qwen2_5-vl-report/data-example.png)

æ•°æ®é›†è½¬åŒ–ä»£ç ä¿å­˜åˆ°scripts/convert2sft_format.pyä¸­ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
"""
å°†æ•°æ®é›†æ ¼å¼è½¬æ¢æˆå¤šæ¨¡æ€æ¨¡å‹ç›‘ç£å¾®è°ƒæ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼Œä¿å­˜æ–‡ä»¶æ ¼å¼ä¸ºjsonlæ ¼å¼ï¼š
{
    "image": "demo/COCO_train2014_000000580957.jpg",
    "conversations": [
        {
            "from": "human",
            "value": "<image>\nLocate house in this image and output the bbox coordinates in JSON format."
        },
        {
            "from": "gpt",
            "value": "{\n"bbox_2d": [135, 114, 1016, 672]\n}"
        }
    ]
}
è¯¥æ ¼å¼æ˜¯å‚è€ƒqwen2.5-vl-finetuneæ–‡ä»¶ä¸­æåˆ°çš„Grounding Exampleæ‰€ç¤ºã€‚

åŸæ•°æ®é›†æ ¼å¼ä¸ºï¼š
{
    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x681 at 0x7FA58E1DB340>, 
    'question': 'what is the name of the company on the card?', 
    'answer': ['blink', 'intergrative nutrition', 'blink', 'blink', 'blink', 'blink', 'blink', 'blink', 'blink', 'blink'], 
    'dataset_id': '36269', 
    'bbox': [712.0, 255.0, 64.0, 43.0]
}

"""


import json
import os
from tqdm import tqdm
from datasets import load_dataset
import math

"""
Qwen2.5-VLä½¿ç”¨ç»å¯¹åè°ƒä¸ºè°ƒæ•´å¤§å°çš„å›¾åƒã€‚å¯¹äºåŸå§‹ç»å¯¹åæ ‡ï¼Œåº”è¯¥ä¹˜ä»¥è°ƒæ•´å¤§å°çš„é«˜åº¦å’Œå®½åº¦ï¼Œç„¶åé™¤ä»¥å…¶åŸå§‹é«˜åº¦å’Œå®½åº¦ã€‚
å…·ä½“ä»£ç å®˜ç½‘ç»™äº†ï¼Œé“¾æ¥ï¼šhttps://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-finetune/tools/process_bbox.ipynb
å¯ä»¥å‚è€ƒå®˜æ–¹çš„é“¾æ¥
"""

# This is the resize function of Qwen2.5-VL
def smart_resize(
    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280
):
    """Rescales the image so that the following conditions are met:
    1. Both dimensions (height and width) are divisible by 'factor'.
    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].
    3. The aspect ratio of the image is maintained as closely as possible.
    """
    if height < factor or width < factor:
        raise ValueError(f"height:{height} or width:{width} must be larger than factor:{factor}")
    elif max(height, width) / min(height, width) > 200:
        raise ValueError(
            f"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}"
        )
    h_bar = round(height / factor) * factor
    w_bar = round(width / factor) * factor
    if h_bar * w_bar > max_pixels:
        beta = math.sqrt((height * width) / max_pixels)
        h_bar = math.floor(height / beta / factor) * factor
        w_bar = math.floor(width / beta / factor) * factor
    elif h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (height * width))
        h_bar = math.ceil(height * beta / factor) * factor
        w_bar = math.ceil(width * beta / factor) * factor
    return h_bar, w_bar


def convert_to_qwen25vl_format(bbox, orig_height, orig_width, factor=28, min_pixels=56*56, max_pixels=14*14*4*1280):
    new_height, new_width = smart_resize(orig_height, orig_width, factor, min_pixels, max_pixels)
    scale_w = new_width / orig_width
    scale_h = new_height / orig_height
    
    x1, y1, x2, y2 = bbox
    x1_new = round(x1 * scale_w)
    y1_new = round(y1 * scale_h)
    x2_new = round(x2 * scale_w)
    y2_new = round(y2 * scale_h)
    
    x1_new = max(0, min(x1_new, new_width - 1))
    y1_new = max(0, min(y1_new, new_height - 1))
    x2_new = max(0, min(x2_new, new_width - 1))
    y2_new = max(0, min(y2_new, new_height - 1))
    
    return [x1_new, y1_new, x2_new, y2_new]


def convert_to_sft_format(data_path,save_path,type='train'):
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(data_path,split='train')

    # æ¯ä¸ªæ•°æ®ä¿å­˜åˆ°ä¸€ä¸ªjsonlæ–‡ä»¶ä¸­ï¼Œå¹¶ä¸”å›¾ç‰‡çš„è¯è¦å¦å¤–æ”¾åˆ°ä¸€èµ·
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    # åˆ›å»º JSONL æ–‡ä»¶
    jsonl_file = os.path.join(save_path, f"{type}.jsonl")
    with open(jsonl_file, 'w', encoding='utf-8') as jsonl_out:
        # éå†æ•°æ®é›†å¹¶ä¿å­˜å›¾ç‰‡ï¼Œå…¶ä»–çš„éƒ¨åˆ†ä¿¡æ¯ä¿å­˜æˆjsonlæ–‡ä»¶
        for idx,sample in tqdm(enumerate(dataset),total=len(dataset)):
            if type == 'train':
                if idx >= 3000:  # åˆ¤æ–­æ˜¯å¦å¤„ç†åˆ°3000æ¡æ•°æ®
                    break
            elif type == 'test':
                # åˆ¤æ–­æ˜¯å¦å¤„ç†åˆ°3001åˆ°3100æ¡æ•°æ®
                if idx < 3000 or idx >= 3100:
                    continue
            # ä¿å­˜å›¾ç‰‡
            image = sample['image']
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆæ ¼å¼ä¸º 000001.jpg, 000002.jpg ç­‰ï¼‰
            filename = f"{idx + 1:06d}.jpg"  # ä½¿ç”¨ 6 ä½æ•°å­—æ ¼å¼åŒ–æ–‡ä»¶å
            jpg_path = os.path.join(save_path, type)
            if not os.path.exists(jpg_path):
                os.makedirs(jpg_path)
            output_path = os.path.join(jpg_path, filename)
            # ä¿å­˜å›¾ç‰‡
            image.save(output_path)

            # ä¿å­˜å…¶ä»–ä¿¡æ¯
            # åæ ‡ä¿¡æ¯
            old_bbox = sample['bbox']
            #### è¿™é‡Œéœ€è¦å°†åæ ‡è½¬æ¢æˆQwen2.5-VLçš„åæ ‡æ ¼å¼   
            image_width, image_height = image.size
            x1, y1, w, h = old_bbox
            new_bboxes = [x1, y1, x1 + w, y1 + h]
            # è½¬æ¢åæ ‡
            qwen25_bboxes = convert_to_qwen25vl_format(new_bboxes, image_height, image_width)
            bbox_dict = {"bbox_2d": qwen25_bboxes}
            formatted_json = json.dumps(bbox_dict, indent=None)
            data = {
                "image":[output_path],
                "query":sample['question'],
                "response":formatted_json,
            }

            # å°†æ•°æ®å†™å…¥ JSONL æ–‡ä»¶
            # å°†æ¯æ¡æ•°æ®å†™å…¥ JSONL æ–‡ä»¶
            jsonl_out.write(json.dumps(data, ensure_ascii=False) + '\n')

    print(f"All images and data have been saved to {save_path} and {jsonl_file}")

# ç¤ºä¾‹è°ƒç”¨
convert_to_sft_format(data_path='/home/lixinyu/data/textvqa_bbox', save_path='./data', type='test')
```

å…¶ä¸­å›¾åƒä¿å­˜åˆ°data/trainä¸­ï¼Œtrain.jsonlä¿å­˜åˆ°dataæ–‡ä»¶å¤¹ä¸­ï¼ŒåŒæ—¶è¿˜æœ‰æµ‹è¯•é›†æ•°æ®ä¹ŸåŒæ ·ä¿å­˜åˆ°testä¸­ï¼Œåªæ˜¯åœ¨è¾“å…¥çš„typeæ ¹æ®éœ€è¦ä¿®æ”¹æˆâ€œtrainâ€æˆ–è€…â€œtestâ€å³å¯ã€‚

> ğŸš¨ğŸš¨ğŸš¨ğŸš¨**æ³¨æ„**ğŸš¨ğŸš¨ğŸš¨ğŸš¨
>
> ---
>
> Qwenå®˜æ–¹æä¾›äº†ä¸€æ®µä»£ç ğŸ‘‰[Qwen2.5VLè½¬æ¢bboxä»£ç ](https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-finetune/tools/process_bbox.ipynb)ï¼Œå¾ˆå®¹æ˜“è¢«å¿½ç•¥ï¼ˆæˆ‘ä¹‹å‰å°±å¿½ç•¥äº†ğŸ˜³ï¼Œè¿˜å¥½æœ‰å¥½å¿ƒäººæé†’ï¼‰ï¼Œé‚£å°±æ˜¯Qwen2.5VLåŸç‰ˆä»£ç è½¬æ¢bboxçš„è¿‡ç¨‹ï¼Œå› ä¸ºQwen2.5Vlè¾“å…¥çš„å›¾åƒè¿›å…¥VisionMLPä¼šè‡ªåŠ¨rescaleï¼Œå¦‚æœä¸æŒ‰ç…§å®˜æ–¹çš„å¯¹åæ ‡è¿›è¡Œä¿®æ”¹çš„è¯ï¼Œæ¨ç†çš„ç»“æœæ˜¯é”™è¯¯çš„ï¼Œæˆ–è€…è¯´æ˜¯æ²¡å¯¹é½çš„ã€‚
>
> ---

æˆ‘éšä¾¿æ‰¾äº†æ•°æ®é›†çš„ä¸€æ¡æ•°æ®ï¼Œ

```json
## è¯¥æ•°æ®æ˜¯æˆ‘ä¹‹å‰é”™è¯¯æ ¼å¼ä¸‹ä¿å­˜çš„ï¼Œæˆ‘ä»¬ä¸»è¦çœ‹ä¸‹bbox
{
    "image": [
        "./data/test/003003.jpg"
    ],
    "query": "what numbered album is the photo from?",
    "response": "{\"bbox_2d\": [948.0, 633.0, 59.0, 21.0]}"
}
```

åœ¨å›¾ä¸­çš„ä½ç½®å¦‚ä¸‹æ‰€ç¤º

![](./qwen2_5-vl-report/example_picture_bbox.png)

ç„¶åç»è¿‡å®˜æ–¹ä»£ç è½¬æ¢åçš„bboxåæ ‡å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
## ç¼©æ”¾å›¾äº†ä¹‹åè§‚å¯Ÿåæ ‡å€¼å’Œå›¾åƒçš„å¯¹åº”å…³ç³»
import math

# This is the resize function of Qwen2.5-VL
def smart_resize(
    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280
):
    """Rescales the image so that the following conditions are met:
    1. Both dimensions (height and width) are divisible by 'factor'.
    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].
    3. The aspect ratio of the image is maintained as closely as possible.
    """
    if height < factor or width < factor:
        raise ValueError(f"height:{height} or width:{width} must be larger than factor:{factor}")
    elif max(height, width) / min(height, width) > 200:
        raise ValueError(
            f"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}"
        )
    h_bar = round(height / factor) * factor
    w_bar = round(width / factor) * factor
    if h_bar * w_bar > max_pixels:
        beta = math.sqrt((height * width) / max_pixels)
        h_bar = math.floor(height / beta / factor) * factor
        w_bar = math.floor(width / beta / factor) * factor
    elif h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (height * width))
        h_bar = math.ceil(height * beta / factor) * factor
        w_bar = math.ceil(width * beta / factor) * factor
    return h_bar, w_bar


def convert_to_qwen25vl_format(bbox, orig_height, orig_width, factor=28, min_pixels=56*56, max_pixels=14*14*4*1280):
    new_height, new_width = smart_resize(orig_height, orig_width, factor, min_pixels, max_pixels)
    scale_w = new_width / orig_width
    scale_h = new_height / orig_height
    
    x1, y1, x2, y2 = bbox
    x1_new = round(x1 * scale_w)
    y1_new = round(y1 * scale_h)
    x2_new = round(x2 * scale_w)
    y2_new = round(y2 * scale_h)
    
    x1_new = max(0, min(x1_new, new_width - 1))
    y1_new = max(0, min(y1_new, new_height - 1))
    x2_new = max(0, min(x2_new, new_width - 1))
    y2_new = max(0, min(y2_new, new_height - 1))
    
    return [x1_new, y1_new, x2_new, y2_new]


new_bboxes = [x1, y1, x1+w, y1+h]
print(f"æ–°çš„åæ ‡æ˜¯{new_bboxes}")
print(f"è½¬æ¢åçš„åæ ‡æ˜¯{convert_to_qwen25vl_format(new_bboxes, image.shape[0], image.shape[1])}")
```

è¾“å‡ºç»“æœï¼š

æ–°çš„åæ ‡æ˜¯[948, 633, 1007, 654] 

è½¬æ¢åçš„åæ ‡æ˜¯[959, 628, 1019, 649]

âš ï¸è¿™ä¸€æ­¥æ˜¯å¿…é¡»è¦æœ‰çš„ï¼Œè¯¦ç»†ä»£ç å¯ä»¥å‚è€ƒä»“åº“ä¸­çš„./scripts/convert2sft_format.pyæ–‡ä»¶ã€‚

## ğŸš€å¾®è°ƒä»£ç 

### 1ã€ç¯å¢ƒè®¾ç½®

- ç¡¬ä»¶ä¿¡æ¯æ¦‚è§ˆï¼š[æ¦‚è§ˆ](https://swanlab.cn/@LiXinYu/qwen2.5-vl-sft-grounding/runs/j426nezsim58am8gwrhb0/environment/overview)

â€‹	**GPUï¼š**8 * NVIDIA H20 96GB

â€‹	**CPUï¼š**AMD EPYC 9K84 96-Core Processor 

â€‹	**æ“ä½œç³»ç»Ÿï¼š**TencentOS Server 3.1 (Final)

â€‹	**pythonç‰ˆæœ¬ï¼š**3.10.17

- pythonè®­ç»ƒç¯å¢ƒï¼š

  ```txt
  modelscope
  qwen_vl_utils
  transformers
  peft
  diffusers
  torch==2.5.1 
  torchvision==0.20.1 
  torchaudio==2.5.1
  swanlab
  deepspeed
  ```


> å®æµ‹3090ä¹Ÿè¡Œï¼Œ8*3090 24GBä¹Ÿå¯ä»¥è¿è¡Œï¼Œä¸è¿‡åç»­çš„å‚æ•°éœ€è¦è°ƒæ•´



### 2ã€æ•°æ®é¢„å¤„ç†

å¯ä»¥è¯´è¯¥æ­¥éª¤æ˜¯å¤§æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒï¼Œå¾ˆå®¹æ˜“å‡ºç°æŠ¥é”™bugï¼Œè¿™é‡Œæ³¨æ„ä¸¤ç‚¹ï¼Œåªè¦è¿™ä¸¤ç‚¹èƒ½åšå¥½ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†å°±ä¸éš¾äº†ï¼Œè¿™ä¸¤ç‚¹éƒ½æ˜¯Trainerä¸­å‡ºç°çš„ã€‚

- train_datasetï¼šæ•°æ®é›†ï¼Œå¹¶ä¸”æ˜¯Datasetæ ¼å¼ï¼Œä¹Ÿå°±æ˜¯huggingfaceèƒ½è¯»æ‡‚çš„æ ¼å¼
- data_collatorï¼šç”¨äºå¤„ç†æ•°æ®çš„æ‰¹é‡ç»„åˆå’Œé¢„å¤„ç†ï¼Œç¡®ä¿æ•°æ®èƒ½å¤Ÿä»¥æ­£ç¡®çš„æ ¼å¼è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚åŒ…æ‹¬å¡«å……ï¼ˆPaddingï¼‰ã€å¼ é‡è½¬æ¢ï¼ˆTensor Conversionï¼‰ã€æˆªæ–­ï¼ˆTruncationï¼‰ç­‰

ä»£ç ä½äºvision_datacollator.pyä¸­ï¼Œå…·ä½“æ€ä¹ˆåšæˆ‘ä»¬çœ‹çœ‹ä¸‹é¢çš„è¯¦ç»†è®²è§£ã€‚

**1ã€train_dataset**

æœ€é‡è¦çš„å°±æ˜¯æ ¼å¼å¯¹åº”ä¸Šå°±è¡Œï¼Œtransformersåº“æ˜¯huggingfaceå¼€æºçš„ä¸“é—¨ç”¨äºå¤„ç†å¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ç­‰çš„å‡½æ•°åº“ï¼Œä¸ºäº†ç¡®ä¿æ•°æ®èƒ½å¤Ÿè¢«æ¨¡å‹æ­£ç¡®åŠ è½½å’Œå¤„ç†ï¼Œæ•°æ®å¿…é¡»ç¬¦åˆç‰¹å®šçš„æ ¼å¼ã€‚è¿™ç§æ ¼å¼é€šå¸¸æ˜¯ `Dataset` å¯¹è±¡ï¼Œè¿™æ˜¯ç”± Hugging Face çš„ `datasets` åº“æä¾›çš„ä¸€ä¸ªç±»ï¼Œç”¨äºè¡¨ç¤ºå’Œæ“ä½œæ•°æ®é›†ã€‚

```python
DatasetDict({
    train: Dataset({
        features: ['image', 'query', 'response'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['image', 'query', 'response'],
        num_rows: 100
    })
})
```

è¯¥éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

```python
 ################
  # Dataset
  ################
  # 1ã€è¯»å–ä¿å­˜çš„jsonlæ–‡ä»¶ï¼Œä½¿ç”¨datasets.load_datasetç”Ÿæˆçš„æ•°æ®é›†å³æ˜¯Datasetæ ¼å¼ï¼Œç¬¦åˆhfä½¿ç”¨æ ‡å‡†æ ¼å¼
  train_dataset = datasets.load_dataset("json", data_files=data_args.train_dataset_name)
  test_dataset = datasets.load_dataset("json", data_files=data_args.test_dataset_name)
  # 2ã€åˆ›å»º DatasetDictï¼Œè¿™éƒ¨åˆ†åªæ˜¯ä¸ºäº†åç»­è¯»å–æµ‹è¯•æ•°æ®æ–¹ä¾¿ï¼Œå› æ­¤æŠŠtrainå’Œtestæ”¾åœ¨ä¸€èµ·
  raw_dataset = datasets.DatasetDict({
    "train": train_dataset["train"],
    "test": test_dataset["train"]
  })
  print(raw_dataset)
  # 3ã€å›ºå®šæ•°æ®é›†æ ¼å¼ç”¨äºåé¢æ‰¹å¤„ç†æ•°æ®é›†
  def preporocess_textvqa(example):
    return {
      "image": example["image"],
      "user": example["query"],
      "assistant": example["response"],
    }

  raw_dataset = raw_dataset.map(
    preporocess_textvqa,
    remove_columns=raw_dataset["train"].column_names,
    desc="Preprocessing textvqa dataset",
  )
  
  
# 4ã€Traineræ•°æ®é›†è°ƒç”¨
train_dataset=raw_dataset["train"],
eval_dataset=(
            raw_dataset["test"] if training_args.eval_strategy != "no" else None
        ),
```

**2ã€data_collator**

ç”±äºæœ¬æ¬¡æ•™ç¨‹æ¶‰åŠåæ ‡çš„ç¼©æ”¾ï¼Œå› æ­¤éœ€è¦è‡ªå·±å†™data_collatoréƒ¨åˆ†ï¼Œé€šè¿‡è°ƒæ•´æ•°æ®é›†æ ¼å¼æ¥ç”¨äºæ¨¡å‹è®­ç»ƒã€‚

1. ç¼©æ”¾å›¾åƒçš„å¤§å°

å› ä¸ºåŸæ•°æ®é›†çš„åæ ‡å¯¹åº”çš„å›¾éƒ½æ˜¯ä¸åŒçš„å¤§å°ï¼Œè€Œä¸”å›¾åƒä¸€èˆ¬éƒ½æ¯”è¾ƒå¤§ï¼Œå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸€èˆ¬åœ¨è®­ç»ƒé˜¶æ®µå¯¹äºå›¾åƒçš„å¤§å°æœ‰è¦æ±‚ï¼Œæ¯”å¦‚256\*256ã€512\*512ç­‰ï¼Œè€ŒåŸå›¾çš„å¤§å°ä¸ä¸€ï¼Œå› æ­¤éœ€è¦ç»Ÿä¸€ä¸‹å›¾åƒå¤§å°ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
# ç¼©æ”¾å›¾åƒçš„å¤§å°ï¼ŒåŒæ—¶å› ä¸ºgroundingä»»åŠ¡ï¼Œéœ€è¦åŒæ—¶ç¼©æ”¾åæ ‡
def resize_with_max_side(image, max_side_length):
    # è·å–åŸå§‹å°ºå¯¸
    width, height = image.size
    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale = min(max_side_length / width, max_side_length / height)
    # è®¡ç®—æ–°çš„å°ºå¯¸
    new_width = int(width * scale)
    new_height = int(height * scale)
    # è°ƒæ•´å›¾åƒå¤§å°
    resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    return resized_image, scale
```

2. ç¼©æ”¾åæ ‡æ•°æ®

å› ä¸ºå›¾åƒç¼©æ”¾äº†ï¼Œå› æ­¤åæ ‡ä½ç½®ä¹Ÿè¦ç¼©æ”¾åˆ°æ–°çš„å›¾åƒçš„å¯¹åº”ä½ç½®ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
def resize_bbox(bbox, scale):
    # ç¼©æ”¾çŸ©å½¢æ¡†åæ ‡
    return [int(coord * scale) for coord in bbox]
```

3. æ„å»ºæ•°æ®é›†çš„input_ids

æ ¹æ®ä¸Šé¢ä¸¤æ­¥è°ƒæ•´çš„ä»£ç å¦‚ä¸‹ï¼Œéœ€è¦åˆ†åˆ«æŠŠç»Ÿä¸€çš„imageã€questionã€answerè¾“å‡ºï¼š

```python
question = example["user"]
answer = example["assistant"]
# éœ€è¦è¯»å–å›¾åƒï¼Œéœ€è¦ç¡®ä¿æ˜¯RGBå›¾åƒ
image_path = example['image'][0]
image = Image.open(image_path)
# è¾“å‡ºç¼©æ”¾åçš„å›¾åƒä»¥åŠç¼©æ”¾å€ç‡
image, scale = resize_with_max_side(
  image, max_side_length=self.max_img_side_length
)
# ç¼©æ”¾answerçš„åæ ‡å€¼
# answeræ˜¯ä¸€ä¸ªjsonå­—ç¬¦ä¸²ï¼Œè§£ææˆå­—å…¸
answer = json.loads(answer)
answer = {"bbox_2d": resize_bbox(answer["bbox_2d"],scale)}
# è½¬åŒ–æ–°çš„answer
answer = json.dumps(answer, indent=None)
```

æ ¹æ®å¾—åˆ°çš„imageã€questionã€answerç»è¿‡å¤§æ¨¡å‹åŠ è½½ä¸ºtokensæ ¼å¼ï¼š

```python
prompt = "Please enclose the corresponding positions using coordinate boxes. Examples of coordinate value formats: [x1,y1,x2,y2]"
question = '<image>\n'+ question+prompt
messages = [
  {
    "role": "user",
    "content": [
      {"type": "image"},
      {"type": "text", "text": question},
    ],
  }
]
prompt = self.processor.tokenizer.apply_chat_template(
  messages, tokenize=False, add_generation_prompt=True
)
answer = f"{answer}<|im_end|>\n"
input_ids = self.processor(
  images=[image],
  text=prompt + answer,
  return_tensors="pt",
  max_length=self.max_seq_length,
  truncation=False,
  padding=False,
)
answer_ids = self.processor.tokenizer(
  answer, add_special_tokens=False, return_tensors="pt"
)
ignore_ids_len = len(input_ids["input_ids"][0]) - len(
  answer_ids["input_ids"][0]
)
input_ids["labels"] = torch.cat(
  [
    torch.tensor([IGNORE_INDEX] * ignore_ids_len).unsqueeze(0),
    answer_ids["input_ids"],
  ],
  dim=1,
)
```

å¢åŠ position_ids

```python
position_ids, _ = self.get_rope_index_2(
  self.processor.image_processor.merge_size,
  input_ids["input_ids"],
  input_ids["image_grid_thw"],
)
input_ids["position_ids"] = position_ids
```

å¡«å……è‡³æœ€å¤§seq_length

```python
# padding
if len(input_ids["labels"]) < self.max_seq_length:
  input_ids["input_ids"] = torch.cat(
    [
      input_ids["input_ids"],
      torch.tensor(
        [self.processor.tokenizer.pad_token_id]
        * (self.max_seq_length - len(input_ids["input_ids"]))
      ).unsqueeze(0),
    ],
    dim=1,
  )
  input_ids["labels"] = torch.cat(
    [
      input_ids["labels"],
      torch.tensor(
        [IGNORE_INDEX]
        * (self.max_seq_length - len(input_ids["labels"]))
      ).unsqueeze(0),
    ],
    dim=1,
  )
  input_ids["attention_mask"] = input_ids["input_ids"].ne(
    self.processor.tokenizer.pad_token_id
  )
  # padding position_ids
  pad_length = self.max_seq_length - input_ids["position_ids"].shape[2]
  input_ids["position_ids"] = torch.nn.functional.pad(
    input_ids["position_ids"], (0, pad_length), "constant", 1
  )
```

å¦‚æœè¶…è¿‡é•¿åº¦éƒ¨åˆ†è¿›è¡Œæˆªæ–­truncate

```python
# truncate
if len(input_ids["input_ids"][0]) > self.max_seq_length:
  input_ids["input_ids"] = input_ids["input_ids"][
    :, : self.max_seq_length
  ]
  input_ids["labels"] = input_ids["labels"][:, : self.max_seq_length]
  input_ids["attention_mask"] = input_ids["attention_mask"][
    :, : self.max_seq_length
  ]
  input_ids["position_ids"] = input_ids["position_ids"][
    :, : self.max_seq_length
  ]
```

æœ€ç»ˆå¾—åˆ°æ‰€æœ‰çš„input_ids

```python
batch_input_ids = {
  "input_ids": torch.cat(
    [input_ids["input_ids"] for input_ids in batch_input_ids], dim=0
  ),
  "attention_mask": torch.cat(
    [input_ids["attention_mask"] for input_ids in batch_input_ids], dim=0
  ),
  "labels": torch.cat(
    [input_ids["labels"] for input_ids in batch_input_ids], dim=0
  ),
  "pixel_values": torch.cat(
    [input_ids["pixel_values"] for input_ids in batch_input_ids], dim=0
  ),
  "image_grid_thw": torch.cat(
    [input_ids["image_grid_thw"] for input_ids in batch_input_ids], dim=0
  ),
  "position_ids": torch.cat(
    [input_ids["position_ids"] for input_ids in batch_input_ids], dim=1
  ),
}
return batch_input_ids
```

***è¯¥éƒ¨åˆ†æ•´ä½“ä»£ç ***

```python
from typing import Optional, Tuple
import copy

import transformers
import torch

from PIL import Image
import json

IGNORE_INDEX = -100

# ç¼©æ”¾å›¾åƒçš„å¤§å°ï¼ŒåŒæ—¶å› ä¸ºgroundingä»»åŠ¡ï¼Œéœ€è¦åŒæ—¶ç¼©æ”¾åæ ‡
def resize_with_max_side(image, max_side_length):
    # è·å–åŸå§‹å°ºå¯¸
    width, height = image.size
    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale = min(max_side_length / width, max_side_length / height)
    # è®¡ç®—æ–°çš„å°ºå¯¸
    new_width = int(width * scale)
    new_height = int(height * scale)
    # è°ƒæ•´å›¾åƒå¤§å°
    resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    return resized_image, scale

def resize_bbox(bbox, scale):
    # ç¼©æ”¾çŸ©å½¢æ¡†åæ ‡
    return [int(coord * scale) for coord in bbox]


class Qwen2_5VLCollator:

    def __init__(
        self, processor, max_seq_length=1024, max_img_side_length=1024, **kwargs
    ):
        self.processor = processor
        # to fix bug in Qwen2.5VL
        self.processor.tokenizer.chat_template =  "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
        self.max_seq_length = max_seq_length
        self.max_img_side_length = max_img_side_length

    def __call__(self, examples):
        batch_input_ids = []
        for example in examples:
            # æ ¹æ®æ•°æ®é›†æ ¼å¼æ¥ï¼Œæ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼š
            """
            {"image": ["./data/train/000001.jpg"], "query": "what is the name of the company on the card?", "response": "{\n  \"bbox_2d\": [\n    712.0,\n    255.0,\n    64.0,\n    43.0\n  ]\n}"}
            """
            question = example["user"]
            answer = example["assistant"]
            # éœ€è¦è¯»å–å›¾åƒï¼Œéœ€è¦ç¡®ä¿æ˜¯RGBå›¾åƒ
            image_path = example['image'][0]
            image = Image.open(image_path)
            # è¾“å‡ºç¼©æ”¾åçš„å›¾åƒä»¥åŠç¼©æ”¾å€ç‡
            image, scale = resize_with_max_side(
                image, max_side_length=self.max_img_side_length
            )
            # ç¼©æ”¾answerçš„åæ ‡å€¼
            # answeræ˜¯ä¸€ä¸ªjsonå­—ç¬¦ä¸²ï¼Œè§£ææˆå­—å…¸
            answer = json.loads(answer)
            answer = {"bbox_2d": resize_bbox(answer["bbox_2d"],scale)}
            # è½¬åŒ–æ–°çš„answer
            answer = json.dumps(answer, indent=None)
            # è¿™äº†ä¸çŸ¥é“æ˜¯å¦éœ€è¦æ·»åŠ prompt
            prompt = "Please enclose the corresponding positions using coordinate boxes. Examples of coordinate value formats: [x1,y1,x2,y2]"
            question = '<image>\n'+ question+prompt
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": question},
                    ],
                }
            ]
            prompt = self.processor.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            answer = f"{answer}<|im_end|>\n"
            input_ids = self.processor(
                images=[image],
                text=prompt + answer,
                return_tensors="pt",
                max_length=self.max_seq_length,
                truncation=False,
                padding=False,
            )
            answer_ids = self.processor.tokenizer(
                answer, add_special_tokens=False, return_tensors="pt"
            )
            ignore_ids_len = len(input_ids["input_ids"][0]) - len(
                answer_ids["input_ids"][0]
            )
            input_ids["labels"] = torch.cat(
                [
                    torch.tensor([IGNORE_INDEX] * ignore_ids_len).unsqueeze(0),
                    answer_ids["input_ids"],
                ],
                dim=1,
            )
            # position_ids
            position_ids, _ = self.get_rope_index_2(
                self.processor.image_processor.merge_size,
                input_ids["input_ids"],
                input_ids["image_grid_thw"],
            )
            input_ids["position_ids"] = position_ids

            # padding
            if len(input_ids["labels"]) < self.max_seq_length:
                input_ids["input_ids"] = torch.cat(
                    [
                        input_ids["input_ids"],
                        torch.tensor(
                            [self.processor.tokenizer.pad_token_id]
                            * (self.max_seq_length - len(input_ids["input_ids"]))
                        ).unsqueeze(0),
                    ],
                    dim=1,
                )
                input_ids["labels"] = torch.cat(
                    [
                        input_ids["labels"],
                        torch.tensor(
                            [IGNORE_INDEX]
                            * (self.max_seq_length - len(input_ids["labels"]))
                        ).unsqueeze(0),
                    ],
                    dim=1,
                )
                input_ids["attention_mask"] = input_ids["input_ids"].ne(
                    self.processor.tokenizer.pad_token_id
                )
                # padding position_ids
                pad_length = self.max_seq_length - input_ids["position_ids"].shape[2]
                input_ids["position_ids"] = torch.nn.functional.pad(
                    input_ids["position_ids"], (0, pad_length), "constant", 1
                )

            # truncate
            if len(input_ids["input_ids"][0]) > self.max_seq_length:
                input_ids["input_ids"] = input_ids["input_ids"][
                    :, : self.max_seq_length
                ]
                input_ids["labels"] = input_ids["labels"][:, : self.max_seq_length]
                input_ids["attention_mask"] = input_ids["attention_mask"][
                    :, : self.max_seq_length
                ]
                input_ids["position_ids"] = input_ids["position_ids"][
                    :, : self.max_seq_length
                ]
            # batching
            batch_input_ids.append(input_ids)

        batch_input_ids = {
            "input_ids": torch.cat(
                [input_ids["input_ids"] for input_ids in batch_input_ids], dim=0
            ),
            "attention_mask": torch.cat(
                [input_ids["attention_mask"] for input_ids in batch_input_ids], dim=0
            ),
            "labels": torch.cat(
                [input_ids["labels"] for input_ids in batch_input_ids], dim=0
            ),
            "pixel_values": torch.cat(
                [input_ids["pixel_values"] for input_ids in batch_input_ids], dim=0
            ),
            "image_grid_thw": torch.cat(
                [input_ids["image_grid_thw"] for input_ids in batch_input_ids], dim=0
            ),
            "position_ids": torch.cat(
                [input_ids["position_ids"] for input_ids in batch_input_ids], dim=1
            ),
        }
        return batch_input_ids

    def get_rope_index_2(
        self,
        spatial_merge_size: Optional[int] = 2,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        second_per_grid_ts: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Calculate the 3D rope index based on image and video's temporal, height and width in LLM.

        Explanation:
            Each embedding sequence contains vision embedding and text embedding or just contains text embedding.

            For pure text embedding sequence, the rotary position embedding has no difference with mordern LLMs.
            Examples:
                input_ids: [T T T T T], here T is for text.
                temporal position_ids: [0, 1, 2, 3, 4]
                height position_ids: [0, 1, 2, 3, 4]
                width position_ids: [0, 1, 2, 3, 4]

            For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
            and 1D rotary position embeddin for text part.
            Examples:
                Assume we have a video input with 3 temporal patches, 2 height patches and 2 width patches.
                input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
                vision temporal position_ids: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]
                vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
                vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
                text temporal position_ids: [3, 4, 5, 6, 7]
                text height position_ids: [3, 4, 5, 6, 7]
                text width position_ids: [3, 4, 5, 6, 7]
                Here we calculate the text start position_ids as the max vision position_ids plus 1.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
                it.
            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
                The temporal, height and width of feature shape of each image in LLM.
            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
                The temporal, height and width of feature shape of each video in LLM.
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

        Returns:
            position_ids (`torch.LongTensor` of shape `(3, batch_size, sequence_length)`)
            mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)
        """
        image_token_id = 151655
        video_token_id = 151656
        vision_start_token_id = 151652
        mrope_position_deltas = []
        if input_ids is not None and (
            image_grid_thw is not None or video_grid_thw is not None
        ):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i] == 1]
                image_nums, video_nums = 0, 0
                vision_start_indices = torch.argwhere(
                    input_ids == vision_start_token_id
                ).squeeze(1)
                vision_tokens = input_ids[vision_start_indices + 1]
                image_nums = (vision_tokens == image_token_id).sum()
                video_nums = (vision_tokens == video_token_id).sum()
                input_tokens = input_ids.tolist()
                llm_pos_ids_list: list = []
                st = 0
                remain_images, remain_videos = image_nums, video_nums
                for _ in range(image_nums + video_nums):
                    if image_token_id in input_tokens and remain_images > 0:
                        ed_image = input_tokens.index(image_token_id, st)
                    else:
                        ed_image = len(input_tokens) + 1
                    if video_token_id in input_tokens and remain_videos > 0:
                        ed_video = input_tokens.index(video_token_id, st)
                    else:
                        ed_video = len(input_tokens) + 1
                    if ed_image < ed_video:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        image_index += 1
                        remain_images -= 1
                        ed = ed_image
                    else:
                        t, h, w = (
                            video_grid_thw[video_index][0],
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )
                        video_index += 1
                        remain_videos -= 1
                        ed = ed_video
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )
                    text_len = ed - st

                    st_idx = (
                        llm_pos_ids_list[-1].max() + 1
                        if len(llm_pos_ids_list) > 0
                        else 0
                    )
                    llm_pos_ids_list.append(
                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                    )

                    t_index = (
                        torch.arange(llm_grid_t)
                        .view(-1, 1)
                        .expand(-1, llm_grid_h * llm_grid_w)
                        .flatten()
                    )
                    h_index = (
                        torch.arange(llm_grid_h)
                        .view(1, -1, 1)
                        .expand(llm_grid_t, -1, llm_grid_w)
                        .flatten()
                    )
                    w_index = (
                        torch.arange(llm_grid_w)
                        .view(1, 1, -1)
                        .expand(llm_grid_t, llm_grid_h, -1)
                        .flatten()
                    )
                    llm_pos_ids_list.append(
                        torch.stack([t_index, h_index, w_index]) + text_len + st_idx
                    )
                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w

                if st < len(input_tokens):
                    st_idx = (
                        llm_pos_ids_list[-1].max() + 1
                        if len(llm_pos_ids_list) > 0
                        else 0
                    )
                    text_len = len(input_tokens) - st
                    llm_pos_ids_list.append(
                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                    )

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(
                    position_ids.device
                )
                mrope_position_deltas.append(
                    llm_positions.max() + 1 - len(total_input_ids[i])
                )
            mrope_position_deltas = torch.tensor(
                mrope_position_deltas, device=input_ids.device
            ).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = (
                    position_ids.unsqueeze(0)
                    .expand(3, -1, -1)
                    .to(attention_mask.device)
                )
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(
                    -1, keepdim=True
                )[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas
        

################
# Data collator map
################
vision_data_collator_map = {
    "Qwen2_5VLCollator": Qwen2_5VLCollator,
}
```

### 3ã€å‚æ•°è®¾ç½®

***1ã€åˆå§‹åŒ–æ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒå‚æ•°***

å› ä¸ºåƒmodel_name_or_pathå¯èƒ½éœ€è¦å¤šæ¬¡ä¿®æ”¹ï¼Œä½†æ˜¯åœ¨ä»£ç é‡Œä¿®æ”¹å¤ªéº»çƒ¦äº†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è„šæœ¬æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼Œå‰æéœ€è¦å¯¹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
## å‚æ•°è®¾ç½®

################
# Model arguments
################
@dataclass
class ModelArguments:
    auto_model_class: Optional[str] = field(
        default="AutoModelForCausalLM",
        metadata={
            "help": (
                "The auto model class to use for the model. Default is AutoModelForCausalLM."
            )
        },
    )
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "Path to pretrained model or model identifier from huggingface.co/models."
        },
    )
    processor_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "Path to pretrained processor or processor identifier from huggingface.co/models."
        },
    )
    trust_remote_code: Optional[bool] = field(
        default=True,
        metadata={
            "help": "Whether to trust the remote code when loading the model and processor. default is True."
        },
    )
    torch_dtype: Optional[str] = field(
        default="bfloat16",
        metadata={"help": "The torch dtype to use for the model. Default is bfloat16."},
    )

    def __post_init__(self):
        if self.processor_name_or_path is None:
            self.processor_name_or_path = self.model_name_or_path

################
# datasets arguments
################
@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    train_dataset_name: Optional[str] = field(
        default=None,
        metadata={"help": "The name of the train dataset to use (via the datasets library)."},
    )
    test_dataset_name: Optional[str] = field(
        default=None,
        metadata={"help": "The name of the test dataset to use (via the datasets library)."},
    )
    data_collator: Optional[str] = field(
        default="vision_data_collator",
        metadata={
            "help": (
                "The data collator to use for the dataset. Default is vision_data_collator."
            )
        },
    )
    max_seq_length: Optional[int] = field(
        default=1024,
        metadata={
            "help": (
                "The maximum total input sequence length after tokenization. Sequences longer "
                "than this will be truncated, sequences shorter will be padded."
            )
        },
    )
    max_image_side: Optional[int] = field(
        default=256,
        metadata={
            "help": ("The size of the image to use for the dataset. Default is 224.")
        },
    )

################
# lora arguments
################
@dataclass
class LoraArguments:
    use_lora: bool = False
    r: int = 8
    lora_alpha: int = 32
    target_modules: List[str] = field(default_factory=lambda: ["q_proj", "v_proj"])
    bias = "none"
    task_type: str = "CAUSAL_LM"
    lora_dropout: float = 0.05
    inference_mode: bool = False
```

å…¶ä¸­å› ä¸ºæœ¬æ¬¡è®­ç»ƒé‡‡ç”¨å…¨å‚æ•°å¾®è°ƒï¼Œå› æ­¤lora argumentsæ²¡ç”¨ä¸Šï¼Œæœ‰å…´è¶£çš„å°ä¼™ä¼´å¯ä»¥å°è¯•ä¸‹loraå¾®è°ƒã€‚

***2ã€è„šæœ¬æ–‡ä»¶è®¾ç½®***

æœ¬æ¬¡æ•™ç¨‹é‡‡ç”¨å•æœºå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œå› æ­¤è„šæœ¬æ–‡ä»¶æœ‰ç‚¹å¤šæœ‰ç‚¹ä¹±ï¼Œä¸‹è¡Œä»£ç é¦–å…ˆå±•ç¤ºå¦‚ä½•æ•´ä½“ä½¿ç”¨è¿™äº›è„šæœ¬æ–‡ä»¶ï¼Œç„¶åä¼šä¸€ä¸€è®²è§£ã€‚

```bash
bash scripts/sft_vqa_8gpu-z2.sh configs/SFT_Qwen2_5-VL-3B-Instruct_vqa.yaml
```

- scripts/sft_vqa_8gpu-z2.shï¼š

  ```bash
  ########################################################
  # train sft.py with 8gpu in deepspeed zero2 bf16
  ########################################################
  accelerate launch \
      --num_processes 8 \
      --main_process_port 25001 \
      --config_file configs/deepspeed_bf16_zero2.yaml \
      sft.py \
      --config $1
  ```

  è¯¥è„šæœ¬ä½¿ç”¨ä½¿ç”¨ `accelerate` å·¥å…·æ¥ç®¡ç†å¤šGPUè®­ç»ƒè¿‡ç¨‹ï¼ŒæŒ‡å®šä½¿ç”¨8ä¸ªGPUè¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒä»»åŠ¡é€šè¿‡ `deepspeed` çš„ `zero2` ä¼˜åŒ–ç­–ç•¥å’Œ `bf16`ï¼ˆbfloat16ï¼‰æµ®ç‚¹æ ¼å¼æ¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è„šæœ¬åŠ è½½é…ç½®æ–‡ä»¶ `deepspeed_bf16_zero2.yaml`ï¼Œè¯¥æ–‡ä»¶å®šä¹‰äº†åˆ†å¸ƒå¼è®­ç»ƒçš„å„é¡¹å‚æ•°ã€‚è®­ç»ƒä»»åŠ¡çš„ä¸»å…¥å£æ˜¯ `sft.py` æ–‡ä»¶ï¼Œæ¥å—ä¸€ä¸ªå¤–éƒ¨å‚æ•° `config`ï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šè®­ç»ƒä»»åŠ¡çš„é…ç½®æ–‡ä»¶å’Œå…¶ä»–ç›¸å…³å‚æ•°ã€‚

  å¦‚æœGPUæ•°é‡æœ‰å˜ï¼Œå¯ä»¥ä¿®æ”¹num_processeséƒ¨åˆ†ï¼Œå…¶ä»–éƒ¨åˆ†ä¸å˜ã€‚

- configs/deepspeed_bf16_zero2.yamlï¼š

```yaml
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  gradient_clipping: 1.0
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: false
  zero_stage: 2
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16'
num_machines: 1
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

è¯¥è„šæœ¬æ–‡ä»¶å®šä¹‰äº†ä½¿ç”¨ DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„é…ç½®ï¼ŒåŸºæœ¬éƒ½æ˜¯é»˜è®¤å‚æ•°ï¼Œä¸éœ€è¦æ”¹å†…éƒ¨å‚æ•°ã€‚

- configs/SFT_Qwen2_5-VL-3B-Instruct_vqa.yamlï¼š

```yaml
# æ¨¡å‹è®¾ç½®ï¼Œå‚æ•°è®¾ç½®å‚è€ƒ trl.ModelConfig
model_name_or_path: /home/jiangqiushan/test/models/Qwen2.5-VL-3B-Instruct
auto_model_class: "Qwen2_5_VLForConditionalGeneration"
torch_dtype: bfloat16

# æ•°æ®é›†è®¾ç½®ï¼Œå‚æ•°è®¾ç½®å‚è€ƒ sft.DataTrainingArguments
train_dataset_name: ./data/train.jsonl
test_dataset_name: ./data/test.jsonl
preprocessing_num_workers: 1
data_collator: "Qwen2_5VLCollator"
max_seq_length: 256

# è®­ç»ƒè®¾ç½®ï¼Œå‚æ•°è®¾ç½®å‚è€ƒ transformers.TrainingArguments trl.SFTConfig
## è®­ç»ƒè¶…å‚æ•°
seed: 2025
data_seed: 2025
remove_unused_columns: False  # æ­¤å¤„éœ€è¦æŒ‡å®šä¸ºfalse
## batchsizeã€è®­ç»ƒæ¬¡æ•°ç›¸å…³
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
max_steps: 2000
## å­¦ä¹ ç‡ç›¸å…³
learning_rate: 1.0e-5
lr_scheduler_type: cosine
warmup_ratio: 0.1
## è®­ç»ƒæ•ˆç‡ç›¸å…³
gradient_checkpointing: false
bf16: true
bf16_full_eval: true
## éªŒè¯è¾“å‡º
eval_strategy: steps
eval_steps: 0.1
## ç»“æœè¾“å‡º+æ—¥å¿—æ—¥å¿—è®¾ç½®
output_dir: /home/jiangqiushan/test/models/SFT_Qwen2_5-VL-3B-Instruct_vqa
save_steps: 0.2
save_total_limit: 1
report_to: swanlab
logging_first_step: true
logging_steps: 0.001
```

è¯¥è„šæœ¬æ–‡ä»¶å‚æ•°åŸºæœ¬å°±æ˜¯ä¸Šè¿°è¯´çš„æ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œå¯ä»¥æ ¹æ®è®­ç»ƒéœ€æ±‚ä¿®æ”¹ç›¸åº”çš„å‚æ•°ã€‚

> ğŸ’¡æ³¨æ„ï¼š
>
> ç”±äºæœ¬æ¬¡æ•™ç¨‹å›ºå®šmax_stepsï¼Œå› æ­¤æœ€ç»ˆçš„epochä¼šå¾ˆå¤§ï¼Œä¼šæœ‰è¿‡æ‹Ÿåˆçš„ç°è±¡ï¼Œå¦‚æœæƒ³è¦ä½¿ç”¨epochï¼Œå¯ä»¥å•ç‹¬è®¾ç½®ã€‚



### 4ã€æ¨¡å‹è®­ç»ƒ&ä¿å­˜

ä¸‹é¢æ˜¯æ¨¡å‹è®­ç»ƒå’Œä¿å­˜ä»£ç ï¼Œåœ¨sft.pyæ–‡ä»¶ä¸­ä¿å­˜ã€‚

```python
def main(data_args, training_args, model_args, lora_args):
    ################
    # Prepare something
    ################
    output_dir = training_args.output_dir
    dir_path, model_name = os.path.split(output_dir)
    new_model_name = device_type + "_" + model_name
    training_args.output_dir = os.path.join(dir_path, new_model_name)
    training_args.run_name = new_model_name
    set_seeds(training_args.seed)

    ################
    # Model init kwargs & Tokenizer
    ################
    # load processor
    processor = AutoProcessor.from_pretrained(
        pretrained_model_name_or_path=model_args.processor_name_or_path,
        trust_remote_code=model_args.trust_remote_code,
        local_files_only=True,
    )
    # load and construct model
    model_class = getattr(transformers, model_args.auto_model_class)  # åŠ¨æ€åŠ è½½æ¨¡å‹ç±»
    if model_class is None:
        raise ValueError(f"Model class {model_args.auto_model_class} is not available.")
    model = model_class.from_pretrained(
        pretrained_model_name_or_path=model_args.model_name_or_path,
        torch_dtype=getattr(torch, model_args.torch_dtype),
        trust_remote_code=model_args.trust_remote_code,
        local_files_only=True,
    )
    if lora_args.use_lora:
        lora_config = LoraConfig(
            r=lora_args.r,
            lora_alpha=lora_args.lora_alpha,
            target_modules=lora_args.target_modules,
            lora_dropout=lora_args.lora_dropout,
            bias=lora_args.bias,
        )
        model = get_peft_model(model, lora_config)

    ################
    # Dataset
    ################
    train_dataset = datasets.load_dataset("json", data_files=data_args.train_dataset_name)
    test_dataset = datasets.load_dataset("json", data_files=data_args.test_dataset_name)
    # åˆ›å»º DatasetDict
    raw_dataset = datasets.DatasetDict({
        "train": train_dataset["train"],
        "test": test_dataset["train"]
    })
    print(raw_dataset)
    # data formatting
    def preporocess_textvqa(example):
        return {
            "image": example["image"],
            "user": example["query"],
            "assistant": example["response"],
        }

    raw_dataset = raw_dataset.map(
        preporocess_textvqa,
        remove_columns=raw_dataset["train"].column_names,
        desc="Preprocessing textvqa dataset",
    )
    data_collator = vision_data_collator_map[data_args.data_collator](
        processor=processor,
        max_seq_length=data_args.max_seq_length,
        max_img_side_length=data_args.max_image_side,
    )

    ################
    # Training
    ################
    last_checkpoint = None  # load last checkpoint if available
    if (
        os.path.isdir(training_args.output_dir)
        and not training_args.overwrite_output_dir
    ):
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        print(
            f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
            "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
        )
        # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=raw_dataset["train"],
        eval_dataset=(
            raw_dataset["test"] if training_args.eval_strategy != "no" else None
        ),
        data_collator=data_collator,
    )
    trainer.train(resume_from_checkpoint=last_checkpoint)
    trainer.save_model(training_args.output_dir)


if __name__ == "__main__":
    dataclass_types = (
        DataTrainingArguments,
        TrainingArguments,
        ModelArguments,
        LoraArguments,
    )
    parser = TrlParser(dataclass_types)
    data_args, training_args, model_args, lora_args = parser.parse_args_and_config()
    main(data_args, training_args, model_args, lora_args)
	
```



### 5ã€å®Œæ•´ä»£ç 

gitä»£ç ğŸ‘‰[[textvqa_grounding_task_qwen2.5-vl-ft](https://github.com/828Tina/textvqa_grounding_task_qwen2.5-vl-ft)](https://github.com/828Tina/textvqa_grounding_task_qwen2.5-vl-ft)

ä»£ç æ€»è§ˆå¦‚ä¸‹ï¼š

```python
project/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ deepspeed_bf16_zero2.yaml
â”‚   â”œâ”€â”€ deepspeed_bf16_zero3.yaml
â”‚   â””â”€â”€ SFT_Qwen2_5-VL-3B-Instruct_vqa.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ merge_model.py
â”‚   â”œâ”€â”€ convert2sft_format.py
â”‚   â”œâ”€â”€ download_data.sh
â”‚   â”œâ”€â”€ download_model.sh
â”‚   â”œâ”€â”€ download_dayiwan.sh
â”‚   â””â”€â”€ sft_vqa_8gpu-z2.sh
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sft.py
â”œâ”€â”€ utils.py
â””â”€â”€ vision_datacollator.py
```

è¿è¡Œé¡ºåºå¦‚ä¸‹ï¼š

```bash
# 1ã€ä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†
bash download_data.sh
bash download_model.sh

# 2ã€æ•°æ®é¢„å¤„ç†ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°
python scripts/convert2sft_format.py

# 3ã€ä¿®æ”¹è¶…å‚æ•°ï¼Œåœ°å€ä¸º./configs/SFT_Qwen2_5-VL-3B-Instruct_vqa.yaml
# ä¸»è¦æ˜¯ä¿®æ”¹å…¶ä¸­çš„æ¨¡å‹åœ°å€å’Œä¿å­˜åœ°å€ç­‰
# å¼€å¯è®­ç»ƒ
bash scripts/sft_vqa_4gpu-z2.sh configs/SFT_Qwen2_5-VL-3B-Instruct_vqa.yaml
```



## ğŸ“ˆSwanLabå¯è§†åŒ–ç»“æœ

é“¾æ¥åœ¨è¿™ğŸ‘‰[SwanLab](https://swanlab.cn/@LiXinYu/qwen2.5-vl-sft-grounding/runs/j426nezsim58am8gwrhb0/chart)

<img src="./qwen2_5-vl-report/swanlab_results.png" alt="swanlabå¯è§†åŒ–è§‚æµ‹ç»“æœ" style="zoom:60%">



## ğŸ“Œå¾®è°ƒæ¨¡å‹åæ¨ç†æµ‹è¯•

æ¨ç†ä»£ç å…¶å®æˆ‘æ˜¯å‚è€ƒäº†é­”æ­ç¤¾åŒºQwen2.5VLçš„å®˜æ–¹æ¨ç†ä»£ç æ‰€å†™ï¼Œç”±äºæœ¬æ¬¡æ•™ç¨‹æœ€ç»ˆå®ç°äº†å…¨å‚å¾®è°ƒï¼Œå› æ­¤ä¸éœ€è¦åˆå¹¶æ“ä½œï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œmodeléœ€è¦çš„æ¨¡å‹åœ°å€ä¸ºè®­ç»ƒå¥½çš„æ¨¡å‹åœ°å€ï¼ˆcheckpoint-xxxæ–‡ä»¶åœ°å€ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ–‡æœ¬ç”Ÿæˆéƒ¨åˆ†ï¼›processoréœ€è¦çš„æ˜¯åŸºçº¿æ¨¡å‹åœ°å€ï¼Œä¹Ÿå°±æ˜¯Qwen2.5-VL-3B-Instructçš„ä¿å­˜åœ°å€ï¼Œå› ä¸ºè¯¥éƒ¨åˆ†ä¸»è¦ç”¨äºå¤„ç†å›¾åƒï¼Œè€Œæˆ‘ä»¬çš„å¾®è°ƒè¿‡ç¨‹ä¸»è¦æ˜¯å¯¹æ–‡æœ¬ç”Ÿæˆéƒ¨åˆ†çš„è®­ç»ƒã€‚

ä»£ç é“¾æ¥ğŸ‘‰[æ¨ç†ä»£ç ](https://github.com/828Tina/textvqa_grounding_task_qwen2.5-vl-ft/blob/main/scripts/inference.py)

è¿è¡Œä¸‹é¢çš„ä»£ç ï¼š

```bash
python inference.py
```

> ğŸ’¡æ³¨æ„ï¼š
>
> å‰é¢è¯´äº†åœ¨è¾“å…¥åˆ°Qwenæ¨¡å‹å‰å¯¹bboxè¿›è¡Œäº†å¤„ç†ï¼Œè¿™é‡Œæ¨ç†çš„æ—¶å€™æˆ‘ä»¬éœ€è¦å¤„ç†å›æ¥ï¼Œå°±æ˜¯ä»Qwenå®˜æ–¹è¦æ±‚çš„bbox_formatè½¬æ¢å›åŸå§‹çš„bboxæ ¼å¼ï¼Œè¿™é‡Œç¡®å®éº»çƒ¦äº†ç‚¹ğŸ˜®â€ğŸ’¨ã€‚

å¾—åˆ°ä¸‹é¢çš„ç»“æœï¼š

```text
æ¨¡å‹è¾“å‡ºï¼š
{"bbox_2d": [446, 630, 521, 660]}
```

ç„¶åæˆ‘ä»¬å°†åæ ‡æ”¾åœ¨å›¾åƒä¸Šçœ‹çœ‹æ˜¯å¦å‡†ç¡®

ä»£ç å¦‚ä¸‹ï¼š

```python
# å…ˆè¯»å–ä¸€ä¸ªå›¾ç‰‡ç„¶åè§‚å¯Ÿåæ ‡
import cv2
import numpy as np
import matplotlib.pyplot as plt
import json

EXAMPLE_IMAGE_PATH = "./data/test/003001.jpg"
EXAMPLE_TEXT_PATH = "./data/test.jsonl"

# è¯»å– JSONL æ–‡ä»¶å¹¶æŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„æ•°æ®
def find_data_with_image_path(file_path, image_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            try:
                data = json.loads(line)  # è§£ææ¯ä¸€è¡Œçš„ JSON æ•°æ®
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é”® "image" ä¸”å…¶å€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä¸”åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ EXAMPLE_IMAGE_PATH
                if 'image' in data and isinstance(data['image'], list) and data['image'][0] == image_path:
                    return data
            except json.JSONDecodeError as e:
                print(f"è§£æ JSON æ—¶å‡ºé”™: {e}")
                continue
    return None

# è°ƒç”¨å‡½æ•°å¹¶è¾“å‡ºç»“æœ
result = find_data_with_image_path(EXAMPLE_TEXT_PATH, EXAMPLE_IMAGE_PATH)
if result:
    print("æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼š")
    print(json.dumps(result, indent=4, ensure_ascii=False))
else:
    print("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®ã€‚")

print("-"*20)
# è¯»å–å›¾åƒï¼Œç„¶åçœ‹çœ‹å›¾åƒçš„å¤§å°å’Œåæ ‡çš„å¯¹åº”å…³ç³»
image = cv2.imread(EXAMPLE_IMAGE_PATH)
print(f"å›¾åƒå¤§å°æ˜¯{image.shape}")

### æ‰€ä»¥åæ ‡çš„è¯åº”è¯¥æ˜¯[å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å®½åº¦, é«˜åº¦]
x1, y1, x2, y2 = [446, 630, 521, 660]
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.imshow(image)
plt.axis('off')
plt.show()
```

å¯ä»¥çœ‹åˆ°

![](./qwen2_5-vl-report/inference_bbox.png)

å¯ä»¥çœ‹åˆ°åæ ‡ä½ç½®è¿˜æ˜¯æ¯”è¾ƒå‡†ç¡®çš„ã€‚

## âš™ï¸F&A

### 1ã€é­”æ­ç¤¾åŒºä¸‹è½½çš„æ•°æ®é›†ç”¨ä¸äº†

ç”±äºæœ¬èº«æ•°æ®é›†æ˜¯æ¥æºäºhuggingfaceï¼Œé­”æ­ç¤¾åŒºä¸Šä¼ çš„æ•°æ®é›†ä¼šæœ‰dataset_infos.jsonæ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶æ˜¯ä¸Šä¼ æ—¶è‡ªåŠ¨ç”Ÿæˆï¼Œç”¨ä»¥åœ¨æ•°æ®é¢„è§ˆåŠŸèƒ½é‡Œå±•ç¤ºæ•°æ®é›†ä¸­æ¯ä¸€ç±»åˆ«æ ‡ç­¾ï¼Œä½†æ˜¯ä¸ç¬¦åˆhuggingfaceçš„æ ¼å¼ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨çš„æ—¶å€™ä¼šè°ƒç”¨datasetsåº“ï¼Œç„¶åä¼šæŠ¥ä¸‹é¢çš„é”™è¯¯ï¼š

ä»£ç ï¼š

```python
from datasets import load_dataset

DATA_PATH = '/data/nvme0/textvqa_bbox'
ds = load_dataset(DATA_PATH,split='train')
```

æŠ¥é”™ï¼š

```python
TypeError: Value.__init__() missing 1 required positional argument: 'dtype'
```

è§£å†³ï¼š

åˆ æ‰ä¸‹è½½åˆ°æœ¬åœ°çš„æ•°æ®é›†æ–‡ä»¶é‡Œçš„dataset_infos.jsonæ–‡ä»¶ã€‚



## å‚è€ƒèµ„æ–™

[https://github.com/QwenLM/Qwen2.5-VL/tree/main](https://github.com/QwenLM/Qwen2.5-VL/tree/main)

[https://github.com/huggingface/trl](https://github.com/huggingface/trl)

[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

[https://www.modelscope.cn/datasets/Tina12345/textVQA_groundingtask_bbox/summary](https://www.modelscope.cn/datasets/Tina12345/textVQA_groundingtask_bbox/summary)

[https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox](https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox)

[https://github.com/modelscope/ms-swift/tree/main](https://github.com/modelscope/ms-swift/tree/main)

[ms-swiftè‡ªå®šä¹‰æ•°æ®é›†æŒ‡å—](https://swift.readthedocs.io/zh-cn/latest/Customization/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86.html)

[Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923#:~:text=We%20introduce%20Qwen2.5-VL%2C%20the%20latest%20flagship%20model%20of,advancements%20in%20both%20foundational%20capabilities%20and%20innovative%20functionalities)

[å¤šæ¨¡æ€å¤§æ¨¡å‹åº”ç”¨å®è·µï¼ˆä¸€ï¼‰- åˆ©ç”¨å¾®è°ƒ LLaVA å®ç°é«˜æ•ˆé…’åº—å›¾ç‰‡åˆ†ç±»](https://aws.amazon.com/cn/blogs/china/multimodal-large-model-application-practice-part-one/)







