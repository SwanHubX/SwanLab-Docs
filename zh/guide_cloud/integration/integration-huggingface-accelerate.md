# ğŸ¤—HuggingFace Accelerate

HuggingFace çš„ [accelerate](https://huggingface.co/docs/accelerate/index) æ˜¯ä¸€ä¸ªç®€åŒ–å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸æ¨ç†çš„å¼€æºåº“ã€‚

> ğŸš€åœ¨å‡ ä¹ä»»ä½•è®¾å¤‡å’Œåˆ†å¸ƒå¼é…ç½®ä¸Šå¯åŠ¨ã€è®­ç»ƒå’Œä½¿ç”¨PyTorchæ¨¡å‹çš„ç®€å•æ–¹æ³•ï¼Œæ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦(åŒ…æ‹¬fp8)ï¼Œä»¥åŠæ˜“äºé…ç½®çš„FSDPå’ŒDeepSpeed

å®ƒæä¾›äº†é«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†çš„å·¥å…·ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´è½»æ¾åœ°åœ¨ä¸åŒç¡¬ä»¶è®¾å¤‡ä¸Šéƒ¨ç½²å’ŒåŠ é€Ÿæ¨¡å‹ã€‚é€šè¿‡ç®€å•çš„å‡ è¡Œä»£ç æ”¹åŠ¨ï¼Œå°±å¯ä»¥è½»æ¾å°†ç°æœ‰çš„è®­ç»ƒä»£ç é›†æˆè¿› `torch_xla` å’Œ `torch.distributed` è¿™ç±»å¹³å°ï¼Œè€Œæ— éœ€ä¸ºå¤æ‚çš„åˆ†å¸ƒå¼è®¡ç®—æ¶æ„çƒ¦æ¼ï¼Œä»è€Œæå‡å·¥ä½œæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

![hf-accelerate-image](./huggingface_accelerate/logo.png)

ä½ å¯ä»¥ä½¿ç”¨`accelerate`å¿«é€Ÿè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä½¿ç”¨SwanLabè¿›è¡Œå®éªŒè·Ÿè¸ªä¸å¯è§†åŒ–ã€‚

> `accelerate`>=1.8.0 çš„ç‰ˆæœ¬ï¼Œå·²å®˜æ–¹é›†æˆäº†swanlab  
> å¦‚æœä½ çš„ç‰ˆæœ¬ä½äº1.8.0ï¼Œè¯·ä½¿ç”¨ **SwanLabTrackeré›†æˆ**


## 1. ä¸¤è¡Œä»£ç å®Œæˆé›†æˆ

```python {4,9}
from accelerate import Accelerator

# å‘Šè¯‰ Accelerator å¯¹è±¡ä½¿ç”¨ swanlab è¿›è¡Œæ—¥å¿—è®°å½•
accelerator = Accelerator(log_with="swanlab")

# åˆå§‹åŒ–æ‚¨çš„ swanlab å®éªŒï¼Œä¼ é€’ swanlab å‚æ•°å’Œä»»ä½•é…ç½®ä¿¡æ¯
accelerator.init_trackers(
    ...
    init_kwargs={"swanlab": {"experiment_name": "hello_world"}}
    )
```

::: warning è¡¥å……ä¿¡æ¯
1. swanlabé¡¹ç›®åç”±`accelerator.init_trackers`çš„`project_name`å‚æ•°æŒ‡å®š
2. å‘`init_kwargs`ä¼ é€’çš„`swanlab`å­—å…¸ï¼Œkey-valueå’Œ`swanlab.init`çš„å‚æ•°å®Œå…¨ä¸€è‡´ï¼ˆé™¤äº†projectï¼‰ã€‚
:::

æœ€å°èƒ½è·‘ä»£ç ï¼š

```python {4,10}
from accelerate import Accelerator

# Tell the Accelerator object to log with swanlab
accelerator = Accelerator(log_with="swanlab")

# Initialise your swanlab experiment, passing swanlab parameters and any config information
accelerator.init_trackers(
    project_name="accelerator",
    config={"dropout": 0.1, "learning_rate": 1e-2},
    init_kwargs={"swanlab": {"experiment_name": "hello_world"}}
    )

for i in range(100):
    # Log to swanlab by calling `accelerator.log`, `step` is optional
    accelerator.log({"train_loss": 1.12, "valid_loss": 0.8}, step=i+1)

# Make sure that the swanlab tracker finishes correctly
accelerator.end_training()
```

## 2. SwanLabTrackeré›†æˆ

å¦‚æœä½ ä½¿ç”¨çš„æ˜¯`accelerate<1.8.0`çš„ç‰ˆæœ¬ï¼Œåˆ™å¯ä»¥ä½¿ç”¨SwanLabCallbacké›†æˆã€‚

### 2.1 å¼•å…¥

```bash
from swanlab.integration.accelerate import SwanLabTracke
```


### 2.2 åœ¨åˆå§‹åŒ–accelerateæ—¶æŒ‡å®šæ—¥å¿—è®°å½•å™¨

```python (1,7,9,12)
from swanlab.integration.accelerate import SwanLabTracker
from accelerate import Accelerator

...

# åˆ›å»ºSwanLabæ—¥å¿—è®°å½•å™¨
tracker = SwanLabTracker("YOUR_SMART_PROJECT_NAME")
# ä¼ å…¥Accelerator
accelerator = Accelerator(log_with=tracker)

# åˆå§‹åŒ–æ‰€æœ‰æ—¥å¿—è®°å½•å™¨
accelerator.init_trackers("YOUR_SMART_PROJECT_NAME", config=config)

# training code
...
```

- è™½ç„¶ä¸Šé¢çš„ä»£ç ä¸¤æ¬¡è®¾å®šäº†é¡¹ç›®åï¼Œå®é™…ä¸Šåªæœ‰ç¬¬ä¸€ä¸ªé¡¹ç›®åè®¾ç½®æ‰èµ·äº†ä½œç”¨

- æ˜¾å¼è°ƒç”¨`init_trackers`æ¥åˆå§‹åŒ–æ‰€æœ‰æ—¥å¿—è®°å½•æ˜¯`accelerate`çš„æœºåˆ¶ï¼Œç¬¬äºŒæ¬¡è®¾ç½®çš„é¡¹ç›®åæ˜¯å½“æœ‰å¤šä¸ªæ—¥å¿—è®°å½•å™¨æ—¶,åˆå§‹åŒ–å†…ç½®çš„æ—¥å¿—è®°å½•å™¨çš„æƒ…å†µä¸‹æ‰ä¼šç”¨åˆ°ã€‚

### 2.3 å®Œæ•´æ¡ˆä¾‹ä»£ç 

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨accelerateè¿›è¡Œcifar10åˆ†ç±»ï¼Œå¹¶ä½¿ç”¨SwanLabè¿›è¡Œæ—¥å¿—è·Ÿè¸ªçš„æ¡ˆä¾‹ï¼š

```python (10,45,46,47,71,90)
import torch
import torch.utils
import torch.utils.data
import torch.utils.data.dataloader
import torchvision
from torchvision.models import resnet18, ResNet18_Weights
from accelerate import Accelerator
from accelerate.logging import get_logger
import swanlab
from swanlab.integration.accelerate import SwanLabTracker


def main():
    # hyperparameters
    config = {
        "num_epoch": 5,
        "batch_num": 16,
        "learning_rate": 1e-3,
    }

    # Download the raw CIFAR-10 data.
    transform = torchvision.transforms.Compose(
        [
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ]
    )
    train_data = torchvision.datasets.CIFAR10(root="./data", train=True, download=True, transform=transform)
    test_data = torchvision.datasets.CIFAR10(root="./data", train=False, download=True, transform=transform)
    BATCH_SIZE = config["batch_num"]
    my_training_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    my_testing_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

    # Using resnet18 model, make simple changes to fit the data set
    my_model = resnet18(weights=ResNet18_Weights.DEFAULT)
    my_model.conv1 = torch.nn.Conv2d(my_model.conv1.in_channels, my_model.conv1.out_channels, 3, 1, 1)
    my_model.maxpool = torch.nn.Identity()
    my_model.fc = torch.nn.Linear(my_model.fc.in_features, 10)

    # Criterion and optimizer
    criterion = torch.nn.CrossEntropyLoss()
    my_optimizer = torch.optim.SGD(my_model.parameters(), lr=config["learning_rate"], momentum=0.9)

    # Init accelerate with swanlab tracker
    tracker = SwanLabTracker("CIFAR10_TRAING")
    accelerator = Accelerator(log_with=tracker)
    accelerator.init_trackers("CIFAR10_TRAING", config=config)
    my_model, my_optimizer, my_training_dataloader, my_testing_dataloader = accelerator.prepare(
        my_model, my_optimizer, my_training_dataloader, my_testing_dataloader
    )
    device = accelerator.device
    my_model.to(device)

    # Get logger
    logger = get_logger(__name__)

    # Begin training

    for ep in range(config["num_epoch"]):
        # train model
        if accelerator.is_local_main_process:
            print(f"begin epoch {ep} training...")
        step = 0
        for stp, data in enumerate(my_training_dataloader):
            my_optimizer.zero_grad()
            inputs, targets = data
            outputs = my_model(inputs)
            loss = criterion(outputs, targets)
            accelerator.backward(loss)
            my_optimizer.step()
            accelerator.log({"training_loss": loss, "epoch_num": ep})
            if accelerator.is_local_main_process:
                print(f"train epoch {ep} [{stp}/{len(my_training_dataloader)}] | train loss {loss}")

        # eval model
        if accelerator.is_local_main_process:
            print(f"begin epoch {ep} evaluating...")
        with torch.no_grad():
            total_acc_num = 0
            for stp, (inputs, targets) in enumerate(my_testing_dataloader):
                predictions = my_model(inputs)
                predictions = torch.argmax(predictions, dim=-1)
                # Gather all predictions and targets
                all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))
                acc_num = (all_predictions.long() == all_targets.long()).sum()
                total_acc_num += acc_num
                if accelerator.is_local_main_process:
                    print(f"eval epoch {ep} [{stp}/{len(my_testing_dataloader)}] | val acc {acc_num/len(all_targets)}")

            accelerator.log({"eval acc": total_acc_num / len(my_testing_dataloader.dataset)})

    accelerator.wait_for_everyone()
    accelerator.save_model(my_model, "cifar_cls.pth")

    accelerator.end_training()


if __name__ == "__main__":
    main()
```
