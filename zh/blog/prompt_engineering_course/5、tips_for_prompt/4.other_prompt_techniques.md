## 零样本提示

在实际使用大模型的时候，尤其是开源的大模型时，我们通常倾向于使用经过许多高质量指令数据微调后的模型，比如Instruct模型进行指令问答，而经过大量数据训练并调整指令的LLM能够执行零样本提示任务。

零样本提示的定义是不提供任何示例，直接让模型完成任务，依赖模型自身的知识和推理能力。一般用于简单的问答，而不对生成的结果有什么格式、内容等要求的情况，我们举一个例子：

*提示词* ：

```Plain
把‘我今天很开心’翻译成英语。仅输出结果
```

*回答* ：

```Plain
I am very happy today.
```

或者：

*提示词* ：

```Plain
请列出红色的三种水果，仅输出结果
```

*回答* ：

```Plain
苹果、草莓、樱桃
```

请注意，在上面的提示中，我们没有向模型提供任何示例——这就是零样本能力的作用。

---

## 少样本提示

虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。

少样本提示（Few-shot Prompting）是指在提示词中提供少量示例（通常 1-5 个），让模型通过模仿示例的模式来完成任务。它比零样本提示更 “具象”，通过示例明确任务规则或格式，帮助模型更精准地理解需求，尤其适合需要特定格式、逻辑或风格的任务。我们举一个简单的例子：

*提示词* ：

```Plain
### 问题
请你根据输入的内容，判断其情绪属于“积极”、“消极”、“中立”的哪一个标签？仅输出标签内容

### 示例
输入：我今天心情很好，工作也很顺利。
输出：积极

输入：我对这部电影的评价很差，觉得浪费时间。
输出：消极

## 用户输入
输入：我对这次旅行的体验还可以，但没有特别惊喜。
输出：
```

*回答* ：

```Plain
中立
```

通过多样本提示，即使任务涉及细微的判断差异（比如区分 “中性事实” 和 “带情绪的描述”），模型也能更稳定地遵循规则，减少偏差。


---

## 思维链(CoT)提示

思维链提示是通过引导模型 `一步步说出推理过程`来解决问题的方式。它不直接要求答案，而是先让模型像人一样 “思考”—— 拆解问题、分析逻辑、逐步推导，最终得出结论。这种方式能帮助模型处理需要多步推理的任务（如数学题、逻辑题），减少因跳跃性思维导致的错误。

我们举一个简单的例子，对比一下普通的零样本提示和思维链（CoT）提示的区别：

*提示词* ：

```Plain
我今年6岁，是妹妹的两倍。那么在我过80岁生日的时候，我的妹妹多少岁？
```

*回答* ：

```Plain
您今年6岁，是您妹妹年龄的两倍。这意味着您妹妹今年3岁（因为6除以2等于3）。
当您过80岁生日时，您妹妹的年龄将是80除以2，即40岁。
所以，当您过80岁生日时，您的妹妹将会是40岁。
```

显而易见，当模型没有自己的思考过程的时候，回答的结果千奇百怪，当我过80岁生日的时候，我妹妹比我小3岁，明显应该是77，但是如果我们在原有的提示词基础上添加了思维链呢？

*提示词* ：

```Plain
我今年6岁，是妹妹的两倍。那么在我过80岁生日的时候，我的妹妹多少岁？让我们一步一步思考
```

*回答* ：

```Plain
好的，我们来一步一步分析这个问题。

1. **确定现在的年龄关系**：你现在6岁，你妹妹的年龄是你的一半，也就是说你妹妹现在3岁。

2. **计算年龄差**：你们之间的年龄差是一样的。所以你和你妹妹的年龄差是6 - 3 = 3岁。

3. **预测未来的情况**：无论时间如何流逝，你们之间的年龄差始终不变。当你80岁时，你妹妹的年龄就是你年龄减去你们之间的年龄差。

4. **具体计算**：当你是80岁的时候，你的妹妹将会是80 - 3 = 77岁。

因此，在你80岁生日的时候，你的妹妹将会是77岁。
```

当模型学会将解答按照步骤拆解，一步一步思考，最终给出的答案的准确度要比零样本提示要高。

不过要注意，并不是所有的问题都适合用该方法，很显然，当你遇到难以解决的数学问题的时候，CoT会有奇效。可如果你仅仅只是问一个很简单的问题，比如“中国的首都在哪？”，让模型一步一步思考显然是一个很浪费时间的行为。

因此可以得出结论，当需要保证结果的准确度、需要解决较为复杂的数学、推理问题的时候，思维链提示是一个好的方法，其他情况可以酌情考虑。





