## 模型参数设置

无论是本地模型推理，还是API调用大模型进行交互，你都需要通过配置一些参数以获得不同的提示结果。调整这些设置对于提高响应的可靠性非常重要，你可能需要进行一些实验才能找出适合您的用例的正确设置。以下是使用不同LLM提供程序时会遇到的常见设置：

- **max length**：max length用于指定大模型上下文文本的总长度上限，包括输入文本（prompt）和生成的新文本两部分。但是一般不用这个参数，因为限制为输入和输出总长，如果超过这个范围，会报错，而一般你并不能预测到能生成多少内容，所以一般用**max new tokens**比较多。
- **max new tokens**：指定生成文本的新生成部分的长度上限，不包括输入文本。

<div style="background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;">   ℹ️ <strong>需要注意的是</strong><br/>   上面两个参数是通用参数，在<a href="https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py#L104" target="_blank" rel="noopener">transformers官方代码</a>上，本地推理的时候基本名称不变，但是使用API的时候由于模仿了ChatGPT的API格式，最大生成文本长度限制可能是`max_tokens`，因此在实际使用的时候需要注意。 </div>

- **temperature**：用于控制生成文本的随机性和创造性。它影响模型在预测下一个 token 时对概率分布的处理方式。temperature范围在[0,1]之间，参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。在实际应用方面，对于质量保障（QA）等任务，我们可以设置更低的 temperature 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，适度地调高 temperature 参数值可能会更好。
- **top_p**：也称为核采样，Nucleus Sampling，是一种用于控制生成文本随机性的策略参数，它与之前提到的 temperature 类似，但采用了不同的机制来筛选下一个可能的 token。如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。top_p 是一个浮点数（通常范围为 0.0~1.0），用于动态选择概率最高的 token 子集，使得该子集中所有 token 的概率之和至少达到 top_p。
- **top_k**：top_k（Top-K 采样） 是一种控制生成文本随机性的策略参数，它通过限制模型在预测下一个 token 时的选择范围，来平衡生成内容的多样性和可靠性。top_k 是一个整数（通常为 1~100），用于指定在生成下一个 token 时，只从概率最高的 前 k 个候选 token 中进行采样。如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。

<div style="background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;">   一般建议是改变temperature、top_p、top_k其中一个参数就行，不用都调整，因为都是为了控制回答的多样性。<br/></div>

- **do_sample**：用于控制生成过程中是否使用随机采样策略。它是决定 temperature、top_k、top_p 等随机性参数是否生效的开关，默认是False。do_sample=True：启用随机采样，根据 token 概率分布进行随机选择，生成结果具有多样性。do_sample=False：禁用随机采样，采用贪婪搜索（Greedy Search），每次直接选择概率最高的 token，生成结果确定性强。

<div style="background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;">   实际使用的时候不一定要设置如此多的参数，每个参数都有默认参数，生成在多样性上是有保证的，我们只需要调整输入以及max length或者max new tokens即可<br/></div>