# ä½¿ç”¨ChatGLM4è¿›è¡Œå¤§æ¨¡å‹æŒ‡ä»¤éµä»å¾®è°ƒï¼ˆé™„ä»£ç å’Œæµ‹è¯•è„šæœ¬ï¼‰

ä½œè€…ï¼šæƒ…æ„Ÿæœºå™¨å®éªŒå®¤-é™ˆå°‘å® é‚®ç®±ï¼š<shaohon_chen@115lab.club>

## æ‘˜è¦

æœ¬æ•™ç¨‹ä¸»è¦å®ç°äº†ä¸€ä¸ªå¤§æ¨¡å‹çš„æŒ‡ä»¤éµä»å¾®è°ƒæ–¹æ³•ã€‚ä¸ºäº†ä¾¿äºå®ç°ï¼Œå‡å°‘ä»£ç é‡ï¼Œæœ¬æ–‡ä½¿ç”¨äº†ğŸ¤—HuggingFaceçš„TRLæ¡†æ¶å®ç°ã€‚è¯¥æ¡†æ¶é™¤äº†æ”¯æŒSFTå¤–ï¼Œå¯¹DPOã€PPOã€GRPOç­‰æµè¡Œçš„å¼ºåŒ–å¾®è°ƒç®—æ³•éƒ½æœ‰å¾ˆå¥½çš„æ”¯æŒã€‚

è™½ç„¶ä½¿ç”¨æ¡†æ¶èƒ½å¤Ÿæå¤§çš„å‡å°‘å·¥ä½œé‡ï¼Œä½†æ˜¯ä¸å¯é¿å…çš„ä¸ºæ–°æ‰‹å­¦ä¹ å¸¦æ¥äº†å›°æ‰°ã€‚å› æ­¤æœ¬æ•™ç¨‹ä¼šå°½é‡é™„ä¸Šå®Œæ•´çš„æ–‡æ¡£å¼•ç”¨æ¥å¸®åŠ©è¯»è€…è¿›ä¸€æ­¥å­¦ä¹ æ¡†æ¶ã€‚è¯šç„¶ä»ä½¿ç”¨pytorchå®ç°å¾®è°ƒè¿‡ç¨‹èƒ½å¤Ÿæå¤§çš„æå‡å¯¹è¿‡ç¨‹çš„ç†è§£ï¼Œç¤¾åŒºä¹Ÿæœ‰ç›¸å½“å¤šä¼˜ç§€çš„é¡¹ç›®ã€‚ä½†æ˜¯ç¬”è€…ä»æ¨èå¤§å®¶å¤šä½¿ç”¨æ¡†æ¶æ¥å®Œæˆè®­ç»ƒï¼Œè¿™æ ·å¯ä»¥å‡å°‘å¤§é‡çš„æ—¶é—´æ¥è®©å¤§å®¶æ›´ä¸“æ³¨äºåˆ›æ–°ã€‚

å› æ­¤æœ¬æ•™ç¨‹å»ºè®®å¯¹ğŸ¤—HuggingFace Transformersæ¡†æ¶æœ‰ä¸€å®šåŸºç¡€çš„è¯»è€…é˜…è¯»ï½ã€‚

æ³¨æ„ï¼šç”±äºChatGLMçš„æ¨¡å‹ç›¸å¯¹è¾ƒå¤§ï¼Œå®é™…è¿è¡Œå¤§æ¦‚éœ€è¦æ˜¾å­˜>=16G

ğŸ‰ **SwanLabè¢«å®˜æ–¹é›†æˆè¿›å…¥äº†ğŸ¤—HuggingFace Transformersï¼š** å¦‚æœæœ¬åœ°ç¯å¢ƒå®‰è£…äº†SwanLabä¼šé»˜è®¤å¼€å¯ï¼ä¹Ÿå¯ä»¥é€šè¿‡`report_to="swanlab"`å¼€å¯è®­ç»ƒè·Ÿè¸ªã€‚

## ç›®å½•

**ç›®å½•ï¼š**

* [TRLåŒ…ä»‹ç»+ç¯å¢ƒå‡†å¤‡](#trlåŒ…ä»‹ç»ç¯å¢ƒå‡†å¤‡)

* [ChatGLM4ä»‹ç»+æ¨¡å‹å‡†å¤‡](#ChatGLM4ä»‹ç»+æ¨¡å‹å‡†å¤‡)

* [æ•°æ®é›†å‡†å¤‡](#æ•°æ®é›†å‡†å¤‡)

* [ä»£ç è¯´æ˜è¶…å‚æ•°è°ƒæ•´](#ä»£ç è¯´æ˜è¶…å‚æ•°è°ƒæ•´)

* [å¯åŠ¨è®­ç»ƒæ•ˆæœè¯„æµ‹](#å¯åŠ¨è®­ç»ƒæ•ˆæœè¯„æµ‹)

* [é™„ä»¶å®Œæ•´ä»£ç ](#é™„ä»¶å®Œæ•´ä»£ç )

**å‚è€ƒèµ„æ–™ï¼š**

* æ™ºè°±AIå®˜ç½‘ï¼š[https://www.zhipuai.cn/](https://www.zhipuai.cn/)

* ChatGLM-9BåŸºåº§æ¨¡å‹ï¼š[https://huggingface.co/THUDM/glm-4-9b-hf](https://huggingface.co/THUDM/glm-4-9b-hf/tree/main)

* ChatGLM-9B-Chatæ¨¡å‹ï¼š[https://huggingface.co/THUDM/glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf/tree/main)

* Alpacaæ•°æ®é›†ä¸­æ–‡ç‰ˆï¼š[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)

* æœ¬åšå®¢å¼€æºé¡¹ç›®é“¾æ¥ï¼š[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)

* SwanLabè®­ç»ƒæ—¥å¿—æŸ¥çœ‹ï¼š[https://swanlab.cn/@ShaohonChen/chatglm-finetune/](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)

## TRLåŒ…ä»‹ç»+ç¯å¢ƒå‡†å¤‡

![trl](./images/glm4-instruct/trl.png)

æœ¬æ•™ç¨‹ä½¿ç”¨[ğŸ¤—HuggingFace TRL](https://huggingface.co/docs/trl/index)æ¡†æ¶æ¥å®Œæˆå¾®è°ƒä»£ç çš„å®ç°ã€‚TRLæ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”ä¾¿äºä½¿ç”¨çš„å¾®è°ƒæ¡†æ¶ï¼Œé™¤äº†æ”¯æŒSFTå¤–ï¼Œä¹Ÿèƒ½è½»æ¾çš„é€šè¿‡æ¥å£è°ƒç”¨DPOã€PPOã€GRPOç­‰æµè¡Œçš„å¼ºåŒ–å¾®è°ƒç®—æ³•ã€‚æ­¤å¤–ä¹Ÿå®Œç¾å…¼å®¹Transformersæ¶æ„ã€‚

é¦–å…ˆæ˜¯å®‰è£…æœ¬æ•™ç¨‹çš„ç¯å¢ƒï¼Œå®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
pip install transformers trl datasets peft swanlab
```

å…¶ä¸­`transformers trl peft`ç”¨äºæ¨¡å‹çš„åŠ è½½å’Œè®­ç»ƒï¼Œ`datasets`ç”¨äºå¯¼å…¥æ•°æ®é›†ï¼Œ`swanlab`ç”¨äºå¯¹è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–è·Ÿè¸ªã€‚

ä¸‹é¢åˆ—ä¸¾ä¸€ä¸ªç®€å•çš„å¾®è°ƒæ¡ˆä¾‹æ¥ä»‹ç»HF TRLæ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•ï¼š

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset("stanfordnlp/imdb", split="train")   # è®¾ç½®å¾®è°ƒæ•°æ®é›†ï¼Œæ­¤å¤„ä½¿ç”¨IMDBç”µå½±è¯„è®ºåˆ†ç±»æ•°æ®

training_args = SFTConfig(  # è®¾ç½®å¾®è°ƒå‚æ•°
    max_length=512,
    output_dir="/tmp",
)
trainer = SFTTrainer(   # è®¾ç½®æ¨¡å‹ï¼Œæ­¤å¤„ä½¿ç”¨facebookçš„opt-350Mï¼Œå‚æ•°é‡æ¯”è¾ƒå°ä¾¿äºä¸‹è½½
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
)
trainer.train() # å¼€å§‹è®­ç»ƒï¼Œæµç¨‹å’ŒTRLä¸€æ ·
```

ä¸Šé¢çš„ä»£ç æ¥è‡ªHFå®˜æ–¹æ–‡æ¡£[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)ï¼Œå¢åŠ äº†æ³¨é‡Šä¾¿äºè¯»è€…ç†è§£ã€‚

ç®€å•æ¥è¯´TRLåŒ…çš„ä½¿ç”¨æ–¹æ³•å’ŒTransformersç±»ä¼¼ï¼Œä¸è¿‡å¤šäº†ä¸¤æ­¥ï¼š

* å¯¼å…¥`SFTConfig`æ¨¡å—ï¼Œè¿™ä¸ªæ¨¡å—åŸºäº`transformers`çš„`TrainingArguments`ï¼Œä¸è¿‡é’ˆå¯¹SFTå¼•å…¥äº†ä¸€ç‚¹é¢å¤–çš„å‚æ•°ï¼Œä»¥åŠloraçš„æ”¯æŒå‚æ•°

* å¯¼å…¥`SFTTrainer`æ¨¡å—ï¼Œè¿™ä¸ªæ¨¡å—åŒ…å«äº†SFTçš„ä»£ç å®ç°ï¼Œè¿˜æœ‰ä¸€äº›å¯¹`peft`çš„loraæ”¯æŒå’Œæ•°æ®é›†æ ¼å¼è½¬æ¢ä»£ç ã€‚

åæ–‡å°†å®Œæ•´çš„ä»‹ç»å¦‚ä½•ä½¿ç”¨TRLåŒ…å®Œæˆå¤§æ¨¡å‹çš„æŒ‡ä»¤éµä»åŠŸèƒ½ã€‚

## ChatGLM4ä»‹ç»+æ¨¡å‹å‡†å¤‡

![chatglm_history](images/glm4-instruct/chatglm_history.png)

GLM-4-9Bæ˜¯[æ™ºè°±AI](https://www.zhipuai.cn/)æ¨å‡ºçš„æœ€æ–°ä¸€ä»£é¢„è®­ç»ƒæ¨¡å‹GLM-4ç³»åˆ—ä¸­çš„å¼€æºç‰ˆæœ¬ã€‚ChatGLMå‘å¸ƒäº†å¤šä¸ªç‰ˆæœ¬ï¼Œå…¶ä¸­GLM-4-9Bæ˜¯ç¬¬å››ä»£åŸºåº§æ¨¡å‹ï¼Œå…¶å¾®è°ƒç‰ˆæœ¬GLM-4-9B-Chatå…·å¤‡ç½‘é¡µæµè§ˆã€ä»£ç æ‰§è¡Œã€è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰å’Œé•¿æ–‡æœ¬æ¨ç†ï¼ˆæ”¯æŒæœ€å¤§ 128K ä¸Šä¸‹æ–‡ï¼‰ç­‰é«˜çº§åŠŸèƒ½ã€‚

æœ¬æ•™ç¨‹ä½¿ç”¨GLM-4-9Bæ¨¡å‹è¿›è¡ŒæŒ‡ä»¤éµä»åŠŸèƒ½å¾®è°ƒï¼Œå¹¶ä½¿ç”¨SwanLabè¿›è¡Œæ¨¡å‹çš„ç»“æœè·Ÿè¸ªã€‚

âš ï¸æ³¨æ„ï¼šChatGLMä¸ºäº†é…åˆHuggingface Transformersæ›´æ–°ï¼Œå‘å¸ƒäº†ä¸¤ä¸ªç‰ˆæœ¬æƒé‡`THUDM/glm-4-9b`å’Œ`THUDM/glm-4-9b-hf`ï¼Œåè€…å¯¹åº”æ›´ä¸ºæ–°ç‰ˆæœ¬çš„transformersï¼Œå› æ­¤æœ¬æ•™ç¨‹ä½¿ç”¨åè€…çš„æƒé‡ã€‚

æœ¬æ•™ç¨‹ä»¥ç»æä¾›å¥½äº†ä¸‹è½½æ¨¡å‹çš„è„šæœ¬ï¼Œä¸‹è½½æ¨¡å‹çš„æ–¹æ³•å¦‚ä¸‹ï¼š

```bash
huggingface-cli download --local-dir ./weights/glm-4-9b-hf THUDM/glm-4-9b-hf
```

æ¨¡å‹å°†ä¼šä¸‹è½½åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„`./weights/glm-4-9b-hf`ä¸­

ä¸‹é¢åˆ—ä¸¾ä¸€ä¸ªä½¿ç”¨`transformers`åŠ è½½ChatGLMæ¨¡å‹å¹¶è¿›è¡Œæ¨ç†çš„ä»£ç ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
device = "cuda"
tokenizer = AutoTokenizer.from_pretrained("THUDM/glm-4-9b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("THUDM/glm-4-9b-chat-hf").eval().to(device)
inputs = tokenizer.encode("æˆ‘æ˜¯ChatGLMï¼Œæ˜¯", return_tensors="pt").to(device)
outputs = model.generate(inputs)
print(tokenizer.decode(outputs[0]))
```

ç”±äºæ˜¯åŸºåº§æ¨¡å‹ï¼Œæ²¡ç»è¿‡å¾®è°ƒï¼Œå› æ­¤æ¨¡å‹åªä¼šå®Œæˆ`"æˆ‘æ˜¯ChatGLMï¼Œæ˜¯"`è¿™æ®µæ–‡æœ¬çš„åç»­è¡¥å…¨ï¼Œè¿è¡Œåä¼šç”Ÿæˆå¦‚ä¸‹ä»£ç ï¼š

```bash
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.35it/s]
[gMASK]<sop>æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹
```

å½“ç„¶ä¸Šé¢çš„ä¾‹å­æ˜¯ä¸€ä¸ªåŸºåº§æ¨¡å‹æ¨ç†çš„ä¾‹å­ï¼Œè¯¥æ¨¡å‹åªèƒ½è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¦‚æœå¸Œæœ›ä½¿ç”¨å¯¹è¯èƒ½åŠ›ï¼Œè¿˜æ˜¯éœ€è¦åŠ è½½å·²ç»å¾®è°ƒå¥½çš„å¯¹è¯æ¨¡å‹ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
from transformers import pipeline

messages = [
    {"role": "user", "content": "ä½ æ˜¯è°"},
]
pipe = pipeline("text-generation", model="THUDM/glm-4-9b-chat-hf")
print(pipe(messages))
```

æ­¤å¤„æˆ‘ä»¬æ¢äº†ç§æ¨ç†æ¥å£ï¼Œç›´æ¥ä½¿ç”¨pipelineå®Œæˆæ¨ç†ï¼Œè¿è¡Œåå°†ä¼šç”Ÿæˆå¦‚ä¸‹ä¿¡æ¯

```bash
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.24it/s]
Device set to use cuda:0
[{'generated_text': [{'role': 'user', 'content': 'ä½ æ˜¯è°'}, {'role': 'assistant', 'content': '\næˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåä¸º ChatGLMã€‚æˆ‘æ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œ'}]}]
```

ä½¿ç”¨`print(model)`å°†æ¨¡å‹çš„ç»“æ„æ‰“å°å‡ºæ¥ï¼Œå±•ç¤ºå¦‚ä¸‹ï¼š

```text
GlmForCausalLM(
  (model): GlmModel(
    (embed_tokens): Embedding(151552, 4096, padding_idx=151329)
    (layers): ModuleList(
      (0-39): 40 x GlmDecoderLayer(
        (self_attn): GlmAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (k_proj): Linear(in_features=4096, out_features=256, bias=True)
          (v_proj): Linear(in_features=4096, out_features=256, bias=True)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): GlmMLP(
          (gate_up_proj): Linear(in_features=4096, out_features=27392, bias=False)
          (down_proj): Linear(in_features=13696, out_features=4096, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)
        (post_attention_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)
      )
    )
    (norm): GlmRMSNorm((4096,), eps=1.5625e-07)
    (rotary_emb): GlmRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=151552, bias=False)
)
```

å¯ä»¥çœ‹åˆ°GLMæ¨¡å‹çš„å±‚æ•°è¾¾åˆ°äº†æƒŠäººçš„40å±‚ğŸ˜‚ï¼Œå› æ­¤æœ¬èº«ä½¿ç”¨Loraè¿›è¡Œå¾®è°ƒæ—¶å…¶å¯è®­ç»ƒå‚æ•°ä¼šæ¯”å…¶ä»–æ¨¡å‹å¤§ä¸€äº›ã€‚

## æ•°æ®é›†å‡†å¤‡

æ•°æ®é›†æˆ‘å·²ç»æå‰åŒ…æ‹¬åœ¨äº†githubé¡¹ç›®å½“ä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ä¸‹è½½å®Œæ•´çš„å®éªŒä»£ç 

```bash
git clone https://github.com/SwanHubX/glm4-finetune.git
```

å¦‚æœåªæƒ³ä¸‹è½½æ•°æ®é›†ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½å¦‚ä¸‹æ–‡ä»¶ï¼š

```bash
wget https://github.com/SwanHubX/glm4-finetune/blob/main/data/alpaca_gpt4_data_zh.json
```

ä¹Ÿå¯ä»¥é€šè¿‡ğŸ¤—huggingfaceä¸Šä¸‹è½½ï¼š[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)

## ä»£ç è¯´æ˜+è¶…å‚æ•°è°ƒæ•´

å®Œæ•´çš„å¾®è°ƒä»£ç å…¬å¼€åœ¨äº†GitHubä¸Šï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å³å¯ä¸‹è½½

```bash
git clone https://github.com/SwanHubX/glm4-finetune.git
```

æ–‡ç« çš„é™„ä»¶ä¸­ä¹Ÿæœ‰å®Œæ•´çš„å®ç°ä»£ç [#ä»£ç é™„ä»¶](#é™„ä»¶å®Œæ•´ä»£ç )

æœ¬æ–‡æ¥ä¸‹æ¥é‡ç‚¹ä»‹ç»å„ä¸ªä»£ç çš„åŠŸèƒ½æ¨¡å—

åŠ è½½æ¨¡å‹çš„è¶…å‚æ•°è®¾ç½®ï¼Œè¿™é‡Œå¯ä»¥é‡ç‚¹å…³æ³¨loraå‚æ•°çš„è®¾ç½®ï¼Œæœ¬æ–‡loraå‚æ•°å‚è€ƒäº†ChatGLMå®˜æ–¹å¾®è°ƒä»£ç çš„loraå‚æ•°è®¾ç½®

è¿™é‡Œè¦æ³¨æ„å­¦ä¹ ç‡ä¸º5e-4ï¼Œå¦‚æœæ˜¯å…¨é‡å¾®è°ƒè¦å°ä¸€ä¸ªæ•°é‡çº§ã€‚

```python
################
# Model kwargs
################
@dataclass
class ChatGLM4ModelConfig(ModelConfig):
    model_name_or_path: Optional[str] = field(
        default="./weights/glm-4-9b-hf",
        metadata={
            "help": "Model checkpoint for weights initialization. default used glm4"
        },
    )
    torch_dtype: Optional[str] = field(
        default="bfloat16",
        metadata={
            "help": "Override the default `torch.dtype` and load the model under this dtype.",
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    use_peft: bool = field(
        default=True,
        metadata={"help": "Whether to use PEFT for training. Default true"},
    )
    lora_r: int = field(
        default=8,
        metadata={"help": "LoRA R value."},
    )
    lora_alpha: int = field(
        default=32,
        metadata={"help": "LoRA alpha."},
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRA dropout."},
    )
    lora_target_modules: Optional[list[str]] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj"],
        metadata={"help": "LoRA target modules."},
    )
```

æ•°æ®é›†è¶…å‚æ•°è®¾ç½®ï¼Œè¿™é‡Œæ¯”è¾ƒç®€å•ï¼Œåªæ˜¯åŠ è½½äº†æœ¬åœ°çš„æ•°æ®é›†

```python
################
# Datasets kwargs
################
@dataclass
class DataTrainingArguments:
    data_files: Optional[str] = field(
        default="./data/alpaca_gpt4_data_zh.json.json",
        metadata={"help": "The name of the dataset to use (via the datasets library)."},
    )
```

ä¸è¿‡ä¸ºäº†æ–¹ä¾¿è¯»è€…ç†è§£æ•°æ®é›†é•¿ä»€ä¹ˆæ ·ï¼Œä»æ—§æä¾›æ•°æ®é›†å±•ç¤ºè„šæœ¬

```python
import datasets
raw_dataset=datasets.load_dataset("json", data_files="data/glaive_toolcall_zh_1k.json")
print(raw_dataset)
"""æ‰“å°å†…å®¹
DatasetDict({
    train: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 42677
    })
})
"""
```

å¯ä»¥çœ‹åˆ°æ•°æ®ä¸€å…±æœ‰1000æ¡ï¼Œå¹¶ä¸”åŒ…æ‹¬`'conversations', 'tools'`ä¸¤ä¸ªå­—æ®µ

è¿›ä¸€æ­¥é€‰å–å…¶ä¸­ä¸€æ¡æ‰“å°ï¼š

```python
print(raw_dataset["train"][0])
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```json
{
    "instruction": "ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚",
    "input": "",
    "output": "ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š\n\n1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚\n\n2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚\n\n3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚"
}
```

è¿™é‡Œå¤§å®¶ä¼šæ³¨æ„åˆ°ä¸ºä»€ä¹ˆä¼šæœ‰Instructå’Œinputä¸¤éƒ¨åˆ†ã€‚å®é™…ä¸Šæ—©æœŸé’ˆå¯¹æŒ‡ä»¤éµä»çš„ç ”ç©¶æ˜¯ä¸ºäº†è·å¾—ä¸€ä¸ªé€šç”¨çš„ä»»åŠ¡å¤„ç†æ¨¡å‹ï¼ˆæ¯”å¦‚æ—¢èƒ½åšç¿»è¯‘åˆèƒ½åšè®¡ç®—è¿™æ ·ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬é€šå¸¸æŠŠå¯¹ä»»åŠ¡çš„æè¿°æ”¾åˆ°instructä¸­ï¼Œå°†å®é™…çš„ä»»åŠ¡æ–‡æœ¬æ”¾åœ¨inputä¸­ã€‚
ä½†æ˜¯éšç€ChatGPTè¿™ç§é€šç”¨çš„AIåŠ©ç†å‡ºç°ï¼Œå¤§å®¶å·²ç»é€æ¸ä¹ æƒ¯ç›´æ¥ä¸‹æŒ‡ä»¤è®©å…¶æ‰§è¡Œäº†ã€‚å› æ­¤instructå’Œpromptçš„è¿™ç§åˆ†ç¦»å°±æ˜¾å¾—æ²¡é‚£ä¹ˆæœ‰å¿…è¦äº†ã€‚å®é™…ä¸Šæ— è®ºåˆ†ç¦»å’Œä¸åˆ†ç¦»æ¨¡å‹çš„æœ¬è´¨éƒ½æ˜¯æ ¹æ®å‰æ–‡è¡¥åæ–‡ã€‚å› æ­¤åˆ†ç¦»ä¸åˆ†ç¦»å¯¹æ¨¡å‹çš„æœ€ç»ˆç»“æœä¸ä¼šæœ‰å¤ªå¤§å½±å“ï¼Œæ— éå°±æ˜¯æ ¼å¼çš„ä¸åŒã€‚
ç°åœ¨çš„å¼€æºChatå¤§è¯­è¨€æ¨¡å‹æµè¡ŒæŠŠâ€œäººè®¾â€æ”¾åœ¨â€œsystem promptâ€ä¸­ï¼ŒæŠŠç”¨æˆ·çš„æŒ‡ä»¤æ”¾åœ¨inputä¸­ï¼Œå› æ­¤åæ–‡æˆ‘ä»¬ä¼šå°†Alpacaæ•°æ®é›†å¤„ç†æˆæ›´é€‚åº”äºä¸»æµChatçš„æ ¼å¼ã€‚

ChatGLMæä¾›çš„æ¨èè¾“å…¥å¾®è°ƒæ•°æ®ç»“æ„å¦‚ä¸‹ï¼š

```json
{
  "messages": [
    {
      "role": "user",
      "content": "ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ€§æ„Ÿ"
    },
    {
      "role": "assistant",
      "content": "3x1çš„è¿™æ¬¾ç‰›ä»”è£¤é‡‡ç”¨æµ…ç™½çš„ç‰›ä»”é¢æ–™ä¸ºè£¤èº«æè´¨ï¼Œå…¶æŸ”ç„¶çš„æ‰‹æ„Ÿå’Œç»†è…»çš„è´¨åœ°ï¼Œåœ¨ç©¿ç€èˆ’é€‚çš„åŒæ—¶ï¼Œé€éœ²ç€æ¸…çº¯ç”œç¾çš„ä¸ªæ€§æ°”è´¨ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæµç•…çš„è£¤èº«å‰ªè£å°†æ€§æ„Ÿçš„è…¿éƒ¨æ›²çº¿å½°æ˜¾çš„æ·‹æ¼“å°½è‡´ï¼Œä¸å¤±ä¸ºä¸€æ¬¾éšæ€§å‡ºè¡—çš„å¿…å¤‡å•å“ã€‚"
    }
  ]
}
```

è¿™é‡Œå¯èƒ½æœ‰ä¸€å®šç»éªŒçš„è¯»è€…ä¼šè¯´ï¼Œä¸å¯¹å‘€ï¼Œæˆ‘ä»¬ä»0è®­ç»ƒæˆ‘ä»¬å½“ç„¶å¯ä»¥å®šä¹‰è‡ªå·±çš„æ•°æ®ç»“æ„ã€‚è¿™ä¹ˆæƒ³æ˜¯å¯¹çš„ï¼Œä½†æ˜¯è®©æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä½¿ç”¨ChatGLMåŸç”Ÿçš„`chat_template`ï¼Œæˆ‘è¿˜æ˜¯å»ºè®®å’±ä»¬éµå®ˆchatglmå®˜æ–¹å®šä¹‰çš„æ•°æ®æ ¼å¼ï¼Œè¿™ä¹ˆåšçš„è¯æ—¢èƒ½å…¼å®¹ChatGLMçš„å¾ˆå¤šå·¥å…·ï¼Œåˆèƒ½å……åˆ†åˆ©ç”¨å®˜æ–¹å®šä¹‰çš„special_tokenã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡HuggingFaceä¸Šå¼€æºçš„`glm-4-9b-chat-hf`çš„`tokenizer_config.json`ä¸­å¯ä»¥æ‰¾åˆ°ä»–ä»¬çš„åŸç”Ÿ`chat_template`ï¼Œä¸‹é¢çš„è„šæœ¬æä¾›ä¸€ä¸ªæ‰“å°`chat_template`çš„ä»£ç 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
device = "cuda"
tokenizer = AutoTokenizer.from_pretrained("THUDM/glm-4-9b-chat-hf")
print(tokenizer.chat_template)
```

è·å–tokenizeré…ç½®çš„é“¾æ¥[https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json](https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json)

è¿™é‡Œæˆ‘ä»¬ç®€å•æ‰“å°ä¸€ä¸‹è½¬æ¢å®Œæˆåæ•°æ®é›†æœ€ç»ˆçš„ä¸€ä¸ªæ•ˆæœï¼Œå‚è€ƒè„šæœ¬å¦‚ä¸‹ï¼š

```python
def formatting_func(example):
    """
    process data format
    """
    prompt = example["instruction"]
    if len(example["input"]) != 0:
        prompt += "\n\n" + example["input"]
    conversations = [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": example["output"]},
    ]
    output_text = tokenizer.apply_chat_template(
        conversation=conversations, tokenize=False
    )
    return output_text
```

è¾“å‡ºæ•ˆæœå¦‚ä¸‹ï¼Œä»¥ä¸‹å­—æ®µä¾¿æ˜¯å®é™…è¿ç”¨äºæ¨¡å‹å¾®è°ƒæ—¶ï¼Œè¾“å…¥ç»™æ¨¡å‹çš„æ•°æ®æ ·å¼ï¼š

```text
[gMASK]<sop><|user|>
ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚<|assistant|>
ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š

1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚

2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚

3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚
```

æœ€åä¾¿æ˜¯è®­ç»ƒçš„è¶…å‚æ•°è®¾ç½®å’Œè®­ç»ƒè¿‡ç¨‹çš„å®ç°ï¼Œè¿™é‡Œç”±äºæ•°æ®è§„æ¨¡æ¯”è¾ƒå°ï¼Œæˆ‘ä»¬è®­ç»ƒ600ä¸ªstepsï¼Œæ¯ä¸ªGPUå®é™…batchå¤§å°ä¸º1*4ï¼š

```python
################
# Train kwargs
################
@dataclass
class MySFTConfig(SFTConfig):
    output_dir: Optional[str] = field(
        default="./output/lora-glm4-9b-alpaca",
        metadata={
            "help": "The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided."
        },
    )
    num_train_epochs: float = field(
        default=3.0, metadata={"help": "Total number of training epochs to perform."}
    )
    per_device_train_batch_size: int = field(
        default=2,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for training."},
    )
    per_device_eval_batch_size: int = field(
        default=4,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation."},
    )
    gradient_accumulation_steps: int = field(
        default=1,
        metadata={
            "help": "Number of updates steps to accumulate before performing a backward/update pass."
        },
    )
    learning_rate: float = field(
        default=5e-4, metadata={"help": "The initial learning rate for AdamW."}
    )
    bf16: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"
                " architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change."
            )
        },
    )
    bf16_full_eval: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may"
                " change."
            )
        },
    )
    max_seq_length: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated "
            "from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the "
            "sequence length."
        },
    )
    eval_strategy: Union[str] = field(
        default="steps",
        metadata={"help": "The evaluation strategy to use."},
    )
    eval_steps: Optional[float] = field(
        default=0.1,
        metadata={
            "help": (
                "Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    logging_steps: float = field(
        default=10,
        metadata={
            "help": (
                "Log every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    save_steps: float = field(
        default=0.1,
        metadata={
            "help": (
                "Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
```

è®­ç»ƒçš„æµç¨‹è¿™å—å¦‚ä¸‹,ä½¿ç”¨HF TRLåæµç¨‹å˜å¾—éå¸¸ç®€æ´ã€‚

```python
################
# Training
################
trainer = SFTTrainer(
    model=model_args.model_name_or_path,
    args=training_args,
    data_collator=None,
    train_dataset=raw_datasets["train"],
    eval_dataset=(
        raw_datasets["test"] if training_args.eval_strategy != "no" else None
    ),
    processing_class=tokenizer,
    peft_config=get_peft_config(model_args),
    formatting_func=formatting_func,
    callbacks=[SavePredictCallback()],
)
trainer.train()
```

## å¯åŠ¨è®­ç»ƒ+æ•ˆæœè¯„æµ‹

æœ¬ä»£ç åœ¨å®ç°è®­ç»ƒæ—¶é»˜è®¤æ˜¯å¼€å¯[SwanLab](https://swanlab.cn)çš„ã€‚SwanLabè¢«å®˜æ–¹é›†æˆè¿›å…¥äº†ğŸ¤—HuggingFace Transformersã€‚å¯ä»¥é€šè¿‡`report_to="swanlab"`å¼€å¯è®­ç»ƒè·Ÿè¸ªã€‚å¦‚æœæœ¬åœ°ç¯å¢ƒå®‰è£…äº†SwanLabä¼šé»˜è®¤å¼€å¯ï¼

å¯åŠ¨è®­ç»ƒçš„å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
python instruct_train.py
```

å¯ä»¥çœ‹åˆ°å¦‚ä¸‹å¯åŠ¨ä¿¡æ¯

![train](images/glm4-instruct/train.png)

å¦‚æœæ²¡ç™»å½•SwanLabå¯èƒ½ä¼šå¼¹å‡ºç™»å½•æç¤ºï¼Œè¿™é‡Œæ¨èé€‰æ‹©1å¹¶åœ¨[https://swanlab.cn](https://swanlab.cn)å®Œæˆæ³¨å†Œã€‚å³å¯åœ¨çº¿æŸ¥çœ‹åˆ°è®­ç»ƒè¿›å±•ã€‚

ç™»é™†å‘½ä»¤å¦‚ä¸‹

```bash
swanlab login
```

ç‚¹å‡»æ‰“å°å‡ºçš„é“¾æ¥å³å¯é€šè¿‡çœ‹æ¿æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š

![swanlab](images/glm4-instruct/swanlab.png)

é€šè¿‡é…ç½®`callback`ï¼ŒSwanLabè¿˜èƒ½è‡ªåŠ¨è®°å½•æ¨¡å‹çš„é¢„æµ‹è¾“å‡ºï¼Œä»£ç å’Œæ•ˆæœå¦‚ä¸‹ï¼š

```python
################
# Print prediction text callback
################
class SavePredictCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_save(self, args, state, control, model, processing_class, **kwargs):
        if state.is_world_process_zero:
            tokenizer = processing_class
            batch_test_message = [
                [{"role": "user", "content": "ä½ å¥½ï¼Œå‘Šè¯‰æˆ‘ä½ çš„åå­—ã€‚"}],
                [{"role": "user", "content": "å‘Šè¯‰æˆ‘1+2ç­‰äºå¤šå°‘ï¼Ÿ"}],
            ]
            batch_inputs_text = tokenizer.apply_chat_template(
                batch_test_message,
                return_tensors="pt",
                return_dict=True,
                padding=True,
                padding_side="left",
                add_generation_prompt=True,
            ).to(model.device)

            # print(batch_inputs_text)
            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)
            batch_reponse = tokenizer.batch_decode(
                outputs, skip_special_tokens=False
            )
            log_text_list = [swanlab.Text(response) for response in batch_reponse]
            swanlab.log({"Prediction": log_text_list}, step=state.global_step)
```

![swanlab-text](images/glm4-instruct/swanlab-text.png)

**å¤šå¡å®éªŒ**

å¦‚æœä½ çš„å¡æ•°æ¯”è¾ƒå¤šï¼Œæ¨èä½¿ç”¨å¤šå¡è®­ç»ƒæ¥æå¤§æå‡è®­ç»ƒé€Ÿåº¦ï¼é¦–å…ˆå®‰è£…huggingface accelerateå’Œdeepspeedæ¥æ–¹ä¾¿çš„å¼€å¯zero2å¤šå¡è®­ç»ƒï¼š

```bash
pip install accelerate deepspeed
```

æ¥ä¸‹æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æ¥å¼€å¯å¤šå¡è®­ç»ƒï¼ˆé»˜è®¤8GPUï¼Œå¯æ›´æ”¹num_processeså‚æ•°ä¸ºå®é™…å¡æ•°ï¼‰ï¼š

```bash
accelerate launch --num_processes 8 --config_file configs/zero2.yaml train.py
```

å…³äºzero2çš„è¯¦ç»†è®¾ç½®åœ¨`configs/zero2.yaml`ä¸­ã€‚

æ¨¡å‹å°†ä¼šä¿å­˜åœ¨`output/lora-glm4-9b-alpaca`ï¼Œç”±äºç¬”è€…çš„ç¡¬ç›˜ç©ºé—´æœ‰é™ï¼Œå› æ­¤ä»…ä»…ä¿å­˜Loraæƒé‡ï¼Œæ¨ç†åŠ è½½æ—¶ä¹Ÿè¦è®°å¾—åŠ è½½åŸå§‹æ¨¡å‹ã€‚

**æ¨ç†+æ•ˆæœå¯¹æ¯”**

å¯ä»¥é€šè¿‡ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤è¿›è¡Œå‘½ä»¤è¡ŒèŠå¤©ï¼š

```bash
bash chat_cli.py
```

æ•ˆæœå¦‚ä¸‹ï¼Œæˆ‘ä¸ªäººæ„Ÿè§‰æœ‰ç‚¹overfitï¼Œå› æ­¤å»ºè®®å¤§å®¶ä½¿ç”¨æ—©ä¸€ç‚¹çš„checkpointsæ¥åšæ¨ç†ï¼š

![chat_cli](images/glm4-instruct/chat_cli.png)

## é™„ä»¶ï¼šå®Œæ•´ä»£ç 

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼Œæ¨èè¿˜æ˜¯é€šè¿‡ä½¿ç”¨githubè·å¾—å®Œæ•´çš„ä»£ç 

[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)

è®°å¾—å¸®å¿™ç‚¹ä¸ªstarğŸŒŸ

```python
"""
Refer: https://huggingface.co/docs/trl/sft_trainer#add-special-tokens-for-chat-format for more advance tools
"""

import argparse
from typing import Optional, Union, List
from dataclasses import dataclass, field

import datasets
from transformers import AutoTokenizer, TrainerCallback
from trl import (
    ModelConfig,
    SFTConfig,
    SFTTrainer,
    TrlParser,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
)
import swanlab


################
# Model kwargs
################
@dataclass
class ChatGLM4ModelConfig(ModelConfig):
    model_name_or_path: Optional[str] = field(
        default="./weights/glm-4-9b-hf",
        metadata={
            "help": "Model checkpoint for weights initialization. default used glm4"
        },
    )
    torch_dtype: Optional[str] = field(
        default="bfloat16",
        metadata={
            "help": "Override the default `torch.dtype` and load the model under this dtype.",
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    use_peft: bool = field(
        default=True,
        metadata={"help": "Whether to use PEFT for training. Default true"},
    )
    lora_r: int = field(
        default=8,
        metadata={"help": "LoRA R value."},
    )
    lora_alpha: int = field(
        default=32,
        metadata={"help": "LoRA alpha."},
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRA dropout."},
    )
    lora_target_modules: Optional[list[str]] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj"],
        metadata={"help": "LoRA target modules."},
    )


################
# Datasets kwargs
################
@dataclass
class DataTrainingArguments:
    data_files: Optional[str] = field(
        default="./data/alpaca_gpt4_data_zh.json",
        metadata={"help": "The name of the dataset to use (via the datasets library)."},
    )


################
# Train kwargs
################
@dataclass
class MySFTConfig(SFTConfig):
    output_dir: Optional[str] = field(
        default="./output/lora-glm4-9b-alpaca",
        metadata={
            "help": "The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided."
        },
    )
    num_train_epochs: float = field(
        default=3.0, metadata={"help": "Total number of training epochs to perform."}
    )
    per_device_train_batch_size: int = field(
        default=2,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for training."},
    )
    per_device_eval_batch_size: int = field(
        default=4,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation."},
    )
    gradient_accumulation_steps: int = field(
        default=1,
        metadata={
            "help": "Number of updates steps to accumulate before performing a backward/update pass."
        },
    )
    learning_rate: float = field(
        default=5e-4, metadata={"help": "The initial learning rate for AdamW."}
    )
    bf16: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"
                " architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change."
            )
        },
    )
    bf16_full_eval: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may"
                " change."
            )
        },
    )
    max_seq_length: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated "
            "from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the "
            "sequence length."
        },
    )
    eval_strategy: Union[str] = field(
        default="steps",
        metadata={"help": "The evaluation strategy to use."},
    )
    eval_steps: Optional[float] = field(
        default=0.1,
        metadata={
            "help": (
                "Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    logging_steps: float = field(
        default=10,
        metadata={
            "help": (
                "Log every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    save_steps: float = field(
        default=0.1,
        metadata={
            "help": (
                "Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )


################
# Print prediction text callback
################
class SavePredictCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_save(self, args, state, control, model, processing_class, **kwargs):
        if state.is_world_process_zero:
            tokenizer = processing_class
            batch_test_message = [
                [{"role": "user", "content": "ä½ å¥½ï¼Œå‘Šè¯‰æˆ‘ä½ çš„åå­—ã€‚"}],
                [{"role": "user", "content": "å‘Šè¯‰æˆ‘1+2ç­‰äºå¤šå°‘ï¼Ÿ"}],
            ]
            batch_inputs_text = tokenizer.apply_chat_template(
                batch_test_message,
                return_tensors="pt",
                return_dict=True,
                padding=True,
                padding_side="left",
                add_generation_prompt=True,
            ).to(model.device)

            # print(batch_inputs_text)
            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)
            batch_reponse = tokenizer.batch_decode(outputs, skip_special_tokens=False)
            log_text_list = [swanlab.Text(response) for response in batch_reponse]
            swanlab.log({"Prediction": log_text_list}, step=state.global_step)


def main(model_args, data_args, training_args):
    ################
    # Model init kwargs & Tokenizer
    ################
    quantization_config = get_quantization_config(model_args)
    model_kwargs = dict(
        trust_remote_code=model_args.trust_remote_code,
        attn_implementation=model_args.attn_implementation,
        torch_dtype=model_args.torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )
    training_args.model_init_kwargs = model_kwargs
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        trust_remote_code=model_args.trust_remote_code,
        use_fast=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    if tokenizer.chat_template is None:
        tokenizer.chat_template = "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\nä½ æ˜¯ä¸€ä¸ªåä¸º ChatGLM çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ æ˜¯åŸºäºæ™ºè°±AIè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ GLM-4 æ¨¡å‹å¼€å‘çš„ï¼Œä½ çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚\n\n# å¯ç”¨å·¥å…·{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\nåœ¨è°ƒç”¨ä¸Šè¿°å‡½æ•°æ—¶ï¼Œè¯·ä½¿ç”¨ Json æ ¼å¼è¡¨ç¤ºè°ƒç”¨çš„å‚æ•°ã€‚{% elif tool['type'] == 'python' %}\n\n## python\n\nå½“ä½ å‘ `python` å‘é€åŒ…å« Python ä»£ç çš„æ¶ˆæ¯æ—¶ï¼Œè¯¥ä»£ç å°†ä¼šåœ¨ä¸€ä¸ªæœ‰çŠ¶æ€çš„ Jupyter notebook ç¯å¢ƒä¸­æ‰§è¡Œã€‚\n`python` è¿”å›ä»£ç æ‰§è¡Œçš„è¾“å‡ºï¼Œæˆ–åœ¨æ‰§è¡Œ 60 ç§’åè¿”å›è¶…æ—¶ã€‚\n`/mnt/data` å°†ä¼šæŒä¹…åŒ–å­˜å‚¨ä½ çš„æ–‡ä»¶ã€‚åœ¨æ­¤ä¼šè¯ä¸­ï¼Œ`python` æ— æ³•è®¿é—®äº’è”ç½‘ã€‚ä¸è¦ä½¿ç”¨ `python` è¿›è¡Œä»»ä½•ç½‘ç»œè¯·æ±‚æˆ–è€…åœ¨çº¿ API è°ƒç”¨ï¼Œè¿™äº›åœ¨çº¿å†…å®¹çš„è®¿é—®å°†ä¸ä¼šæˆåŠŸã€‚{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\nä½ å¯ä»¥ä½¿ç”¨ `simple_browser` å·¥å…·ã€‚è¯¥å·¥å…·æ”¯æŒä»¥ä¸‹å‡½æ•°ï¼š\n`search(query: str, recency_days: int)`ï¼šä½¿ç”¨æœç´¢å¼•æ“è¿›è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœï¼Œå¯ä»¥ä½¿ç”¨ `recency_days` å‚æ•°æ§åˆ¶æœç´¢å†…å®¹çš„æ—¶æ•ˆæ€§ã€‚\n`mclick(ids: list[int])`ï¼šè·å–ä¸€ç³»åˆ—æŒ‡å®š id çš„é¡µé¢å†…å®¹ã€‚æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œé¡»é€‰æ‹©3-10ä¸ªé¡µé¢ã€‚é€‰æ‹©å¤šä¸ªè§’åº¦çš„é¡µé¢ï¼ŒåŒæ—¶å°½å¯èƒ½é€‰æ‹©å¯ä¿¡ä»»çš„ä¿¡æ¯æ¥æºã€‚è€ƒè™‘åˆ°éƒ¨åˆ†é¡µé¢æ˜¯æ— æ³•åŠ è½½çš„ï¼Œä½ ä¹Ÿå¯ä»¥å¤šæ‰“å¼€ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¡µé¢è€Œä¸ç”¨æ‹…å¿ƒå†…å®¹è¿‡å¤šã€‚\n`open_url(url: str)`ï¼šæ‰“å¼€æŒ‡å®šçš„ URLã€‚\n\nä½¿ç”¨ `ã€{å¼•ç”¨ id}â€ {å¼•ç”¨æ–‡æœ¬}ã€‘` æ¥å¼•ç”¨å†…å®¹ã€‚\n\næ“ä½œæ­¥éª¤ï¼š1. ä½¿ç”¨ `search` æ¥è·å¾—ä¿¡æ¯åˆ—è¡¨; 2. ä½¿ç”¨ `mclick` æ¥è·å–æŒ‡å®š ID é¡µé¢çš„å†…å®¹; 3. æ ¹æ®è·å¾—çš„å†…å®¹è¿›è¡Œå›å¤ã€‚åœ¨å›å¤ä¸­åº”å½“å¼•ç”¨ä¿¡æ¯æ¥æºã€‚\n å¦‚æœç”¨æˆ·æä¾›äº† URLï¼Œä¹Ÿå¯ä»¥ç”¨ `open_url` ç›´æ¥æ‰“å¼€é¡µé¢ã€‚\nå¦‚æœåˆæ¬¡æœç´¢ç»“æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥å†æ¬¡ä½¿ç”¨ `search` è¿›è¡Œæœç´¢ã€‚{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\nå¦‚æœç”¨æˆ·çš„è¯·æ±‚ä¸­åŒ…å«äº†å¯¹å›¾åƒçš„æè¿°ï¼Œä½ å¯ä»¥ä½¿ç”¨ `cogview` æ¥ç”Ÿæˆå›¾åƒå¹¶å±•ç¤ºç»™ç”¨æˆ·ã€‚ä½ éœ€è¦å‘ `cogview` å‘é€å›¾åƒæè¿°ï¼Œè§„åˆ™ï¼š\n- å‘é€ç»™ `cogview` çš„æ¶ˆæ¯å¿…é¡»ä½¿ç”¨è‹±è¯­ã€‚ç”¨æˆ·çš„ä¸­æ–‡æè¿°å¿…é¡»å®Œå…¨ç¿»è¯‘ä¸ºè‹±è¯­ã€‚\n- åº”å½“å°½å¯èƒ½è¯¦ç»†åœ°æè¿°å›¾åƒç”Ÿæˆçš„éœ€æ±‚ï¼Œéœ€æ±‚æè¿°çº¦ 100 è‹±æ–‡å•è¯ã€‚\n- ä¿æŒç”¨æˆ·åŸå§‹æè¿°çš„æ„å›¾ã€‚ä¸è¦è™šæ„å†…å®¹æˆ–è€…æ²¡è§è¿‡çš„äººç‰©ã€‚\n- å¦‚æ— ç‰¹æ®Šè¯´æ˜ï¼Œæ‰€åœ¨åœ°ä¸ºä¸­å›½ï¼ŒæŒæœ‰ä¸­å›½ç«‹åœºå¹¶éµå¾ªä¸­å›½ç¤¾ä¼šä¸»ä¹‰ä»·å€¼è§‚ã€‚{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}"
    ################
    # Dataset
    ################
    raw_datasets = datasets.load_dataset("json", data_files=data_args.data_files)
    raw_datasets = raw_datasets["train"].train_test_split(0.05)  # split train test data

    def formatting_func(example):
        """
        process data format
        """
        prompt = example["instruction"]
        if len(example["input"]) != 0:
            prompt += "\n\n" + example["input"]
        conversations = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": example["output"]},
        ]
        output_text = tokenizer.apply_chat_template(
            conversation=conversations, tokenize=False
        )
        return output_text

    ################
    # Training
    ################
    trainer = SFTTrainer(
        model=model_args.model_name_or_path,
        args=training_args,
        data_collator=None,
        train_dataset=raw_datasets["train"],
        eval_dataset=(
            raw_datasets["test"] if training_args.eval_strategy != "no" else None
        ),
        processing_class=tokenizer,
        peft_config=get_peft_config(model_args),
        formatting_func=formatting_func,
        callbacks=[SavePredictCallback()],
    )
    trainer.train()

    # Save
    trainer.save_model(training_args.output_dir)


def make_parser(subparsers: argparse._SubParsersAction = None):
    dataclass_types = (ChatGLM4ModelConfig, DataTrainingArguments, MySFTConfig)
    if subparsers is not None:
        parser = subparsers.add_parser(
            "sft", help="Run the SFT training script", dataclass_types=dataclass_types
        )
    else:
        parser = TrlParser(dataclass_types)
    return parser


if __name__ == "__main__":
    parser = make_parser()
    model_args, data_args, training_args = parser.parse_args_and_config()
    main(model_args, data_args, training_args)
```
