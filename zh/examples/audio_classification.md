# éŸ³é¢‘åˆ†ç±»

:::info
éŸ³é¢‘åˆ†ç±»ã€éŸ³é¢‘å¤„ç†å…¥é—¨
:::

[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification-simple/charts)

éŸ³é¢‘åˆ†ç±»ä»»åŠ¡æ˜¯æŒ‡å°†éŸ³é¢‘ä¿¡å·æŒ‰ç…§å…¶å†…å®¹çš„ç±»åˆ«å½’å±è¿›è¡Œåˆ’åˆ†ã€‚ä¾‹å¦‚ï¼ŒåŒºåˆ†ä¸€æ®µéŸ³é¢‘æ˜¯éŸ³ä¹ã€è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³ï¼ˆå¦‚é¸Ÿé¸£ã€é›¨å£°ã€æœºå™¨è¿è½¬å£°ï¼‰è¿˜æ˜¯åŠ¨ç‰©å«å£°ç­‰ã€‚å…¶ç›®çš„æ˜¯é€šè¿‡è‡ªåŠ¨åˆ†ç±»çš„æ–¹å¼ï¼Œé«˜æ•ˆåœ°å¯¹å¤§é‡éŸ³é¢‘æ•°æ®è¿›è¡Œç»„ç»‡ã€æ£€ç´¢å’Œç†è§£ã€‚

![alt text](/assets/examples/audio_classification/example-audio-classification-1.png)

åœ¨ç°åœ¨éŸ³é¢‘åˆ†ç±»çš„åº”ç”¨åœºæ™¯ï¼Œæ¯”è¾ƒå¤šçš„æ˜¯åœ¨éŸ³é¢‘æ ‡æ³¨ã€éŸ³é¢‘æ¨èè¿™ä¸€å—ã€‚åŒæ—¶ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¥½çš„å…¥é—¨éŸ³é¢‘æ¨¡å‹è®­ç»ƒçš„ä»»åŠ¡ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¼šåŸºäºPyTorchæ¡†æ¶ï¼Œä½¿ç”¨ ResNetç³»åˆ—æ¨¡å‹åœ¨ GTZAN æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ä½¿ç”¨[SwanLab](https://swanlab.cn)ç›‘æ§è®­ç»ƒè¿‡ç¨‹ã€è¯„ä¼°æ¨¡å‹æ•ˆæœã€‚

* Githubï¼š[https://github.com/Zeyi-Lin/PyTorch-Audio-Classification](https://github.com/Zeyi-Lin/PyTorch-Audio-Classification)
* æ•°æ®é›†ï¼š[https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e) æå–ç : 1a9e
* SwanLabå®éªŒæ—¥å¿—ï¼š[https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification-simple/charts](https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification-simple/charts)
* æ›´å¤šå®éªŒæ—¥å¿—ï¼š[https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification/charts](https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification/charts)



## 1. éŸ³é¢‘åˆ†ç±»é€»è¾‘

æœ¬æ•™ç¨‹å¯¹éŸ³é¢‘åˆ†ç±»ä»»åŠ¡çš„é€»è¾‘å¦‚ä¸‹ï¼š

1. è½½å…¥éŸ³é¢‘æ•°æ®é›†ï¼Œæ•°æ®é›†ä¸ºéŸ³é¢‘WAVæ–‡ä»¶ä¸å¯¹åº”çš„æ ‡ç­¾
2. ä»¥8:2çš„æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
3. ä½¿ç”¨`torchaudio`åº“ï¼Œå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ¢…å°”é¢‘è°±å›¾ï¼Œæœ¬è´¨å°†å…¶è½¬æ¢ä¸ºå›¾åƒåˆ†ç±»ä»»åŠ¡
4. ä½¿ç”¨ResNetæ¨¡å‹å¯¹æ¢…å°”é¢‘è°±å›¾è¿›è¡Œè®­ç»ƒè¿­ä»£
5. ä½¿ç”¨SwanLabè®°å½•è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µçš„lossã€accå˜åŒ–ï¼Œå¹¶å¯¹æ¯”ä¸åŒå®éªŒä¹‹é—´çš„æ•ˆæœå·®å¼‚

## 2. ç¯å¢ƒå®‰è£…

æœ¬æ¡ˆä¾‹åŸºäº**Python>=3.8**ï¼Œè¯·åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šå®‰è£…å¥½Pythonã€‚

æˆ‘ä»¬éœ€è¦å®‰è£…ä»¥ä¸‹è¿™å‡ ä¸ªPythonåº“ï¼š

```python
torch
torchvision
torchaudio
swanlab
pandas
scikit-learn
```

ä¸€é”®å®‰è£…å‘½ä»¤ï¼š

```shellscript
pip install torch torchvision torchaudio swanlab pandas scikit-learn
```



## 3. GTZANæ•°æ®é›†å‡†å¤‡

æœ¬ä»»åŠ¡ä½¿ç”¨çš„æ•°æ®é›†ä¸ºGTZANï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨éŸ³ä¹æµæ´¾è¯†åˆ«ç ”ç©¶ä¸­å¸¸ç”¨çš„å…¬å¼€æ•°æ®é›†ã€‚GTZANæ•°æ®é›†åŒ…å« 1000 ä¸ªéŸ³é¢‘ç‰‡æ®µï¼Œæ¯ä¸ªéŸ³é¢‘ç‰‡æ®µçš„æ—¶é•¿ä¸º 30 ç§’ï¼Œå…±åˆ†ä¸º 10 ç§éŸ³ä¹æµæ´¾ï¼šåŒ…æ‹¬å¸ƒé²æ–¯ï¼ˆBluesï¼‰ã€å¤å…¸ï¼ˆClassicalï¼‰ã€ä¹¡æ‘ï¼ˆCountryï¼‰ã€è¿ªæ–¯ç§‘ï¼ˆDiscoï¼‰ã€å˜»å“ˆï¼ˆHip Hopï¼‰ã€çˆµå£«ï¼ˆJazzï¼‰ã€é‡‘å±ï¼ˆMetalï¼‰ã€æµè¡Œï¼ˆPopï¼‰ã€é›·é¬¼ï¼ˆReggaeï¼‰ã€æ‘‡æ»šï¼ˆRockï¼‰ï¼Œä¸”æ¯ç§æµæ´¾éƒ½æœ‰ 100 ä¸ªéŸ³é¢‘ç‰‡æ®µã€‚

![alt text](/assets/examples/audio_classification/example-audio-classification-2.png)

GTZANæ•°æ®é›†æ˜¯åœ¨ 2000-2001 å¹´ä»å„ç§æ¥æºæ”¶é›†çš„ï¼ŒåŒ…æ‹¬ä¸ªäºº CDã€æ”¶éŸ³æœºã€éº¦å…‹é£å½•éŸ³ç­‰ï¼Œä»£è¡¨äº†å„ç§å½•éŸ³æ¡ä»¶ä¸‹çš„å£°éŸ³ã€‚

**æ•°æ®ä¸‹è½½æ–¹å¼ï¼ˆå¤§å°1.4GBï¼‰ï¼š**

1. ç™¾åº¦ç½‘ç›˜ä¸‹è½½ï¼šé“¾æ¥: [https://pan.baidu.com/s/14CTI\_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI\_9MD1vXCqyVxmAbeMw?pwd=1a9e) æå–ç : 1a9e
2. é€šè¿‡Kaggleä¸‹è½½ï¼š[https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)
3. åœ¨Hyperè¶…ç¥ç»ç½‘ç«™ä¸‹è½½BTç§å­è¿›è¡Œä¸‹è½½ï¼š[https://hyper.ai/cn/datasets/32001](https://hyper.ai/cn/datasets/32001)

> æ³¨æ„ï¼Œæ•°æ®é›†ä¸­æœ‰ä¸€ä¸ªéŸ³é¢‘æ˜¯æŸåçš„ï¼Œåœ¨ç™¾åº¦ç½‘ç›˜ç‰ˆæœ¬é‡Œå·²ç»å°†å…¶å‰”é™¤ã€‚

ä¸‹è½½å®Œæˆåï¼Œè§£å‹åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹å³å¯ã€‚



## 4. ç”Ÿæˆæ•°æ®é›†CSVæ–‡ä»¶

æˆ‘ä»¬å°†æ•°æ®é›†ä¸­çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œå¤„ç†æˆä¸€ä¸ª`audio_dataset.csv`æ–‡ä»¶ï¼Œå…¶ä¸­ç¬¬ä¸€åˆ—ä¸ºæ–‡ä»¶è·¯å¾„ï¼Œç¬¬äºŒåˆ—ä¸ºæ ‡ç­¾ï¼š

ï¼ˆè¿™ä¸€éƒ¨åˆ†å…ˆä¸æ‰§è¡Œï¼Œåœ¨å®Œæ•´ä»£ç é‡Œä¼šå¸¦ä¸Šï¼‰

```python
import os
import pandas as pd

def create_dataset_csv():
    # æ•°æ®é›†æ ¹ç›®å½•
    data_dir = './GTZAN/genres_original'
    data = []
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for label in os.listdir(data_dir):
        label_dir = os.path.join(data_dir, label)
        if os.path.isdir(label_dir):
            # éå†å­ç›®å½•ä¸­çš„æ‰€æœ‰wavæ–‡ä»¶
            for audio_file in os.listdir(label_dir):
                if audio_file.endswith('.wav'):
                    audio_path = os.path.join(label_dir, audio_file)
                    data.append([audio_path, label])
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(data, columns=['path', 'label'])
    df.to_csv('audio_dataset.csv', index=False)
    return df


# ç”Ÿæˆæˆ–åŠ è½½æ•°æ®é›†CSVæ–‡ä»¶
if not os.path.exists('audio_dataset.csv'):
    df = create_dataset_csv()
else:
    df = pd.read_csv('audio_dataset.csv')
```

å¤„ç†åï¼Œä½ ä¼šåœ¨æ ¹ç›®å½•ä¸‹çœ‹åˆ°ä¸€ä¸ª`audio_dataset.csv`æ–‡ä»¶ï¼š

![alt text](/assets/examples/audio_classification/example-audio-classification-3.png)


## 5. é…ç½®è®­ç»ƒè·Ÿè¸ªå·¥å…·SwanLab

SwanLab æ˜¯ä¸€æ¬¾å¼€æºã€è½»é‡çš„ AI å®éªŒè·Ÿè¸ªå·¥å…·ï¼Œæä¾›äº†ä¸€ä¸ªè·Ÿè¸ªã€æ¯”è¾ƒã€å’Œåä½œå®éªŒçš„å¹³å°ã€‚SwanLab æä¾›äº†å‹å¥½çš„ API å’Œæ¼‚äº®çš„ç•Œé¢ï¼Œç»“åˆäº†è¶…å‚æ•°è·Ÿè¸ªã€æŒ‡æ ‡è®°å½•ã€åœ¨çº¿åä½œã€å®éªŒé“¾æ¥åˆ†äº«ç­‰åŠŸèƒ½ï¼Œè®©æ‚¨å¯ä»¥å¿«é€Ÿè·Ÿè¸ª AI å®éªŒã€å¯è§†åŒ–è¿‡ç¨‹ã€è®°å½•è¶…å‚æ•°ï¼Œå¹¶åˆ†äº«ç»™ä¼™ä¼´ã€‚

![alt text](/assets/examples/audio_classification/example-audio-classification-4.png)

é…ç½®SwanLabçš„æ–¹å¼å¾ˆç®€å•ï¼š

1. æ³¨å†Œä¸€ä¸ªè´¦å·ï¼š[https://swanlab.cn](https://swanlab.cn)
2. åœ¨å®‰è£…å¥½swanlabåï¼ˆpip install swanlabï¼‰ï¼Œç™»å½•ï¼š

```bash
swanlab login
```

åœ¨æç¤ºè¾“å…¥API Keyæ—¶ï¼Œå»[è®¾ç½®é¡µé¢](https://swanlab.cn/settings/overview)å¤åˆ¶API Keyï¼Œç²˜è´´åæŒ‰å›è½¦å³å¯ã€‚

![alt text](/assets/examples/audio_classification/example-audio-classification-5.png)



## 6. å®Œæ•´ä»£ç 

å¼€å§‹è®­ç»ƒæ—¶çš„ç›®å½•ç»“æ„ï¼š

```
|--- train.py
|--- GTZAN
```

train.pyåšçš„äº‹æƒ…åŒ…æ‹¬ï¼š

1. ç”Ÿæˆæ•°æ®é›†csvæ–‡ä»¶
2. åŠ è½½æ•°æ®é›†å’Œresnet18æ¨¡å‹ï¼ˆImageNeté¢„è®­ç»ƒï¼‰
3. è®­ç»ƒ20ä¸ªepochï¼Œæ¯ä¸ªepochè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°
4. è®°å½•losså’Œaccï¼Œä»¥åŠå­¦ä¹ ç‡çš„å˜åŒ–æƒ…å†µï¼Œåœ¨swanlabä¸­å¯è§†åŒ–



train.pyï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio
import torchvision.models as models
from torch.utils.data import Dataset, DataLoader
import os
import pandas as pd
from sklearn.model_selection import train_test_split
import swanlab


def create_dataset_csv():
    # æ•°æ®é›†æ ¹ç›®å½•
    data_dir = './GTZAN/genres_original'
    data = []
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for label in os.listdir(data_dir):
        label_dir = os.path.join(data_dir, label)
        if os.path.isdir(label_dir):
            # éå†å­ç›®å½•ä¸­çš„æ‰€æœ‰wavæ–‡ä»¶
            for audio_file in os.listdir(label_dir):
                if audio_file.endswith('.wav'):
                    audio_path = os.path.join(label_dir, audio_file)
                    data.append([audio_path, label])
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(data, columns=['path', 'label'])
    df.to_csv('audio_dataset.csv', index=False)
    return df

# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class AudioDataset(Dataset):
    def __init__(self, df, resize, train_mode=True):
        self.audio_paths = df['path'].values
        # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}
        self.labels = [self.label_to_idx[label] for label in df['label'].values]
        self.resize = resize
        self.train_mode = train_mode  # æ·»åŠ è®­ç»ƒæ¨¡å¼æ ‡å¿—
    def __len__(self):
        return len(self.audio_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])
        
        # å°†éŸ³é¢‘è½¬æ¢ä¸ºæ¢…å°”é¢‘è°±å›¾
        transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=2048,
            hop_length=640,
            n_mels=128
        )
        mel_spectrogram = transform(waveform)

        # ç¡®ä¿æ•°å€¼åœ¨åˆç†èŒƒå›´å†…
        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)
        
        # è½¬æ¢ä¸º3é€šé“å›¾åƒæ ¼å¼ (ä¸ºäº†é€‚é…ResNet)
        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)
        
        # ç¡®ä¿å°ºå¯¸ä¸€è‡´
        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))
        mel_spectrogram = resize(mel_spectrogram)
        
        return mel_spectrogram, self.labels[idx]

# ä¿®æ”¹ResNetæ¨¡å‹
class AudioClassifier(nn.Module):
    def __init__(self, num_classes):
        super(AudioClassifier, self).__init__()
        # åŠ è½½é¢„è®­ç»ƒçš„ResNet
        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        # ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚
        self.resnet.fc = nn.Linear(512, num_classes)
        
    def forward(self, x):
        return self.resnet(x)

# è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):
    
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            loss.backward()

            optimizer.step()
            optimizer.zero_grad()
            
            running_loss += loss.item()
            
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        train_loss = running_loss/len(train_loader)
        train_acc = 100.*correct/total
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_loss = val_loss/len(val_loader)
        val_acc = 100.*correct/total
        
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
        swanlab.log({
            "train/loss": train_loss,
            "train/acc": train_acc,
            "val/loss": val_loss,
            "val/acc": val_acc,
            "train/epoch": epoch,
            "train/lr": current_lr
        })
            
        print(f'Epoch {epoch+1}:')
        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
        print(f'Learning Rate: {current_lr:.6f}')

# ä¸»å‡½æ•°
def main():
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    run = swanlab.init(
        project="PyTorch_Audio_Classification-simple",
        experiment_name="resnet18",
        config={
            "batch_size": 16,
            "learning_rate": 1e-4,
            "num_epochs": 20,
            "resize": 224,
        },
    )
    
    # ç”Ÿæˆæˆ–åŠ è½½æ•°æ®é›†CSVæ–‡ä»¶
    if not os.path.exists('audio_dataset.csv'):
        df = create_dataset_csv()
    else:
        df = pd.read_csv('audio_dataset.csv')
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_df = pd.DataFrame()
    val_df = pd.DataFrame()
    
    for label in df['label'].unique():
        label_df = df[df['label'] == label]
        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)
        train_df = pd.concat([train_df, label_train])
        val_df = pd.concat([val_df, label_val])
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ 
    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)
    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)
    
    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    num_classes = len(df['label'].unique())  # æ ¹æ®å®é™…åˆ†ç±»æ•°é‡è®¾ç½®
    print("num_classes", num_classes)
    model = AudioClassifier(num_classes).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=run.config.learning_rate)  
    
    # è®­ç»ƒæ¨¡å‹
    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=run.config.num_epochs, device=device)

if __name__ == "__main__":
    main()
```

çœ‹åˆ°ä¸‹é¢çš„è¾“å‡ºï¼Œåˆ™ä»£è¡¨è®­ç»ƒå¼€å§‹ï¼š

![alt text](/assets/examples/audio_classification/example-audio-classification-6.png)

è®¿é—®æ‰“å°çš„swanlabé“¾æ¥ï¼Œå¯ä»¥çœ‹åˆ°è®­ç»ƒçš„å…¨è¿‡ç¨‹ï¼š

![alt text](/assets/examples/audio_classification/example-audio-classification-7.png)

å¯ä»¥çœ‹åˆ°Reset18æ¨¡å‹ï¼Œä¸”ä¸åŠ ä»»ä½•ç­–ç•¥çš„æ¡ä»¶ä¸‹ï¼Œåœ¨è®­ç»ƒé›†çš„å‡†ç¡®ç‡ä¸º99.5%ï¼ŒéªŒè¯é›†çš„å‡†ç¡®ç‡æœ€é«˜ä¸º71.5%ï¼Œval lossåœ¨ç¬¬3ä¸ªepochå¼€å§‹åè€Œåœ¨ä¸Šå‡ï¼Œå‘ˆç°ã€Œè¿‡æ‹Ÿåˆã€çš„è¶‹åŠ¿ã€‚



## 7. è¿›é˜¶ä»£ç 

ä¸‹é¢æ˜¯æˆ‘è®­å‡ºéªŒè¯é›†å‡†ç¡®ç‡87.5%çš„å®éªŒï¼Œå…·ä½“ç­–ç•¥åŒ…æ‹¬ï¼š

1. å°†æ¨¡å‹æ¢æˆresnext101\_32x8d
2. å°†æ¢…å°”é¡¿å›¾çš„resizeæé«˜åˆ°512
3. å¢åŠ warmupç­–ç•¥
4. å¢åŠ æ—¶é—´é®è”½ã€é¢‘ç‡å±è”½ã€é«˜æ–¯å™ªå£°ã€éšæœºå“åº¦è¿™å››ç§æ•°æ®å¢å¼ºç­–ç•¥
5. å¢åŠ å­¦ä¹ ç‡æ¢¯åº¦è¡°å‡ç­–ç•¥

![alt text](/assets/examples/audio_classification/example-audio-classification-8.png)

è¿›é˜¶ä»£ç ï¼ˆéœ€è¦24GBæ˜¾å­˜ï¼Œå¦‚æœè¦é™ä½æ˜¾å­˜æ¶ˆè€—çš„è¯ï¼Œå¯ä»¥è°ƒä½batch\_sizeï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio
import torchvision.models as models
from torch.utils.data import Dataset, DataLoader
import os
import pandas as pd
from sklearn.model_selection import train_test_split
import swanlab
import random
import numpy as np

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def create_dataset_csv():
    # æ•°æ®é›†æ ¹ç›®å½•
    data_dir = './GTZAN/genres_original'
    data = []
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for label in os.listdir(data_dir):
        label_dir = os.path.join(data_dir, label)
        if os.path.isdir(label_dir):
            # éå†å­ç›®å½•ä¸­çš„æ‰€æœ‰wavæ–‡ä»¶
            for audio_file in os.listdir(label_dir):
                if audio_file.endswith('.wav'):
                    audio_path = os.path.join(label_dir, audio_file)
                    data.append([audio_path, label])
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(data, columns=['path', 'label'])
    df.to_csv('audio_dataset.csv', index=False)
    return df

# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class AudioDataset(Dataset):
    def __init__(self, df, resize, train_mode=True):
        self.audio_paths = df['path'].values
        # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}
        self.labels = [self.label_to_idx[label] for label in df['label'].values]
        self.resize = resize
        self.train_mode = train_mode  # æ·»åŠ è®­ç»ƒæ¨¡å¼æ ‡å¿—
    def __len__(self):
        return len(self.audio_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])
        
        # å°†éŸ³é¢‘è½¬æ¢ä¸ºæ¢…å°”é¢‘è°±å›¾
        transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=2048,
            hop_length=640,
            n_mels=128
        )
        mel_spectrogram = transform(waveform)
        
        # ä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿›è¡Œæ•°æ®å¢å¼º
        if self.train_mode:
            # 1. æ—¶é—´é®è”½ (Time Masking)ï¼šé€šè¿‡éšæœºé€‰æ‹©ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç„¶åé®è”½æ‰20ä¸ªæ—¶é—´æ­¥
            time_mask = torchaudio.transforms.TimeMasking(time_mask_param=20)
            mel_spectrogram = time_mask(mel_spectrogram)
            
            # 2. é¢‘ç‡é®è”½ (Frequency Masking)ï¼šé€šè¿‡éšæœºé€‰æ‹©ä¸€ä¸ªé¢‘ç‡æ­¥ï¼Œç„¶åé®è”½æ‰20ä¸ªé¢‘ç‡æ­¥
            freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)
            mel_spectrogram = freq_mask(mel_spectrogram)
            
            # 3. éšæœºå¢åŠ é«˜æ–¯å™ªå£°
            if random.random() < 0.5:
                noise = torch.randn_like(mel_spectrogram) * 0.01
                mel_spectrogram = mel_spectrogram + noise
            
            # 4. éšæœºè°ƒæ•´å“åº¦
            if random.random() < 0.5:
                gain = random.uniform(0.8, 1.2)
                mel_spectrogram = mel_spectrogram * gain

        # ç¡®ä¿æ•°å€¼åœ¨åˆç†èŒƒå›´å†…
        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)
        
        # è½¬æ¢ä¸º3é€šé“å›¾åƒæ ¼å¼ (ä¸ºäº†é€‚é…ResNet)
        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)
        
        # ç¡®ä¿å°ºå¯¸ä¸€è‡´
        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))
        mel_spectrogram = resize(mel_spectrogram)
        
        return mel_spectrogram, self.labels[idx]

# ä¿®æ”¹ResNetæ¨¡å‹
class AudioClassifier(nn.Module):
    def __init__(self, num_classes):
        super(AudioClassifier, self).__init__()
        # åŠ è½½é¢„è®­ç»ƒçš„ResNet
        self.resnet = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.IMAGENET1K_V1)
        # ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚
        self.resnet.fc = nn.Linear(2048, num_classes)
        
    def forward(self, x):
        return self.resnet(x)

# è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, run):
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        # å‰5ä¸ªepochè¿›è¡Œwarmup
        if epoch < 5:
            warmup_factor = (epoch + 1) / 5
            for param_group in optimizer.param_groups:
                param_group['lr'] = run.config.learning_rate * warmup_factor
        
        # optimizer.zero_grad()  # ç§»åˆ°å¾ªç¯å¤–éƒ¨
        
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            
            optimizer.step()
            optimizer.zero_grad()
            
            running_loss += loss.item()
            
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        train_loss = running_loss
        train_acc = 100.*correct/total
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_loss = val_loss/len(val_loader)
        val_acc = 100.*correct/total
        
        # åªåœ¨warmupç»“æŸåä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        if epoch >= 5:
            scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
        swanlab.log({
            "train/loss": train_loss,
            "train/acc": train_acc,
            "val/loss": val_loss,
            "val/acc": val_acc,
            "train/epoch": epoch,
            "train/lr": current_lr
        })
            
        print(f'Epoch {epoch+1}:')
        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
        print(f'Learning Rate: {current_lr:.6f}')

# ä¸»å‡½æ•°
def main():
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    run = swanlab.init(
        project="PyTorch_Audio_Classification-simple",
        experiment_name="ğŸ˜„resnext101_32x8d",
        config={
            "batch_size": 16,
            "learning_rate": 1e-4,
            "num_epochs": 30,
            "resize": 512,
            "weight_decay": 0  # æ·»åŠ åˆ°é…ç½®ä¸­
        },
    )
    
    # ç”Ÿæˆæˆ–åŠ è½½æ•°æ®é›†CSVæ–‡ä»¶
    if not os.path.exists('audio_dataset.csv'):
        df = create_dataset_csv()
    else:
        df = pd.read_csv('audio_dataset.csv')
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_df = pd.DataFrame()
    val_df = pd.DataFrame()
    
    for label in df['label'].unique():
        label_df = df[df['label'] == label]
        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)
        train_df = pd.concat([train_df, label_train])
        val_df = pd.concat([val_df, label_val])
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ 
    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)
    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)
    
    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    num_classes = len(df['label'].unique())  # æ ¹æ®å®é™…åˆ†ç±»æ•°é‡è®¾ç½®
    model = AudioClassifier(num_classes).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(), 
        lr=run.config.learning_rate,
        weight_decay=run.config.weight_decay
    )  # Adamä¼˜åŒ–å™¨
    
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.StepLR(
        optimizer,
        step_size=10,  # åœ¨ç¬¬10ä¸ªepochè¡°å‡
        gamma=0.1,     # è¡°å‡ç‡ä¸º0.1
        verbose=True
    )
    
    # è®­ç»ƒæ¨¡å‹
    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 
                num_epochs=run.config.num_epochs, device=device, run=run)
    

if __name__ == "__main__":
    main()

```

![alt text](/assets/examples/audio_classification/example-audio-classification-9.png)

å¯ä»¥çœ‹åˆ°æå‡çš„éå¸¸æ˜æ˜¾

> æœŸå¾…æœ‰è®­ç»ƒå¸ˆèƒ½æŠŠeval accåˆ·ä¸Š90ï¼



## 8. ç›¸å…³é“¾æ¥

* Githubï¼š[https://github.com/Zeyi-Lin/PyTorch-Audio-Classification](https://github.com/Zeyi-Lin/PyTorch-Audio-Classification)
* æ•°æ®é›†ï¼š[https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e) æå–ç : 1a9e
* SwanLabå®éªŒæ—¥å¿—ï¼š[https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification-simple/charts](https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification-simple/charts)
* æ›´å¤šå®éªŒæ—¥å¿—ï¼š[https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification/charts](https://swanlab.cn/@ZeyiLin/PyTorch\_Audio\_Classification/charts)
* SwanLabå®˜ç½‘ï¼š[https://swanlab.cn](https://swanlab.cn)