# ä»é›¶é¢„è®­ç»ƒä¸€ä¸ªè‡ªå·±çš„å¤§æ¨¡å‹

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼Œç®€ç§°LLMï¼‰ï¼ŒæŒ‡ä½¿ç”¨å¤§é‡æ–‡æœ¬æ•°æ®è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬æˆ–ç†è§£è¯­è¨€æ–‡æœ¬çš„å«ä¹‰ã€‚

![llm](/assets/examples/pretrain_llm/llm.png)

è™½ç„¶ç½‘ä¸Šæœ‰å¤§é‡å…³äºtransformerç†è®ºã€å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æ•™ç¨‹ã€‚ä½†æ˜¯å°‘æœ‰å…³äºé¢„è®­ç»ƒçš„è§£é‡Šã€‚æœ¬æ–‡åˆ™ä»å¦‚ä½•è‡ªå·±å®æˆ˜é¢„è®­ç»ƒä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„è§’åº¦ï¼Œä½¿ç”¨wikiæ•°æ®é›†è¿›è¡Œä¸€ä¸ªç®€å•çš„ä»é›¶é¢„è®­ç»ƒå·¥ä½œï¼Œå¹¶é™„ä¸Šä½¿ç”¨swanlab launchç™½å«–æ˜¾å¡çš„æ–¹æ³•

* æœ¬æ•™ç¨‹å®Œæ•´ä»£ç ï¼š[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)

* å®éªŒè®°å½•ï¼š[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)

* æ•°æ®é›†ä¸‹è½½ï¼š[ç™¾åº¦ç½‘ç›˜ï¼ˆj8eeï¼‰](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)ï¼Œ[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)

## å®‰è£…ç¯å¢ƒ

é¦–å…ˆï¼Œé¡¹ç›®æ¨èä½¿ç”¨python3.10ã€‚éœ€è¦å®‰è£…çš„pythonåŒ…å¦‚ä¸‹ï¼š

```txt
swanlab
transformers
datasets
accelerate
```

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ä¸€é”®å®‰è£…ï¼š

```bash
pip install swanlab transformers datasets accelerate modelscope
```

## ä¸‹è½½æ•°æ®é›†

æœ¬æ•™ç¨‹ä½¿ç”¨çš„æ˜¯ä¸­æ–‡wikiæ•°æ®ï¼Œç†è®ºä¸Šé¢„è®­ç»ƒæ•°æ®é›†ç§ç±»è¶Šä¸°å¯Œã€æ•°æ®é‡è¶Šå¤§è¶Šå¥½ï¼Œåç»­ä¼šå¢åŠ åˆ«çš„æ•°æ®é›†ã€‚

![dataset](/assets/examples/pretrain_llm/dataset.png)

huggingfaceé“¾æ¥ï¼š[wikipedia-zh-cn](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)

ç™¾åº¦ç½‘ç›˜ä¸‹è½½åœ°å€ï¼š[ç™¾åº¦ç½‘ç›˜ï¼ˆj8eeï¼‰](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)

ä¸‹è½½`wikipedia-zh-cn-20240820.json`æ–‡ä»¶åæ”¾åˆ°é¡¹ç›®ç›®å½•ä¸‹`./WIKI_CN/`æ–‡ä»¶å¤¹ä¸­

è¯¥æ•°æ®é›†æ–‡ä»¶çº¦1.99Gå¤§ï¼Œå…±æœ‰1.44Mæ¡æ•°æ®ã€‚è™½ç„¶æ•°æ®é›†ä¸­åŒ…å«æ–‡ç« æ ‡é¢˜ï¼Œä½†æ˜¯å®é™…ä¸Šåœ¨é¢„è®­ç»ƒé˜¶æ®µç”¨ä¸ä¸Šã€‚æ­£æ–‡ç‰‡æ®µå‚è€ƒï¼š

```txt
æ•°å­¦æ˜¯ç ”ç©¶æ•°é‡ã€ç»“æ„ä»¥åŠç©ºé—´ç­‰æ¦‚å¿µåŠå…¶å˜åŒ–çš„ä¸€é—¨å­¦ç§‘ï¼Œå±äºå½¢å¼ç§‘å­¦çš„ä¸€ç§ã€‚æ•°å­¦åˆ©ç”¨æŠ½è±¡åŒ–å’Œé€»è¾‘æ¨ç†ï¼Œä»è®¡æ•°ã€è®¡ç®—ã€é‡åº¦ã€å¯¹ç‰©ä½“å½¢çŠ¶åŠè¿åŠ¨çš„è§‚å¯Ÿå‘å±•è€Œæˆã€‚æ•°å­¦å®¶ä»¬æ‹“å±•è¿™äº›æ¦‚å¿µ...
```

ä½¿ç”¨[ğŸ¤—Huggingface Datasets](https://huggingface.co/docs/datasets/index)åŠ è½½æ•°æ®é›†çš„ä»£ç å¦‚ä¸‹ï¼š

```python
from datasets import load_dataset

ds = load_dataset("fjcanyue/wikipedia-zh-cn")
```

å¦‚æœä½¿ç”¨ç™¾åº¦ç½‘ç›˜ä¸‹è½½çš„jsonæ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç åŠ è½½

```python
raw_datasets = datasets.load_dataset(
    "json", data_files="data/wikipedia-zh-cn-20240820.json"
)

raw_datasets = raw_datasets["train"].train_test_split(test_size=0.1, seed=2333)
print("dataset info")
print(raw_datasets)
```

## æ„å»ºè‡ªå·±çš„å¤§è¯­è¨€æ¨¡å‹

æœ¬æ•™ç¨‹ä½¿ç”¨[ğŸ¤—huggingface transformers](https://huggingface.co/docs/transformers/index)æ„å»ºè‡ªå·±çš„å¤§æ¨¡å‹ã€‚

å› ä¸ºç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªä¸­æ–‡å¤§æ¨¡å‹ã€‚å› æ­¤æˆ‘ä»¬å‚è€ƒ[é€šä¹‰åƒé—®2](https://qwen.readthedocs.io/zh-cn/latest/run_locally/mlx-lm.html)çš„tokenizeå’Œæ¨¡å‹æ¶æ„ï¼Œä»…ä»…åšä¸€äº›ç®€å•çš„æ›´æ”¹è®©æ¨¡å‹æ›´å°æ›´å¥½è®­ç»ƒã€‚

å› ä¸ºå›½å†…æ— æ³•ç›´æ¥è®¿é—®åˆ°huggingfaceï¼Œæ¨èä½¿ç”¨modelscopeå…ˆæŠŠæ¨¡å‹é…ç½®æ–‡ä»¶å’Œcheckpointä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¿è¡Œå¦‚ä¸‹ä»£ç 

```python
import modelscope

modelscope.AutoConfig.from_pretrained("Qwen/Qwen2-0.5B").save_pretrained(
    "Qwen2-0.5B"
)
modelscope.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B").save_pretrained(
    "Qwen2-0.5B"
)
```

é…ç½®å‚æ•°ï¼Œå¹¶ä¿®æ”¹æ¨¡å‹æ³¨æ„åŠ›å¤´æ•°é‡ã€æ¨¡å‹å±‚æ•°å’Œä¸­é—´å±‚å¤§å°ï¼ŒæŠŠæ¨¡å‹æ§åˆ¶åˆ°å¤§æ¦‚120Må‚æ•°å·¦å³ï¼ˆè·ŸGPT2æ¥è¿‘ï¼‰ã€‚

```python
import transformers

tokenizer = transformers.AutoTokenizer.from_pretrained("./Qwen2-0.5B")   # è¿™é‡Œä½¿ç”¨qwen2çš„tokenzier
config = transformers.AutoConfig.from_pretrained(
        "./Qwen2-0.5B",
        vocab_size=len(tokenizer),
        hidden_size=512,
        intermediate_size=2048,
        num_attention_heads=8,
        num_hidden_layers=12,
        n_ctx=context_length,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
print("Model Config:")
print(config)
```

ä½¿ç”¨transformersåº“åˆå§‹åŒ–æ¨¡å‹

```python
model = transformers.Qwen2ForCausalLM(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"Model Size: {model_size/1000**2:.1f}M parameters")
```

## è®¾ç½®è®­ç»ƒå‚æ•°

è®¾ç½®é¢„è®­ç»ƒè¶…å‚æ•°ï¼š

```python
args = transformers.TrainingArguments(
    output_dir="checkpoints",
    per_device_train_batch_size=24,  # æ¯ä¸ªGPUçš„è®­ç»ƒbatchæ•°
    per_device_eval_batch_size=24,  # æ¯ä¸ªGPUçš„æµ‹è¯•batchæ•°
    eval_strategy="steps",
    eval_steps=5_000,
    logging_steps=500,
    gradient_accumulation_steps=12,  # æ¢¯åº¦ç´¯è®¡æ€»æ•°
    num_train_epochs=2, # è®­ç»ƒepochæ•°
    weight_decay=0.1,
    warmup_steps=1_000,
    optim="adamw_torch",  # ä¼˜åŒ–å™¨ä½¿ç”¨adamw
    lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
    learning_rate=5e-4,  # åŸºç¡€å­¦ä¹ ç‡ï¼Œ
    save_steps=5_000,
    save_total_limit=10,
    bf16=True,  # å¼€å¯bf16è®­ç»ƒ, å¯¹äºAmperæ¶æ„ä»¥ä¸‹çš„æ˜¾å¡å»ºè®®æ›¿æ¢ä¸ºfp16=True
)
print("Train Args:")
print(args)
```

## åˆå§‹åŒ–è®­ç»ƒ+ä½¿ç”¨swanlabè¿›è¡Œè®°å½•

ä½¿ç”¨transformersè‡ªå¸¦çš„trainå¼€å§‹è®­ç»ƒï¼Œå¹¶ä¸”å¼•å…¥swanlabä½œä¸ºå¯è§†åŒ–æ—¥å¿—è®°å½•

```python
from swanlab.integration.huggingface import SwanLabCallback
trainer = transformers.Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    callbacks=[SwanLabCallback()],
)
trainer.train()
```

å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨SwanLabï¼Œéœ€è¦ç™»é™†SwanLabå®˜ç½‘[https://swanlab.cn/](https://swanlab.cn/)ï¼Œæ³¨å†Œï¼Œå¹¶ä¸”åœ¨å¦‚ä¸‹ä½ç½®æ‰¾åˆ°å’Œå¤åˆ¶è‡ªå·±çš„keyã€‚

![findkey](/assets/examples/pretrain_llm/findkey.png)

æ¥ä¸‹æ¥åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥

```sh
swanlab login
```

ä¼šçœ‹åˆ°æç¤ºè¾“å…¥key

![login](/assets/examples/pretrain_llm/login.png)

æŒ‰ç…§æç¤ºå°†keyç²˜è´´è¿›å»ï¼ˆæ³¨æ„keyæ˜¯ä¸ä¼šæ˜¾ç¤ºåˆ°ç»ˆç«¯å½“ä¸­çš„ï¼‰å°±å¯ä»¥å®Œæˆé…ç½®ï¼Œå®Œæˆæ•ˆæœå¦‚ä¸‹ï¼š

![login2](/assets/examples/pretrain_llm/login2.png)

## å®Œæ•´ä»£ç 

é¡¹ç›®ç›®å½•ç»“æ„ï¼š

```txt
|---data\
|------wikipedia-zh-cn-20240820.json    # æ•°æ®é›†æ”¾åœ¨dataæ–‡ä»¶å¤¹ä¸­
|--- pretrain.py
```

`pretrain.py`ä»£ç å¦‚ä¸‹ï¼š

```python
import datasets
import transformers
import swanlab
from swanlab.integration.huggingface import SwanLabCallback
import modelscope

def main():
    # using swanlab to save log
    swanlab.init("WikiLLM")

    # load dataset
    raw_datasets = datasets.load_dataset(
        "json", data_files="/data/WIKI_CN/wikipedia-zh-cn-20240820.json"
    )

    raw_datasets = raw_datasets["train"].train_test_split(test_size=0.1, seed=2333)
    print("dataset info")
    print(raw_datasets)

    # load tokenizers
    # å› ä¸ºå›½å†…æ— æ³•ç›´æ¥è®¿é—®HuggingFaceï¼Œå› æ­¤ä½¿ç”¨é­”æ­å°†æ¨¡å‹çš„é…ç½®æ–‡ä»¶å’ŒTokenizerä¸‹è½½ä¸‹æ¥
    modelscope.AutoConfig.from_pretrained("Qwen/Qwen2-0.5B").save_pretrained(
        "Qwen2-0.5B"
    )
    modelscope.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B").save_pretrained(
        "Qwen2-0.5B"
    )
    context_length = 512  # use a small context length
    # tokenizer = transformers.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        "./Qwen2-0.5B"
    )  # download from local

    # preprocess dataset
    def tokenize(element):
        outputs = tokenizer(
            element["text"],
            truncation=True,
            max_length=context_length,
            return_overflowing_tokens=True,
            return_length=True,
        )
        input_batch = []
        for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
            if length == context_length:
                input_batch.append(input_ids)
        return {"input_ids": input_batch}

    tokenized_datasets = raw_datasets.map(
        tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
    )
    print("tokenize dataset info")
    print(tokenized_datasets)
    tokenizer.pad_token = tokenizer.eos_token
    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)

    # prepare a model from scratch
    config = transformers.AutoConfig.from_pretrained(
        "./Qwen2-0.5B",
        vocab_size=len(tokenizer),
        hidden_size=512,
        intermediate_size=2048,
        num_attention_heads=8,
        num_hidden_layers=12,
        n_ctx=context_length,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    model = transformers.Qwen2ForCausalLM(config)
    model_size = sum(t.numel() for t in model.parameters())
    print("Model Config:")
    print(config)
    print(f"Model Size: {model_size/1000**2:.1f}M parameters")

    # train
    args = transformers.TrainingArguments(
        output_dir="WikiLLM",
        per_device_train_batch_size=32,  # æ¯ä¸ªGPUçš„è®­ç»ƒbatchæ•°
        per_device_eval_batch_size=32,  # æ¯ä¸ªGPUçš„æµ‹è¯•batchæ•°
        eval_strategy="steps",
        eval_steps=5_00,
        logging_steps=50,
        gradient_accumulation_steps=8,  # æ¢¯åº¦ç´¯è®¡æ€»æ•°
        num_train_epochs=2,  # è®­ç»ƒepochæ•°
        weight_decay=0.1,
        warmup_steps=2_00,
        optim="adamw_torch",  # ä¼˜åŒ–å™¨ä½¿ç”¨adamw
        lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
        learning_rate=5e-4,  # åŸºç¡€å­¦ä¹ ç‡ï¼Œ
        save_steps=5_00,
        save_total_limit=10,
        bf16=True,  # å¼€å¯bf16è®­ç»ƒ, å¯¹äºAmperæ¶æ„ä»¥ä¸‹çš„æ˜¾å¡å»ºè®®æ›¿æ¢ä¸ºfp16=True
    )
    print("Train Args:")
    print(args)
    # enjoy training
    trainer = transformers.Trainer(
        model=model,
        tokenizer=tokenizer,
        args=args,
        data_collator=data_collator,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        callbacks=[SwanLabCallback()],
    )
    trainer.train()

    # save model
    model.save_pretrained("./WikiLLM/Weight")  # ä¿å­˜æ¨¡å‹çš„è·¯å¾„

    # generate
    pipe = transformers.pipeline("text-generation", model=model, tokenizer=tokenizer)
    print("GENERATE:", pipe("äººå·¥æ™ºèƒ½", num_return_sequences=1)[0]["generated_text"])
    prompts = ["ç‰›é¡¿", "åŒ—äº¬å¸‚", "äºšæ´²å†å²"]
    examples = []
    for i in range(3):
        # æ ¹æ®æç¤ºè¯ç”Ÿæˆæ•°æ®
        text = pipe(prompts[i], num_return_sequences=1)[0]["generated_text"]
        text = swanlab.Text(text)
        examples.append(text)
    swanlab.log({"Generate": examples})


if __name__ == "__main__":
    main()

```

## è®­ç»ƒç»“æœæ¼”ç¤º

è¿è¡Œå¦‚ä¸‹å‘½ä»¤

```
python pretrain.py
```

å¯ä»¥çœ‹åˆ°å¦‚ä¸‹è®­ç»ƒæ—¥å¿—ã€‚ç”±äºè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œæ¨èä½¿ç”¨tmuxå°†è®­ç»ƒä»»åŠ¡holdä½

![terminal](/assets/examples/pretrain_llm/terminal.png)

å¯ä»¥åœ¨[SwanLab](https://swanlab.cn)ä¸­æŸ¥çœ‹æœ€ç»ˆçš„è®­ç»ƒç»“æœï¼š

![log](/assets/examples/pretrain_llm/log.png)

<!-- å¹¶ä¸”èƒ½å¤Ÿçœ‹åˆ°ä¸€äº›æœ€ç»ˆç”Ÿæˆçš„æ¡ˆä¾‹ï¼š

![sample]() -->

## ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†

ä»¥â€œäººå·¥æ™ºèƒ½â€ä¸ºå¼€å¤´ç”Ÿæˆå†…å®¹çš„ä»£ç å¦‚ä¸‹ï¼š

```python
pipe = transformers.pipeline("text-generation", model=model, tokenizer=tokenizer)
print("GENERATE:", pipe("äººå·¥æ™ºèƒ½", num_return_sequences=1)[0]["generated_text"])
```

æ¨ç†æ•ˆæœå¦‚ä¸‹ï¼š

ï¼ˆæ¨¡å‹è®­ç»ƒingï¼Œå¯ä»¥åœ¨[https://swanlab.cn/@ShaohonChen/WikiLLM/overview](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)å®æ—¶æŸ¥çœ‹è®­ç»ƒè¿›å±•å’Œæ¨ç†æ•ˆæœï¼‰
<!-- ![result]() -->

## ä½¿ç”¨SwanLab Launchç”¨è¿œç¨‹GPUè¿›è¡Œè®­ç»ƒ

::: info
ç¡®ä¿swanlabç‰ˆæœ¬ä¸º0.3.19
:::

é¢„è®­ç»ƒLLMå¯¹äºGPUçš„ç®—åŠ›å’Œæ˜¾å­˜è¦æ±‚éå¸¸é«˜ï¼Œæœ¬æ–‡æ¨èä½¿ç”¨[SwanLab Launch](/api/cli-swanlab-remote-gpu)åˆ©ç”¨äº‘ä¸ŠGPUè¿›è¡Œé¢„è®­ç»ƒã€‚

é¦–å…ˆä½¿ç”¨`swanlab upload -n WIKI_CN WIKI_CN`å‘½ä»¤ä¸Šä¼ æ•°æ®é›†

![upload](/assets/examples/pretrain_llm/launch_upload.png)

ä¸Šä¼ å®Œåä¼šè·å¾—æ•°æ®é›†çš„IDï¼ˆå¦‚ä¸‹å›¾ï¼‰

![upload](/assets/examples/pretrain_llm/launch_upload2.png)

ä¹Ÿå¯ä»¥ä½¿ç”¨`swanlab task list`æŸ¥çœ‹ä¸Šä¼ çš„æ•°æ®é›†ID

![show_id](/assets/examples/pretrain_llm/show_id.png)

å‚è€ƒ[SwanLab Launchå®˜æ–¹æ–‡æ¡£](/api/cli-swanlab-remote-gpu)ï¼Œæœ¬åœ°åˆ›å»º`swanlab.yaml`æ–‡ä»¶å¹¶å†™å…¥å¦‚ä¸‹ä¿¡æ¯

```yaml
apiVersion: swanlab/v1
kind: Folder
metadata:
  name: WikiLLM
  desc: Pretrain LLM using wiki data
spec:
  python: "3.10"
  entry: "pretrain.py"
  volumes:
    - name: "WIKI_CN"
      id: "<æ›¿æ¢ä¸ºå¯¹åº”æ•°æ®é›†çš„ID>"
  exclude:
    - "WIKI_CN"
```

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¼€å¯è¿œç¨‹è®­ç»ƒï¼š

```bash
swanlab launch -f swanlab.yaml
```

å³å¯å¼€å¯è¿œç¨‹è®­ç»ƒï¼å¯ä»¥åœ¨SwanLabä¸Šè·Ÿè¸ªè¿œç¨‹å®éªŒæ—¥å¿—ã€‚

![remote_log](/assets/examples/pretrain_llm/remote_log.png)

å¯ä»¥çœ‹åˆ°è¯¥å®éªŒçš„ç¡¬ä»¶ä¸ºè¿œç¨‹çš„H800æœåŠ¡å™¨ï¼Œé€Ÿåº¦è¿˜æ˜¯å¾ˆå¿«çš„ï¼Œç›¸æ¯”äºA100å¤§æ¦‚èƒ½æå‡2-3å€çš„é€Ÿåº¦

![remote_devices](/assets/examples/pretrain_llm/remote_devices.png)

å…³äºå¦‚ä½•æŸ¥çœ‹ã€ç»ˆæ­¢è¿œç¨‹å®éªŒï¼Œå¯å‚è€ƒ[SwanLab Launchå®˜æ–¹æ–‡æ¡£](/api/cli-swanlab-remote-gpu)

## å‚è€ƒé“¾æ¥

* æœ¬æ•™ç¨‹å®Œæ•´ä»£ç :[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)

* å®éªŒè®°å½•ï¼š[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)

* æ•°æ®é›†ä¸‹è½½ï¼š[ç™¾åº¦ç½‘ç›˜ï¼ˆj8eeï¼‰](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)ï¼Œ[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)
