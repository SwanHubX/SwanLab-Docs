# openMindå¤§æ¨¡å‹å¾®è°ƒæ•™ç¨‹

## ç®€ä»‹

é­”ä¹ç¤¾åŒºï¼ˆ[Modelers.cn](https://modelers.cn)ï¼‰æ˜¯ä¸€ä¸ªä¸ºäººå·¥æ™ºèƒ½å¼€å‘è€…åŠçˆ±å¥½è€…æ‰“é€ çš„ç¤¾åŒºï¼Œæä¾›å·¥å…·é“¾ã€æ•°æ®é›†ã€æ¨¡å‹å’Œåº”ç”¨ç­‰AIé¢†åŸŸç”Ÿäº§è¦ç´ çš„æ‰˜ç®¡åŠå±•ç¤ºæœåŠ¡å’Œæ”¯æ’‘ç³»ç»Ÿã€‚ç›®å‰ï¼Œé­”ä¹ç¤¾åŒºå·²æ”¯æŒopenMind Libraryã€‚è¯¥å·¥å…·é€šè¿‡ç®€å•çš„APIæ¥å£ï¼Œå¸®åŠ©å¼€å‘è€…å®Œæˆæ¨¡å‹é¢„è®­ç»ƒã€å¾®è°ƒã€æ¨ç†ç­‰æµç¨‹ã€‚åŒæ—¶ï¼ŒopenMind LibraryåŸç”Ÿå…¼å®¹PyTorch å’Œ MindSpore ç­‰ä¸»æµæ¡†æ¶ï¼ŒåŸç”Ÿæ”¯æŒæ˜‡è…¾NPUå¤„ç†å™¨ã€‚openMind Libraryå¯ä»¥å’ŒPEFTã€DeepSpeedç­‰ä¸‰æ–¹åº“é…åˆä½¿ç”¨ï¼Œæ¥æå‡æ¨¡å‹å¾®è°ƒæ•ˆç‡ã€‚

å‹æƒ…é“¾æ¥ï¼š

* [é­”ä¹ç¤¾åŒº](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)
* [Huggingface](https://huggingface.co)
* [SwanLab](https://swanlab.cn)

---

## 1ã€åŸºæœ¬æ¦‚å¿µ

1ã€[openMind Library](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)--->[Huggingface Transformers](https://huggingface.co/docs/transformers/index)

openMind Libraryç±»ä¼¼äºtransformersçš„å¤§æ¨¡å‹å°è£…å·¥å…·ï¼Œå…¶ä¸­å°±æœ‰AutoModelForSequenceClassificationã€AutoModelForCausalLMç­‰ç­‰æ¨¡å‹åŠ è½½å·¥å…·ä»¥åŠåƒTrainingArgumentså‚æ•°é…ç½®å·¥å…·ç­‰ç­‰ï¼ŒåŸç†åŸºæœ¬ä¸€æ ·ï¼Œä¸è¿‡å¯¹NPUé€‚é…æ›´å‹å¥½äº›ã€‚
![openmind vs transformers](./openMind/openmind_transformers.png)

2ã€[é­”ä¹ç¤¾åŒº](https://modelers.cn/)--->[HuggingFace](https://huggingface.co/)

é­”ä¹ç¤¾åŒºç±»ä¼¼äºhuggingfaceè¿™ç§æ¨¡å‹æ‰˜ç®¡ç¤¾åŒºï¼Œé‡Œé¢é™¤äº†torchçš„æ¨¡å‹è¿˜æœ‰ä½¿ç”¨MindSporeå®ç°çš„æ¨¡å‹ã€‚transformerså¯ä»¥ç›´æ¥ä»huggingfaceè·å–æ¨¡å‹æˆ–è€…æ•°æ®é›†ï¼ŒopenMindä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œå¯ä»¥ä»é­”ä¹ç¤¾åŒºè·å–æ¨¡å‹å’Œæ•°æ®é›†ã€‚
![é­”ä¹ç¤¾åŒº vs huggingface](./openMind/mole.png)

---

## 2ã€å¾®è°ƒä»£ç 

å¦‚æœäº†è§£äº†ä¸Šè¿°çš„å¯¹åº”æœºåˆ¶ï¼Œé‚£ä¹ˆå°±å¯ä»¥è·‘ä¸€ä¸ªç®€å•çš„å¾®è°ƒä»£ç äº†ï¼Œè¯¥ä»£ç å‚è€ƒäº†[é­”ä¹ç¤¾åŒºçš„æ•™ç¨‹æ–‡æ¡£](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)ï¼Œç¨ä½œè°ƒæ•´ï¼Œå¯ä»¥å¯¹æ¯”NVIDIAæ˜¾å¡çš„ç»“æœã€‚

### æ¦‚è¿°

openMind Libraryæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ å¼€å‘å¥—ä»¶ï¼Œé€šè¿‡ç®€å•æ˜“ç”¨çš„APIæ”¯æŒæ¨¡å‹é¢„è®­ç»ƒã€å¾®è°ƒã€æ¨ç†ç­‰æµç¨‹ã€‚openMind Libraryé€šè¿‡ä¸€å¥—æ¥å£å…¼å®¹PyTorchå’ŒMindSporeç­‰ä¸»æµæ¡†æ¶ï¼ŒåŒæ—¶åŸç”Ÿæ”¯æŒæ˜‡è…¾NPUå¤„ç†å™¨ï¼ŒåŒæ—¶openMind Libraryå¯ä»¥å’ŒPEFTã€DeepSpeedç­‰ä¸‰æ–¹åº“é…åˆä½¿ç”¨ï¼Œæ¥åŠ é€Ÿæ¨¡å‹å¾®è°ƒæ•ˆç‡ã€‚

### ç¯å¢ƒé…ç½®

å¦‚æœæ˜¯æ˜‡è…¾AIå¡ç³»åˆ—çš„è¯ï¼Œé…ç½®ç¯å¢ƒå‰éœ€è¦å…ˆå®‰è£…é©±åŠ¨ç­‰è®¾å¤‡ï¼Œå…·ä½“å¯ä»¥å‚è€ƒ[è½¯ä»¶å®‰è£…-CANNå•†ç”¨ç‰ˆ8.0.RC3å¼€å‘æ–‡æ¡£-æ˜‡è…¾ç¤¾åŒº](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)ã€‚

**é©±åŠ¨å®‰è£…&éªŒè¯**

é¦–å…ˆå¾—ç¡®å®šæœ‰NPUå¡å’ŒNPUç›¸å…³é©±åŠ¨ï¼Œé©±åŠ¨æ˜¯8.0.RC3.beta1ï¼Œå¦‚æœæ²¡å®‰è£…å¯ä»¥å‚è€ƒä¸Šé¢è½¯ä»¶å®‰è£…çš„é“¾æ¥æŸ¥çœ‹ã€‚

å®‰è£…å¥½åçš„éªŒè¯æ–¹æ³•æ˜¯è¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œè¯¥å‘½ä»¤ä½œç”¨ä¸nvidia-smiç±»ä¼¼ï¼Œè¿™é‡Œæ˜¯æŸ¥çœ‹NPUçš„çŠ¶æ€å’Œæ€§èƒ½

```
npu-smi info
```

å¯ä»¥çœ‹åˆ°å¦‚ä¸‹ä¿¡æ¯çš„è¯å°±è¡¨ç¤ºé©±åŠ¨å·²ç»å®‰è£…å®Œæˆäº†ï¼Œå·¦ä¾§æ˜¯å®‰è£…æˆåŠŸåè¿è¡Œä»£ç åçš„ç»“æœï¼Œå³ä¾§æ˜¯æ¯ä¸€éƒ¨åˆ†çš„å«ä¹‰

![npu-smi info](./openMind/npu-info.png)

ç„¶åå®‰è£…å¥½é©±åŠ¨äº†ä¹‹åå°±å¯ä»¥é…ç½®ç¯å¢ƒäº†ï¼Œæœ¬æ¬¡å¾®è°ƒä»£ç ä½¿ç”¨pytorchæ¡†æ¶ï¼ŒopenMindä¸­è‡ªå¸¦äº†åŸºäºpytorchæ¡†æ¶çš„å„ç±»å‡½æ•°ï¼Œå› æ­¤æ­£å¸¸å®‰è£…openMindå°±è¡Œã€‚

> æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
>
> 1ã€å¯ä»¥ä½¿ç”¨é•œåƒæºæ¥å®‰è£…ç¯å¢ƒï¼Œä¸ç„¶ä¼šå¾ˆæµªè´¹æ—¶é—´ï¼Œå¯ä»¥ä½¿ç”¨æ¸…åæºï¼š
>
> ```
> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple name
> ```
>
> 2ã€é­”ä¹ç¤¾åŒºä¸­æœ‰ä¸¤ä¸ªæ¡†æ¶çš„åˆ†ç±»ï¼Œå¦‚æœæ˜¯pytorchå°±åªèƒ½é€‰æ‹©pytorchæ¡†æ¶ï¼ŒåŒç†å¦‚æœæ˜¯mindsporeå°±åªèƒ½é€‰æ‹©mindsporeæ¡†æ¶
> ![é­”ä¹ç¤¾åŒºæ¨¡å‹](./openMind/models.png)
> 3ã€é…ç½®ç¯å¢ƒçš„æ—¶å€™ï¼ŒæŒ‰ç…§openmindå®˜æ–¹æ–‡æ¡£è¯´å¯ä»¥åŒæ—¶å­˜åœ¨ä¸¤ä¸ªæ¡†æ¶ï¼Œä½¿ç”¨çš„æ—¶å€™åˆ†åˆ«è®¾ç½®å°±è¡Œï¼Œä½†æ˜¯å®é™…ä½¿ç”¨çš„æ—¶å€™åªèƒ½å­˜åœ¨ä¸€ä¸ªæ¡†æ¶ï¼Œä¸€æ—¦è®¾ç½®äº†ä¸¤ä¸ªæ¡†æ¶ï¼Œä½¿ç”¨çš„æ—¶å€™æ— è®ºå¦‚ä½•è®¾ç½®éƒ½ä¼šæŠ¥é”™è¯´openmindä¸çŸ¥é“ä½¿ç”¨å“ªä¸ªæ¡†æ¶ï¼Œæ‰€ä»¥æœ€å¥½åœ¨ç¯å¢ƒé‡Œåªå®‰è£…ä¸€ä¸ª
>
> ```
> >>>import openmind
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   File "/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/__init__.py", line 20, in <module>
>     from .utils import is_ms_available, is_torch_available
>   File "/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/__init__.py", line 14, in <module>
>     from .import_utils import (
>   File "/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py", line 69, in <module>
>     CURRENT_FRAMEWORK = get_framework()
>   File "/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py", line 66, in get_framework
>     raise RuntimeError(replace_invalid_characters(error_msg))
> RuntimeError: Multiple frameworks detected, including: pt, ms.
> ```

ç„¶åç›´æ¥å®‰è£…ç¯å¢ƒï¼Œä¸‹å›¾è¡¨ç¤ºçš„æœ‰äº›æ¨¡å‹æµ‹è¯•ä¼šç›´æ¥ç»™å‡ºé•œåƒç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å®‰è£…ç¯å¢ƒ

![bertæ¨¡å‹ç¯å¢ƒ](./openMind/bert.png)

### åˆ«å¿˜äº†å®‰è£…SwanLab ğŸ˜ƒ

```
pip install openmind
pip install torch
pip install transformers
pip install swanlab
```


### æ•°æ®é›†å¤„ç†

OmDataset.load_dataset()æ–¹æ³•ç›®å‰æ”¯æŒä¸‹è½½çš„æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼š

* parquet
* jsonæˆ–è€…jsonl
* tar.gz
* csv
* ä¸‹è½½pythonè„šæœ¬åŠ è½½é­”ä¹ç¤¾åŒºæ•°æ®é›†
* ä¸‹è½½pythonè„šæœ¬åŠ è½½ä¸‰æ–¹ç«™ç‚¹æ•°æ®é›†

```
from openmind import OmDataset
from openmind import AutoTokenizer
 
### å‡†å¤‡æ•°æ®é›†
dataset = OmDataset.load_dataset("AI_Connect/glue", "cola")
 
### ç»“æœ
"""
DatasetDict({
    train: Dataset({
        features: ['sentence', 'label', 'idx'],
        num_rows: 8551
    })
    validation: Dataset({
        features: ['sentence', 'label', 'idx'],
        num_rows: 1043
    })
    test: Dataset({
        features: ['sentence', 'label', 'idx'],
        num_rows: 1063
    })
})
"""
 
### åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("PyTorch-NPU/bert_base_cased")
 
### å¤„ç†æ•°æ®é›†
def tokenize_function(examples):
    return tokenizer(examples["sentence"],truncation=True,padding="max_length",max_length=512)
 
### è®­ç»ƒæ•°æ®å°è£…
tokenized_datasets = dataset.map(tokenize_function, batched=True)
 
# è®­ç»ƒæ•°æ®+éªŒè¯æ•°æ®ï¼ŒéªŒè¯å‘ç”Ÿåœ¨æ¯ä¸ªepochä¹‹å
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42)
small_eval_dataset = tokenized_datasets["validation"].shuffle(seed=42)
```

### åŠ è½½æ¨¡å‹

å’Œtransformersä½¿ç”¨å·®ä¸å¤šï¼Œåˆ†åˆ«åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨

```
from openmind import AutoTokenizer
from openmind import AutoModelForSequenceClassification  ## åšåˆ†ç±»ä»»åŠ¡
 
 
### åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("PyTorch-NPU/bert_base_cased")
 
### åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("PyTorch-NPU/bert_base_cased", num_labels=2)  # äºŒåˆ†ç±»ä»»åŠ¡
```

### è®­ç»ƒå‚æ•°é…ç½®

åˆ›å»ºä¸€ä¸ªTrainingArgumentsç±»ï¼Œå…¶ä¸­åŒ…å«å¯ä»¥è°ƒæ•´çš„æ‰€æœ‰è¶…å‚æ•°ä»¥åŠä¸åŒçš„è®­ç»ƒé€‰é¡¹ã€‚

```
from openmind import TrainingArguments
 
### å‚æ•°åˆå§‹åŒ–
# æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„è·¯å¾„
training_args = TrainingArguments(logging_steps=1,
                                  output_dir="test_trainer",
                                  evaluation_strategy="epoch",
                                  half_precision_backend="auto",  # auto:è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ··åˆç²¾åº¦è®­ç»ƒåç«¯ï¼›apexï¼šè‹±ä¼Ÿè¾¾çš„ ï¼›cpu_ampï¼šåœ¨CPUä¸Šè¿è¡Œ
                                  per_device_train_batch_size=4,
                                  optim="adamw_torch",
                                  learning_rate=2e-5)
```

### è¯„ä¼°å‚æ•°è®¾ç½®

Traineråœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œéœ€è¦å‘Trainerä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’Œå±•ç¤ºæŒ‡æ ‡ã€‚

```
import numpy as np
from openmind import metrics
 
### é…ç½®è¯„ä¼°å‚æ•°
metric = metrics.Accuracy()
 
 
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(preds=preds, labels=labels)
```

### å¯è§†åŒ–å·¥å…·é…ç½®

swanlabæ”¯æŒè®°å½•openMind Libraryã€‚èƒ½å¤Ÿåœ¨çº¿/ç¦»çº¿æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ã€‚SwanLabæ”¯æŒopenMind Libraryé€šè¿‡callbackè°ƒç”¨ï¼Œè°ƒç”¨ä»£ç å¯å‚è€ƒåæ–‡ã€‚

![SwanLabå¯è§†åŒ–å·¥å…·](./openMind/juzhong.png)
å…³äºSwanLabçš„ä½¿ç”¨æ–¹æ³•å¯ä»¥å‚è€ƒ[SwanLabå®˜æ–¹æ–‡æ¡£-å¿«é€Ÿå¼€å§‹](https://docs.swanlab.cn/guide_cloud/general/quick-start.html)

> å¦‚æœæç¤ºç™»å½•swanlabï¼Œå¯ä»¥åœ¨[å®˜ç½‘å®Œæˆæ³¨å†Œ](https://swanlab.cn)åï¼Œä½¿ç”¨[è·å–API KEY](https://swanlab.cn/settings)æ‰¾åˆ°å¯¹åº”çš„ç™»é™†å¯†é’¥å¹¶ç²˜è´´ï¼Œè¿™æ ·å°†èƒ½å¤Ÿä½¿ç”¨**äº‘ä¸Šçœ‹ç‰ˆ**éšæ—¶æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ä¸ç»“æœã€‚

```
from openmind import Trainer
from swanlab.integration.transformers import SwanLabCallback
 
### ä½¿ç”¨swanlabç›‘æµ‹
swanlab_config = {
    "dataset": "glue",
    "fp16_backend":"auto",
    "datacollator":"transformer"
}
swanlab_callback = SwanLabCallback(
    project="new_qwen2.5-7B-finetune",
    experiment_name="è·‘çš„å®˜æ–¹ä¾‹å­çš„å¾®è°ƒ",
    description="è¿™ä¸ªæ˜¯ä½¿ç”¨transformersçš„datacollatorå°è£…å‡½æ•°",
    workspace=None,
    config=swanlab_config,
)
 
### åˆ›å»ºè®­ç»ƒå™¨å¹¶ä¸”å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[swanlab_callback],
)
 
trainer.train()
 
### ä¿å­˜æ¨¡å‹
output_dir="./output"
final_save_path = join(output_dir)
trainer.save_model(final_save_path)
```

### å…¨è¿‡ç¨‹ä»£ç 

```
from openmind import OmDataset
from openmind import AutoTokenizer
from openmind import AutoModelForSequenceClassification
from openmind import TrainingArguments
from openmind import metrics
import numpy as np
from openmind import Trainer
from swanlab.integration.transformers import SwanLabCallback
from os.path import join
 
 
### å‡†å¤‡æ•°æ®é›†
dataset = OmDataset.load_dataset("AI_Connect/glue", "cola")
 
### åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("PyTorch-NPU/bert_base_cased")
 
 
### å¤„ç†æ•°æ®é›†
def tokenize_function(examples):
    # å¡«å……
    return tokenizer(examples["sentence"],truncation=True,padding="max_length",max_length=512)
 
 
tokenized_datasets = dataset.map(tokenize_function, batched=True)
 
# å‡å°‘æ•°æ®é‡
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42)
small_eval_dataset = tokenized_datasets["validation"].shuffle(seed=42)
 
 
### åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("PyTorch-NPU/bert_base_cased", num_labels=2)
 
### å‚æ•°åˆå§‹åŒ–
# æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„è·¯å¾„
training_args = TrainingArguments(logging_steps=1,
                                  output_dir="test_trainer",
                                  evaluation_strategy="epoch",
                                  half_precision_backend="auto",  # auto:è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ··åˆç²¾åº¦è®­ç»ƒåç«¯ï¼›apexï¼šè‹±ä¼Ÿè¾¾çš„ ï¼›cpu_ampï¼šåœ¨CPUä¸Šè¿è¡Œ
                                  per_device_train_batch_size=4,
                                  optim="adamw_torch",
                                  learning_rate=2e-5)
 
### é…ç½®è¯„ä¼°å‚æ•°
metric = metrics.Accuracy()
 
 
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(preds=preds, labels=labels)
 
 
### ä½¿ç”¨swanlabç›‘æµ‹
swanlab_config = {
    "dataset": "glue",
    "fp16_backend":"auto",
    "datacollator":"transformer"
}
swanlab_callback = SwanLabCallback(
    project="new_qwen2.5-7B-finetune",
    experiment_name="è·‘çš„å®˜æ–¹ä¾‹å­çš„å¾®è°ƒ",
    description="è¿™ä¸ªæ˜¯ä½¿ç”¨transformersçš„datacollatorå°è£…å‡½æ•°",
    workspace=None,
    config=swanlab_config,
)
### åˆ›å»ºè®­ç»ƒå™¨å¹¶ä¸”å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[swanlab_callback],
)
 
trainer.train()
 
### ä¿å­˜æ¨¡å‹
output_dir="./output"
final_save_path = join(output_dir)
trainer.save_model(final_save_path)
```

---

## 3ã€ç»“æœå±•ç¤º

è¿™é‡Œä½¿ç”¨HF Transformerså®ç°åŒæ ·çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä½¿ç”¨NVIDIA-A100å¡æ¥è·‘äº†ä¸€æ¬¡åšä¸ªå¯¹æ¯”ï¼ŒA100å¯¹åº”çš„ä»£ç å¦‚ä¸‹ï¼š

```
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
import evaluate
import numpy as np
from transformers import Trainer
from swanlab.integration.transformers import SwanLabCallback
from os.path import join
import os
 
# è®¾ç½®åªä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä½¿ç”¨ç¬¬ä¸€å— GPU
 
### åŠ è½½æ•°æ®é›†
dataset = load_dataset("nyu-mll/glue","cola")
 
### åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
 
### å¤„ç†æ•°æ®é›†
def tokenize_function(examples):
    # å¡«å……
    return tokenizer(examples["sentence"], padding="max_length", truncation=True)
 
tokenized_datasets = dataset.map(tokenize_function, batched=True)
 
# å‡å°‘æ•°æ®é‡
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42)
small_eval_dataset = tokenized_datasets["validation"].shuffle(seed=42)
 
### åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=2)
 
### å‚æ•°åˆå§‹åŒ–
# æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„è·¯å¾„
training_args = TrainingArguments(logging_steps=1,
                                  output_dir="test_trainer",
                                  evaluation_strategy="epoch",
                                  half_precision_backend="auto",  
                                  per_device_train_batch_size=4,
                                  optim="adamw_torch",
                                  learning_rate=2e-5)
 
### é…ç½®è¯„ä¼°å‚æ•°
metric = evaluate.load("accuracy")
 
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    # æ·»åŠ è¯„ä¼°æ•°æ®
    metric.add_batch(predictions=preds, references=labels)  # ä½¿ç”¨add_batchæ–¹æ³•æ·»åŠ æ‰¹æ¬¡æ•°æ®
    # è®¡ç®—å‡†ç¡®åº¦
    return metric.compute()
 
### ä½¿ç”¨swanlabç›‘æµ‹
swanlab_config = {
    "dataset": "glue"
}
swanlab_callback = SwanLabCallback(
    project="new_qwen2.5-7B-finetune",
    experiment_name="è·‘çš„å®˜æ–¹ä¾‹å­çš„å¾®è°ƒ",
    description="ç”¨ä¾‹å­è·‘çš„ï¼Œæ¨¡å‹ç”¨çš„æ˜¯bertï¼Œåšæ–‡æœ¬åˆ†ç±»ä»»åŠ¡",
    workspace=None,
    config=swanlab_config,
)
### åˆ›å»ºè®­ç»ƒå™¨å¹¶ä¸”å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[swanlab_callback],
)
 
trainer.train()
 
### ä¿å­˜æ¨¡å‹
output_dir="./output/A100"
final_save_path = join(output_dir)
trainer.save_model(final_save_path)
```

ä¸‹é¢æ˜¯å„é¡¹å¯¹æ¯”ï¼Œ

é¦–å…ˆæ˜¯å®éªŒæ—¶é—´ï¼Œæ­¤æ¬¡å®éªŒepoch=3ï¼Œ

![æ—¶é—´å¯¹æ¯”](./openMind/time.png)

çœ‹æ ·å­æ˜‡è…¾å¡æ¯”A100ç¨å¾®å¿«ç‚¹

ç„¶åæ˜¯æ˜¾å­˜æ¶ˆè€—ï¼Œå…¶ä¸­ä¸¤ä¸ªç›‘æµ‹NPU/GPUçŠ¶æ€çš„ä»£ç å¦‚ä¸‹ï¼š

```
NPUï¼š
watch -n 1 npu-smi info
 
GPUï¼š
nvtop
```

![æ˜¾å­˜å¯¹æ¯”](./openMind/xiancun.png)

æ˜¾å­˜æ¶ˆè€—å·®ä¸å¤š

æœ€åæ˜¯lossç­‰å‚æ•°çš„å˜åŒ–

![losså¯¹æ¯”](./openMind/loss.png)

æ„Ÿè§‰A100ä¸Šè¿è¡Œçš„ç»“æœéœ‡è¡æ¯”è¾ƒæ˜æ˜¾ï¼Œæ˜‡è…¾å¡éœ‡è¡æ¯”è¾ƒå°‘ã€‚
