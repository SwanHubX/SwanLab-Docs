# Fine-tuning ChatGLM4 for Large Model Instruction Following (with Code and Test Scripts)  

Author: Emotional Machine Lab - Chen Shaohong  
Email: <shaohon_chen@115lab.club>  

[[toc]]

## Abstract  

This tutorial primarily implements a fine-tuning method for large model instruction following. To simplify implementation and reduce code complexity, this article uses the ğŸ¤—HuggingFace TRL framework. In addition to supporting SFT, this framework provides excellent support for popular reinforcement fine-tuning algorithms such as DPO, PPO, and GRPO.  

While using frameworks can significantly reduce workload, it inevitably poses challenges for beginners. Therefore, this tutorial will include comprehensive documentation references to help readers further understand the framework. Although implementing the fine-tuning process from scratch with PyTorch can greatly enhance understanding, and there are many excellent community projects available, the author still recommends using frameworks for training. This approach saves substantial time, allowing users to focus more on innovation.  

Thus, this tutorial is recommended for readers with some familiarity with the ğŸ¤—HuggingFace Transformers framework.  

Note: Due to the relatively large size of the ChatGLM model, actual execution requires approximately >=16GB of GPU memory.  

ğŸ‰ **SwanLab has been officially integrated into ğŸ¤—HuggingFace Transformers:** If SwanLab is installed locally, it will be enabled by default! It can also be activated via `report_to="swanlab"` for training tracking.  

**References:**  

â€¢ Zhipu AI Official Website: [https://www.zhipuai.cn/](https://www.zhipuai.cn/)  
â€¢ ChatGLM-9B Base Model: [https://huggingface.co/THUDM/glm-4-9b-hf](https://huggingface.co/THUDM/glm-4-9b-hf/tree/main)  
â€¢ ChatGLM-9B-Chat Model: [https://huggingface.co/THUDM/glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf/tree/main)  
â€¢ Chinese Version of Alpaca Dataset: [https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)  
â€¢ This Blog's Open-Source Project Link: [https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)  
â€¢ SwanLab Training Logs: [https://swanlab.cn/@ShaohonChen/chatglm-finetune/](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)  

## TRL Package Introduction + Environment Setup  

![trl](./images/glm4-instruct/trl.png)  

This tutorial uses the [ğŸ¤—HuggingFace TRL](https://huggingface.co/docs/trl/index) framework to implement the fine-tuning code. TRL is a powerful and user-friendly fine-tuning framework that not only supports SFT but also easily integrates popular reinforcement fine-tuning algorithms like DPO, PPO, and GRPO through its interfaces. Additionally, it is fully compatible with the Transformers architecture.  

First, set up the environment for this tutorial with the following installation command:  

```bash  
pip install transformers trl datasets peft swanlab  
```  

Here, `transformers`, `trl`, and `peft` are used for model loading and training, `datasets` for importing datasets, and `swanlab` for visualizing and tracking the training process.  

Below is a simple fine-tuning example to introduce the usage of the HF TRL framework:  

```python  
from datasets import load_dataset  
from trl import SFTConfig, SFTTrainer  

dataset = load_dataset("stanfordnlp/imdb", split="train")   # Set the fine-tuning dataset; here, IMDB movie review classification data is used  

training_args = SFTConfig(  # Set fine-tuning parameters  
    max_length=512,  
    output_dir="/tmp",  
)  
trainer = SFTTrainer(   # Set the model; here, Facebook's opt-350M is used, which has a small parameter count for easy downloading  
    "facebook/opt-350m",  
    train_dataset=dataset,  
    args=training_args,  
)  
trainer.train() # Start training; the process is similar to TRL  
```  

The above code is from the HF official documentation [https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer), with added comments for better understanding.  

In short, the TRL package is used similarly to Transformers, but with two additional steps:  

â€¢ Import the `SFTConfig` module, which is based on `transformers`' `TrainingArguments` but introduces some extra parameters for SFT and LoRA support.  
â€¢ Import the `SFTTrainer` module, which includes the SFT implementation code and some additional support for `peft`'s LoRA and dataset format conversion.  

The following sections will provide a complete guide on using the TRL package to achieve large model instruction following.  

## ChatGLM4 Introduction + Model Preparation  

![chatglm_history](images/glm4-instruct/chatglm_history.png)  

GLM-4-9B is the latest open-source version of the GLM-4 series of pre-trained models released by [Zhipu AI](https://www.zhipuai.cn/). ChatGLM has released multiple versions, with GLM-4-9B being the fourth-generation base model. Its fine-tuned version, GLM-4-9B-Chat, includes advanced features such as web browsing, code execution, custom tool invocation (Function Call), and long-context reasoning (supporting up to 128K tokens).  

This tutorial uses the GLM-4-9B model for instruction following fine-tuning and employs SwanLab for model performance tracking.  

âš ï¸ Note: To align with HuggingFace Transformers updates, ChatGLM has released two versions of weights: `THUDM/glm-4-9b` and `THUDM/glm-4-9b-hf`. The latter corresponds to newer versions of transformers, so this tutorial uses the latter.  

This tutorial provides a script to download the model. The download method is as follows:  

```bash  
huggingface-cli download --local-dir ./weights/glm-4-9b-hf THUDM/glm-4-9b-hf  
```  

The model will be downloaded to `./weights/glm-4-9b-hf` in the project directory.  

Below is an example of loading the ChatGLM model using `transformers` and performing inference:  

```python  
from transformers import AutoTokenizer, AutoModelForCausalLM  
device = "cuda"  
tokenizer = AutoTokenizer.from_pretrained("THUDM/glm-4-9b-chat-hf")  
model = AutoModelForCausalLM.from_pretrained("THUDM/glm-4-9b-chat-hf").eval().to(device)  
inputs = tokenizer.encode("æˆ‘æ˜¯ChatGLMï¼Œæ˜¯", return_tensors="pt").to(device)  
outputs = model.generate(inputs)  
print(tokenizer.decode(outputs[0]))  
```  

Since this is a base model without fine-tuning, it will only complete the text following `"æˆ‘æ˜¯ChatGLMï¼Œæ˜¯"`. Running this will generate the following output:  

```bash  
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.35it/s]  
[gMASK]<sop>æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹  
```  

The above example demonstrates inference with a base model, which can only perform text generation. To enable conversational capabilities, the fine-tuned chat model must be loaded, as shown below:  

```python  
from transformers import pipeline  

messages = [  
    {"role": "user", "content": "ä½ æ˜¯è°"},  
]  
pipe = pipeline("text-generation", model="THUDM/glm-4-9b-chat-hf")  
print(pipe(messages))  
```  

Here, we use the `pipeline` interface for inference. Running this will generate the following output:  

```bash  
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.24it/s]  
Device set to use cuda:0  
[{'generated_text': [{'role': 'user', 'content': 'ä½ æ˜¯è°'}, {'role': 'assistant', 'content': '\næˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåä¸º ChatGLMã€‚æˆ‘æ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œ'}]}]  
```  

Printing the model structure with `print(model)` reveals the following:  

```text  
GlmForCausalLM(  
  (model): GlmModel(  
    (embed_tokens): Embedding(151552, 4096, padding_idx=151329)  
    (layers): ModuleList(  
      (0-39): 40 x GlmDecoderLayer(  
        (self_attn): GlmAttention(  
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)  
          (k_proj): Linear(in_features=4096, out_features=256, bias=True)  
          (v_proj): Linear(in_features=4096, out_features=256, bias=True)  
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)  
        )  
        (mlp): GlmMLP(  
          (gate_up_proj): Linear(in_features=4096, out_features=27392, bias=False)  
          (down_proj): Linear(in_features=13696, out_features=4096, bias=False)  
          (activation_fn): SiLU()  
        )  
        (input_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)  
        (post_attention_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)  
      )  
    )  
    (norm): GlmRMSNorm((4096,), eps=1.5625e-07)  
    (rotary_emb): GlmRotaryEmbedding()  
  )  
  (lm_head): Linear(in_features=4096, out_features=151552, bias=False)  
)  
```  

The GLM model has an impressive 40 layers ğŸ˜‚, so when using LoRA for fine-tuning, the number of trainable parameters will be larger compared to other models.  

## Dataset Preparation  

The dataset has already been included in the GitHub project and can be downloaded directly using the following command to obtain the complete experimental code:  

```bash  
git clone https://github.com/SwanHubX/glm4-finetune.git  
```  

If you only want to download the dataset, you can directly download the following file:  

```bash  
wget https://github.com/SwanHubX/glm4-finetune/blob/main/data/alpaca_gpt4_data_zh.json  
```  

Alternatively, it can be downloaded from ğŸ¤—HuggingFace: [https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)  

## Code Explanation + Hyperparameter Tuning  

The complete fine-tuning code is publicly available on GitHub. You can download it using the following command:  

```bash  
git clone https://github.com/SwanHubX/glm4-finetune.git  
```  

The article's attachments also include the full implementation code [#Code Attachment](#Attachment: Full Code).  

This article will now focus on explaining the functional modules of the code.  

### Model Loading and Hyperparameter Settings  
Here, special attention should be paid to the LoRA parameter settings. The LoRA parameters in this article are based on the official ChatGLM fine-tuning code.  

Note that the learning rate is set to 5e-4. For full fine-tuning, it should be reduced by an order of magnitude.  

```python  
################  
# Model kwargs  
################  
@dataclass  
class ChatGLM4ModelConfig(ModelConfig):  
    model_name_or_path: Optional[str] = field(  
        default="./weights/glm-4-9b-hf",  
        metadata={  
            "help": "Model checkpoint for weights initialization. default used glm4"  
        },  
    )  
    torch_dtype: Optional[str] = field(  
        default="bfloat16",  
        metadata={  
            "help": "Override the default `torch.dtype` and load the model under this dtype.",  
            "choices": ["auto", "bfloat16", "float16", "float32"],  
        },  
    )  
    use_peft: bool = field(  
        default=True,  
        metadata={"help": "Whether to use PEFT for training. Default true"},  
    )  
    lora_r: int = field(  
        default=8,  
        metadata={"help": "LoRA R value."},  
    )  
    lora_alpha: int = field(  
        default=32,  
        metadata={"help": "LoRA alpha."},  
    )  
    lora_dropout: float = field(  
        default=0.1,  
        metadata={"help": "LoRA dropout."},  
    )  
    lora_target_modules: Optional[list[str]] = field(  
        default_factory=lambda: ["q_proj", "k_proj", "v_proj"],  
        metadata={"help": "LoRA target modules."},  
    )  
```  

### Dataset Hyperparameter Settings  
This part is relatively simple, as it only loads a local dataset.  

```python  
################  
# Datasets kwargs  
################  
@dataclass  
class DataTrainingArguments:  
    data_files: Optional[str] = field(  
        default="./data/alpaca_gpt4_data_zh.json.json",  
        metadata={"help": "The name of the dataset to use (via the datasets library)."},  
    )  
```  

However, to help readers understand what the dataset looks like, a dataset display script is provided:  

```python  
import datasets  
raw_dataset=datasets.load_dataset("json", data_files="data/glaive_toolcall_zh_1k.json")  
print(raw_dataset)  
"""Print Output  
DatasetDict({  
    train: Dataset({  
        features: ['instruction', 'input', 'output'],  
        num_rows: 42677  
    })  
})  
"""  
```  

The dataset contains 1,000 entries, with fields including `'conversations', 'tools'`.  

To further examine a single entry:  

```python  
print(raw_dataset["train"][0])  
```  

Output:  

```json  
{  
    "instruction": "ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚",  
    "input": "",  
    "output": "ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š\n\n1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚\n\n2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚\n\n3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚"  
}  
```  

Here, readers might wonder why there are both `instruction` and `input` sections. Early research on instruction-following aimed to create a general-purpose task-processing model (e.g., capable of both translation and calculations). Thus, task descriptions were placed in `instruction`, while the actual task text was placed in `input`.  
However, with the advent of general-purpose AI assistants like ChatGPT, users have grown accustomed to directly issuing instructions. As a result, separating `instruction` and `prompt` has become less necessary. Fundamentally, whether separated or not, the model's core function remains predicting subsequent text based on context. The separation only affects the format, not the final outcome.  
Modern open-source chat-based large language models (LLMs) often place "persona" in the `system prompt` and user instructions in `input`. Therefore, we will later reformat the Alpaca dataset to align with mainstream chat model conventions.  

The recommended input fine-tuning data structure provided by ChatGLM is as follows:  

```json  
{  
  "messages": [  
    {  
      "role": "user",  
      "content": "ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ€§æ„Ÿ"  
    },  
    {  
      "role": "assistant",  
      "content": "3x1çš„è¿™æ¬¾ç‰›ä»”è£¤é‡‡ç”¨æµ…ç™½çš„ç‰›ä»”é¢æ–™ä¸ºè£¤èº«æè´¨ï¼Œå…¶æŸ”ç„¶çš„æ‰‹æ„Ÿå’Œç»†è…»çš„è´¨åœ°ï¼Œåœ¨ç©¿ç€èˆ’é€‚çš„åŒæ—¶ï¼Œé€éœ²ç€æ¸…çº¯ç”œç¾çš„ä¸ªæ€§æ°”è´¨ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæµç•…çš„è£¤èº«å‰ªè£å°†æ€§æ„Ÿçš„è…¿éƒ¨æ›²çº¿å½°æ˜¾çš„æ·‹æ¼“å°½è‡´ï¼Œä¸å¤±ä¸ºä¸€æ¬¾éšæ€§å‡ºè¡—çš„å¿…å¤‡å•å“ã€‚"  
    }  
  ]  
}  
```  

Experienced readers might argue that since we are training from scratch, we can define our own data structure. While this is true, to leverage ChatGLM's native `chat_template`, we recommend adhering to the official data format. This ensures compatibility with ChatGLM's tools and full utilization of its special tokens.  

The native `chat_template` can be found in the `tokenizer_config.json` of the open-source `glm-4-9b-chat-hf` on HuggingFace. The following script prints the `chat_template`:  

```python  
from transformers import AutoTokenizer, AutoModelForCausalLM  
device = "cuda"  
tokenizer = AutoTokenizer.from_pretrained("THUDM/glm-4-9b-chat-hf")  
print(tokenizer.chat_template)  
```  

Link to the tokenizer configuration:  
[https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json](https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json)  

Below is a simple demonstration of the final dataset format after conversion:  

```python  
def formatting_func(example):  
    """  
    process data format  
    """  
    prompt = example["instruction"]  
    if len(example["input"]) != 0:  
        prompt += "\n\n" + example["input"]  
    conversations = [  
        {"role": "user", "content": prompt},  
        {"role": "assistant", "content": example["output"]},  
    ]  
    output_text = tokenizer.apply_chat_template(  
        conversation=conversations, tokenize=False  
    )  
    return output_text  
```  

Output example (this is the actual data format fed to the model during fine-tuning):  

```text  
[gMASK]<sop><|user|>  
ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚<|assistant|>  
ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š  

1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚  

2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚  

3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚  
```  

### Training Hyperparameters and Implementation  
Given the small dataset size, we train for 600 steps with a per-GPU batch size of 1*4:  

```python  
################  
# Train kwargs  
################  
@dataclass  
class MySFTConfig(SFTConfig):  
    output_dir: Optional[str] = field(  
        default="./output/lora-glm4-9b-alpaca",  
        metadata={  
            "help": "The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided."  
        },  
    )  
    num_train_epochs: float = field(  
        default=3.0, metadata={"help": "Total number of training epochs to perform."}  
    )  
    per_device_train_batch_size: int = field(  
        default=2,  
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for training."},  
    )  
    per_device_eval_batch_size: int = field(  
        default=4,  
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation."},  
    )  
    gradient_accumulation_steps: int = field(  
        default=1,  
        metadata={  
            "help": "Number of updates steps to accumulate before performing a backward/update pass."  
        },  
    )  
    learning_rate: float = field(  
        default=5e-4, metadata={"help": "The initial learning rate for AdamW."}  
    )  
    bf16: bool = field(  
        default=True,  
        metadata={  
            "help": (  
                "Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"  
                " architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change."  
            )  
        },  
    )  
    bf16_full_eval: bool = field(  
        default=True,  
        metadata={  
            "help": (  
                "Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may"  
                " change."  
            )  
        },  
    )  
    max_seq_length: Optional[int] = field(  
        default=512,  
        metadata={  
            "help": "Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated "  
            "from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the "  
            "sequence length."  
        },  
    )  
    eval_strategy: Union[str] = field(  
        default="steps",  
        metadata={"help": "The evaluation strategy to use."},  
    )  
    eval_steps: Optional[float] = field(  
        default=0.1,  
        metadata={  
            "help": (  
                "Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. "  
                "If smaller than 1, will be interpreted as ratio of total training steps."  
            )  
        },  
    )  
    logging_steps: float = field(  
        default=10,  
        metadata={  
            "help": (  
                "Log every X updates steps. Should be an integer or a float in range `[0,1)`. "  
                "If smaller than 1, will be interpreted as ratio of total training steps."  
            )  
        },  
    )  
    save_steps: float = field(  
        default=0.1,  
        metadata={  
            "help": (  
                "Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. "  
                "If smaller than 1, will be interpreted as ratio of total training steps."  
            )  
        },  
    )  
```  

The training process is streamlined using HF TRL:  

```python  
################  
# Training  
################  
trainer = SFTTrainer(  
    model=model_args.model_name_or_path,  
    args=training_args,  
    data_collator=None,  
    train_dataset=raw_datasets["train"],  
    eval_dataset=(  
        raw_datasets["test"] if training_args.eval_strategy != "no" else None  
    ),  
    processing_class=tokenizer,  
    peft_config=get_peft_config(model_args),  
    formatting_func=formatting_func,  
    callbacks=[SavePredictCallback()],  
)  
trainer.train()  
```  

## Starting Training + Performance Evaluation  
By default, this code enables [SwanLab](https://swanlab.cn) during training. SwanLab is officially integrated into ğŸ¤—HuggingFace Transformers and can be enabled via `report_to="swanlab"`. If SwanLab is installed locally, it will be enabled by default!  

Start training with the following command:  

```bash  
python instruct_train.py  
```  

You will see the following startup information:  

![train](images/glm4-instruct/train.png)  

If you are not logged into SwanLab, a login prompt may appear. We recommend selecting option 1 and registering at [https://swanlab.cn](https://swanlab.cn) to view training progress online.  

Login command:  

```bash  
swanlab login  
```  

Click the printed link to view training logs via the dashboard:  

![swanlab](images/glm4-instruct/swanlab.png)  

By configuring `callback`, SwanLab can also automatically record model predictions. Code and example:  

```python  
################  
# Print prediction text callback  
################  
class SavePredictCallback(TrainerCallback):  
    def __init__(self, num_steps=10):  
        self.num_steps = num_steps  

    def on_save(self, args, state, control, model, processing_class, **kwargs):  
        if state.is_world_process_zero:  
            tokenizer = processing_class  
            batch_test_message = [  
                [{"role": "user", "content": "ä½ å¥½ï¼Œå‘Šè¯‰æˆ‘ä½ çš„åå­—ã€‚"}],  
                [{"role": "user", "content": "å‘Šè¯‰æˆ‘1+2ç­‰äºå¤šå°‘ï¼Ÿ"}],  
            ]  
            batch_inputs_text = tokenizer.apply_chat_template(  
                batch_test_message,  
                return_tensors="pt",  
                return_dict=True,  
                padding=True,  
                padding_side="left",  
                add_generation_prompt=True,  
            ).to(model.device)  

            # print(batch_inputs_text)  
            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)  
            batch_reponse = tokenizer.batch_decode(  
                outputs, skip_special_tokens=False  
            )  
            log_text_list = [swanlab.Text(response) for response in batch_reponse]  
            swanlab.log({"Prediction": log_text_list}, step=state.global_step)  
```  

![swanlab-text](images/glm4-instruct/swanlab-text.png)  

**Multi-GPU Training**  
If you have multiple GPUs, we recommend using multi-GPU training to significantly speed up training! First, install HuggingFace `accelerate` and `deepspeed` to easily enable Zero2 multi-GPU training:  

```bash  
pip install accelerate deepspeed  
```  

Then, use the following command to start multi-GPU training (default: 8 GPUs; adjust `num_processes` as needed):  

```bash  
accelerate launch --num_processes 8 --config_file configs/zero2.yaml train.py  
```  

Detailed Zero2 settings are in `configs/zero2.yaml`.  

The model will be saved in `output/lora-glm4-9b-alpaca`. Due to limited disk space, only LoRA weights are saved. Remember to load the original model during inference.  

**Inference + Performance Comparison**  
Use the following command for command-line chat:  

```bash  
bash chat_cli.py  
```  

Example output (note: overfitting may occur; earlier checkpoints are recommended for inference):  

![chat_cli](images/glm4-instruct/chat_cli.png)  

## Attachment: Full Code  
The full code is provided below, but we recommend downloading the complete code from GitHub:  

[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)  

Don't forget to give it a starğŸŒŸ!  

```python  
"""
Refer: https://huggingface.co/docs/trl/sft_trainer#add-special-tokens-for-chat-format for more advance tools
"""

import argparse
from typing import Optional, Union, List
from dataclasses import dataclass, field

import datasets
from transformers import AutoTokenizer, TrainerCallback
from trl import (
    ModelConfig,
    SFTConfig,
    SFTTrainer,
    TrlParser,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
)
import swanlab


################
# Model kwargs
################
@dataclass
class ChatGLM4ModelConfig(ModelConfig):
    model_name_or_path: Optional[str] = field(
        default="./weights/glm-4-9b-hf",
        metadata={
            "help": "Model checkpoint for weights initialization. default used glm4"
        },
    )
    torch_dtype: Optional[str] = field(
        default="bfloat16",
        metadata={
            "help": "Override the default `torch.dtype` and load the model under this dtype.",
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    use_peft: bool = field(
        default=True,
        metadata={"help": "Whether to use PEFT for training. Default true"},
    )
    lora_r: int = field(
        default=8,
        metadata={"help": "LoRA R value."},
    )
    lora_alpha: int = field(
        default=32,
        metadata={"help": "LoRA alpha."},
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRA dropout."},
    )
    lora_target_modules: Optional[list[str]] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj"],
        metadata={"help": "LoRA target modules."},
    )


################
# Datasets kwargs
################
@dataclass
class DataTrainingArguments:
    data_files: Optional[str] = field(
        default="./data/alpaca_gpt4_data_zh.json",
        metadata={"help": "The name of the dataset to use (via the datasets library)."},
    )


################
# Train kwargs
################
@dataclass
class MySFTConfig(SFTConfig):
    output_dir: Optional[str] = field(
        default="./output/lora-glm4-9b-alpaca",
        metadata={
            "help": "The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided."
        },
    )
    num_train_epochs: float = field(
        default=3.0, metadata={"help": "Total number of training epochs to perform."}
    )
    per_device_train_batch_size: int = field(
        default=2,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for training."},
    )
    per_device_eval_batch_size: int = field(
        default=4,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation."},
    )
    gradient_accumulation_steps: int = field(
        default=1,
        metadata={
            "help": "Number of updates steps to accumulate before performing a backward/update pass."
        },
    )
    learning_rate: float = field(
        default=5e-4, metadata={"help": "The initial learning rate for AdamW."}
    )
    bf16: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"
                " architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change."
            )
        },
    )
    bf16_full_eval: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may"
                " change."
            )
        },
    )
    max_seq_length: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated "
            "from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the "
            "sequence length."
        },
    )
    eval_strategy: Union[str] = field(
        default="steps",
        metadata={"help": "The evaluation strategy to use."},
    )
    eval_steps: Optional[float] = field(
        default=0.1,
        metadata={
            "help": (
                "Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    logging_steps: float = field(
        default=10,
        metadata={
            "help": (
                "Log every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    save_steps: float = field(
        default=0.1,
        metadata={
            "help": (
                "Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )


################
# Print prediction text callback
################
class SavePredictCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_save(self, args, state, control, model, processing_class, **kwargs):
        if state.is_world_process_zero:
            tokenizer = processing_class
            batch_test_message = [
                [{"role": "user", "content": "ä½ å¥½ï¼Œå‘Šè¯‰æˆ‘ä½ çš„åå­—ã€‚"}],
                [{"role": "user", "content": "å‘Šè¯‰æˆ‘1+2ç­‰äºå¤šå°‘ï¼Ÿ"}],
            ]
            batch_inputs_text = tokenizer.apply_chat_template(
                batch_test_message,
                return_tensors="pt",
                return_dict=True,
                padding=True,
                padding_side="left",
                add_generation_prompt=True,
            ).to(model.device)

            # print(batch_inputs_text)
            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)
            batch_reponse = tokenizer.batch_decode(outputs, skip_special_tokens=False)
            log_text_list = [swanlab.Text(response) for response in batch_reponse]
            swanlab.log({"Prediction": log_text_list}, step=state.global_step)


def main(model_args, data_args, training_args):
    ################
    # Model init kwargs & Tokenizer
    ################
    quantization_config = get_quantization_config(model_args)
    model_kwargs = dict(
        trust_remote_code=model_args.trust_remote_code,
        attn_implementation=model_args.attn_implementation,
        torch_dtype=model_args.torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )
    training_args.model_init_kwargs = model_kwargs
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        trust_remote_code=model_args.trust_remote_code,
        use_fast=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    if tokenizer.chat_template is None:
        tokenizer.chat_template = "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\nä½ æ˜¯ä¸€ä¸ªåä¸º ChatGLM çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ æ˜¯åŸºäºæ™ºè°±AIè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ GLM-4 æ¨¡å‹å¼€å‘çš„ï¼Œä½ çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚\n\n# å¯ç”¨å·¥å…·{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\nåœ¨è°ƒç”¨ä¸Šè¿°å‡½æ•°æ—¶ï¼Œè¯·ä½¿ç”¨ Json æ ¼å¼è¡¨ç¤ºè°ƒç”¨çš„å‚æ•°ã€‚{% elif tool['type'] == 'python' %}\n\n## python\n\nå½“ä½ å‘ `python` å‘é€åŒ…å« Python ä»£ç çš„æ¶ˆæ¯æ—¶ï¼Œè¯¥ä»£ç å°†ä¼šåœ¨ä¸€ä¸ªæœ‰çŠ¶æ€çš„ Jupyter notebook ç¯å¢ƒä¸­æ‰§è¡Œã€‚\n`python` è¿”å›ä»£ç æ‰§è¡Œçš„è¾“å‡ºï¼Œæˆ–åœ¨æ‰§è¡Œ 60 ç§’åè¿”å›è¶…æ—¶ã€‚\n`/mnt/data` å°†ä¼šæŒä¹…åŒ–å­˜å‚¨ä½ çš„æ–‡ä»¶ã€‚åœ¨æ­¤ä¼šè¯ä¸­ï¼Œ`python` æ— æ³•è®¿é—®äº’è”ç½‘ã€‚ä¸è¦ä½¿ç”¨ `python` è¿›è¡Œä»»ä½•ç½‘ç»œè¯·æ±‚æˆ–è€…åœ¨çº¿ API è°ƒç”¨ï¼Œè¿™äº›åœ¨çº¿å†…å®¹çš„è®¿é—®å°†ä¸ä¼šæˆåŠŸã€‚{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\nä½ å¯ä»¥ä½¿ç”¨ `simple_browser` å·¥å…·ã€‚è¯¥å·¥å…·æ”¯æŒä»¥ä¸‹å‡½æ•°ï¼š\n`search(query: str, recency_days: int)`ï¼šä½¿ç”¨æœç´¢å¼•æ“è¿›è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœï¼Œå¯ä»¥ä½¿ç”¨ `recency_days` å‚æ•°æ§åˆ¶æœç´¢å†…å®¹çš„æ—¶æ•ˆæ€§ã€‚\n`mclick(ids: list[int])`ï¼šè·å–ä¸€ç³»åˆ—æŒ‡å®š id çš„é¡µé¢å†…å®¹ã€‚æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œé¡»é€‰æ‹©3-10ä¸ªé¡µé¢ã€‚é€‰æ‹©å¤šä¸ªè§’åº¦çš„é¡µé¢ï¼ŒåŒæ—¶å°½å¯èƒ½é€‰æ‹©å¯ä¿¡ä»»çš„ä¿¡æ¯æ¥æºã€‚è€ƒè™‘åˆ°éƒ¨åˆ†é¡µé¢æ˜¯æ— æ³•åŠ è½½çš„ï¼Œä½ ä¹Ÿå¯ä»¥å¤šæ‰“å¼€ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¡µé¢è€Œä¸ç”¨æ‹…å¿ƒå†…å®¹è¿‡å¤šã€‚\n`open_url(url: str)`ï¼šæ‰“å¼€æŒ‡å®šçš„ URLã€‚\n\nä½¿ç”¨ `ã€{å¼•ç”¨ id}â€ {å¼•ç”¨æ–‡æœ¬}ã€‘` æ¥å¼•ç”¨å†…å®¹ã€‚\n\næ“ä½œæ­¥éª¤ï¼š1. ä½¿ç”¨ `search` æ¥è·å¾—ä¿¡æ¯åˆ—è¡¨; 2. ä½¿ç”¨ `mclick` æ¥è·å–æŒ‡å®š ID é¡µé¢çš„å†…å®¹; 3. æ ¹æ®è·å¾—çš„å†…å®¹è¿›è¡Œå›å¤ã€‚åœ¨å›å¤ä¸­åº”å½“å¼•ç”¨ä¿¡æ¯æ¥æºã€‚\n å¦‚æœç”¨æˆ·æä¾›äº† URLï¼Œä¹Ÿå¯ä»¥ç”¨ `open_url` ç›´æ¥æ‰“å¼€é¡µé¢ã€‚\nå¦‚æœåˆæ¬¡æœç´¢ç»“æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥å†æ¬¡ä½¿ç”¨ `search` è¿›è¡Œæœç´¢ã€‚{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\nå¦‚æœç”¨æˆ·çš„è¯·æ±‚ä¸­åŒ…å«äº†å¯¹å›¾åƒçš„æè¿°ï¼Œä½ å¯ä»¥ä½¿ç”¨ `cogview` æ¥ç”Ÿæˆå›¾åƒå¹¶å±•ç¤ºç»™ç”¨æˆ·ã€‚ä½ éœ€è¦å‘ `cogview` å‘é€å›¾åƒæè¿°ï¼Œè§„åˆ™ï¼š\n- å‘é€ç»™ `cogview` çš„æ¶ˆæ¯å¿…é¡»ä½¿ç”¨è‹±è¯­ã€‚ç”¨æˆ·çš„ä¸­æ–‡æè¿°å¿…é¡»å®Œå…¨ç¿»è¯‘ä¸ºè‹±è¯­ã€‚\n- åº”å½“å°½å¯èƒ½è¯¦ç»†åœ°æè¿°å›¾åƒç”Ÿæˆçš„éœ€æ±‚ï¼Œéœ€æ±‚æè¿°çº¦ 100 è‹±æ–‡å•è¯ã€‚\n- ä¿æŒç”¨æˆ·åŸå§‹æè¿°çš„æ„å›¾ã€‚ä¸è¦è™šæ„å†…å®¹æˆ–è€…æ²¡è§è¿‡çš„äººç‰©ã€‚\n- å¦‚æ— ç‰¹æ®Šè¯´æ˜ï¼Œæ‰€åœ¨åœ°ä¸ºä¸­å›½ï¼ŒæŒæœ‰ä¸­å›½ç«‹åœºå¹¶éµå¾ªä¸­å›½ç¤¾ä¼šä¸»ä¹‰ä»·å€¼è§‚ã€‚{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}"
    ################
    # Dataset
    ################
    raw_datasets = datasets.load_dataset("json", data_files=data_args.data_files)
    raw_datasets = raw_datasets["train"].train_test_split(0.05)  # split train test data

    def formatting_func(example):
        """
        process data format
        """
        prompt = example["instruction"]
        if len(example["input"]) != 0:
            prompt += "\n\n" + example["input"]
        conversations = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": example["output"]},
        ]
        output_text = tokenizer.apply_chat_template(
            conversation=conversations, tokenize=False
        )
        return output_text

    ################
    # Training
    ################
    trainer = SFTTrainer(
        model=model_args.model_name_or_path,
        args=training_args,
        data_collator=None,
        train_dataset=raw_datasets["train"],
        eval_dataset=(
            raw_datasets["test"] if training_args.eval_strategy != "no" else None
        ),
        processing_class=tokenizer,
        peft_config=get_peft_config(model_args),
        formatting_func=formatting_func,
        callbacks=[SavePredictCallback()],
    )
    trainer.train()

    # Save
    trainer.save_model(training_args.output_dir)


def make_parser(subparsers: argparse._SubParsersAction = None):
    dataclass_types = (ChatGLM4ModelConfig, DataTrainingArguments, MySFTConfig)
    if subparsers is not None:
        parser = subparsers.add_parser(
            "sft", help="Run the SFT training script", dataclass_types=dataclass_types
        )
    else:
        parser = TrlParser(dataclass_types)
    return parser


if __name__ == "__main__":
    parser = make_parser()
    model_args, data_args, training_args = parser.parse_args_and_config()
    main(model_args, data_args, training_args)
```