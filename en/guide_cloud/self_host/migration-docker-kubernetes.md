# Migrating from Docker Version to K8S Version

![migration from docker to kubernetes](./kubernetes/migration.png)

This guide is intended for migrating data from the SwanLab Docker version to the **SwanLab Kubernetes (K8S)** version, and is *only* applicable to scenarios where external services are integrated into the **SwanLab Kubernetes (K8S)** version (see [Custom Basic Service Resources](/en/guide_cloud/self_host/kubernetes-deploy.md#_3-1-customizing-basic-service-resources)).

If you wish to migrate to managed services provided by cloud vendors or to self-hosted cloud-native high-availability services, this guide can serve as a reference. Meanwhile, please refer to the official migration documentation of the respective cloud vendors and cloud-native projects, and ensure that database names, table names, and object storage bucket names are correct during migration.

[[toc]]

-----

**This solution requires:**

1.  Migrate data first, then deploy services.
2.  You use the [Custom Basic Service Resources](/en/guide_cloud/self_host/kubernetes-deploy.md#_3-1-customizing-basic-service-resources) feature.
3.  You have a `busybox` image for performing migration tasks.

We pre-create storage volumes by packaging data, downloading it, and storing it in data volumes, and then mounting the storage volume resources.

-----

**Please identify the resources you need to migrate:**

1.  **PostgreSQL Single Instance**: Used to store SwanLab's core business data.
2.  **ClickHouse Single Instance**: Used to store metric data.
3.  **MinIO Single Instance**: Used to store media resources.
4.  **Redis Single Instance**: Cache service.
    *Note: SwanLab-House does not require data migration.*

:::warning
Before migrating, please ensure:

1.  Your Docker service has been stopped, or you need to ensure the migrated data is extracted based on a specific storage snapshot.
2.  You have located the data volume path mounted by the SwanLab Docker version. By default, it should be in the `docker/swanlab/data` directory of the [self-hosted](https://github.com/SwanHubX/self-hosted) project. If you have forgotten the storage path, you can use the `docker inspect` command to find the data volume mount location of the corresponding service container.
3.  You have found the `docker-compose.yaml` generated by SwanLab. This is mainly for migrating account passwords. If you have forgotten the location of the `docker-compose.yaml` file, you can still find the corresponding account password environment variables via `docker inspect`.
:::

For ease of description, subsequent docker-related commands are based on `self-hosted/docker/swanlab/`. Please adjust the corresponding paths according to your actual situation.

Additionally, the account passwords involved in this guide are for reference only; please adjust them according to your actual configuration.

Please clarify the storage locations of various basic service resources in the Docker version:

1.  PostgreSQL data is at `self-hosted/docker/swanlab/data/postgres`
2.  ClickHouse data is stored at `self-hosted/docker/swanlab/data/clickhouse`
3.  MinIO data is stored at `self-hosted/docker/swanlab/data/minio`
4.  Redis data is stored at `self-hosted/docker/swanlab/data/redis`

<img src="./kubernetes/datadir.png" alt="Data Directory" width=300>

<br>

-----

**Glossary:**

  - `self-hosted`: The deployed SwanLab Kubernetes cluster.

## 1\. Migrating Redis

The SwanLab Docker version uses a built-in Redis service to store core business data (such as experiment records, project configurations, etc.) by default. The connection string is fixed as `redis://default@redis:6379` (where `redis` is the service name within the Docker network, and `6379` is the default port). The core goal of migration is to completely migrate the data from this Redis instance to the Redis service of the Self-Hosted cluster, ensuring normal operation of all SwanLab functions.

:::warning
It is recommended to stop the SwanLab Docker version service before migration to avoid incomplete migration caused by data writing; also, back up the Redis data directory to prevent data loss in case of migration failure.
:::

### 1.1 Package and Upload Compressed File

This step needs to be executed on the server running the SwanLab Docker version. The core operation is to package the Redis data directory and then upload it to an object storage service accessible by the cluster (such as Aliyun OSS, AWS S3, etc.) to prepare for subsequent data replication within the cluster.

The Redis data of the SwanLab Docker version is mounted by default in the host's `data/redis` directory (this path is the host mount path specified in the Docker Compose configuration; if you have customized the mount directory, please replace it with the actual path). You can verify the directory existence and data integrity via the following commands:

```bash
# Check Redis data directory structure
ls -l data/redis
# If files like dump.rdb exist in the directory, the data directory is correct
```

Use the `tar` command to package the Redis data directory, executing the command as follows:

```bash
tar -czvf redis-data.tar.gz -C data/redis .
```

After execution, a `redis-data.tar.gz` compressed package will be generated in the current directory.

Then, upload the packaged file to a network storage service accessible by all nodes in the cluster. Taking Aliyun OSS as an example, the link sample after uploading is:

```bash
https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/redis-data.tar.gz
```

### 1.2 Copy Data to Redis Storage Volume

This step is executed within the cluster via a Kubernetes Job resource. The core operation is to download the data package from object storage, unzip it, and copy it to the persistent volume claim (PVC) corresponding to the Redis service, achieving data migration from object storage to the cluster storage volume.

Create a Job configuration file named `redis-migrate.yaml`. A reference sample is as follows:

```yaml {20}
apiVersion: batch/v1
kind: Job
metadata:
  name: redis-migrate
  labels:
    swanlab: redis
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: redis-migrate
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: redis-volume
              mountPath: /data
          env:
            - name: FILE_URL
              value: "https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/redis-data.tar.gz"
          command:
            - /bin/sh
            - -c
            - |
              wget $FILE_URL -O /tmp/redis-data.tar.gz
              tar -xzvf /tmp/redis-data.tar.gz -C /data
      volumes:
        - name: redis-volume
          persistentVolumeClaim:
            claimName: redis-docker-pvc
```

Pay attention to the PVC name. After confirming it is correct, execute the following commands on the cluster control node to create and run the Job:

```bash
# Apply Job configuration file
kubectl apply -f redis-migrate.yaml
# Check Job execution status (RUNNING means executing, SUCCEEDED means successful execution)
kubectl get jobs -l swanlab=redis
# View the Pod logs corresponding to the Job to confirm the data download and decompression process
kubectl logs -l job-name=redis-migrate
```

### 1.3 Start Redis Service and Define Secret

This step requires deploying the Redis service (using the storage volume with migrated data) and creating a Secret to store the Redis connection string for the SwanLab Self-Hosted service to access.

Reference Redis deployment configuration is as follows:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-docker
  labels:
    app: redis-docker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-docker
  template:
    metadata:
      name: redis-docker
      labels:
        app: redis-docker
    spec:
      restartPolicy: Always
      containers:
        - name: redis
          image: redis-stack:7.4.0-v8
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 6379
              protocol: TCP
          volumeMounts:
            - mountPath: /data
              name: redis-volume
      volumes:
        - name: redis-volume
          persistentVolumeClaim:
            claimName: redis-docker-pvc

---

apiVersion: v1
kind: Service
metadata:
  name: redis-docker
spec:
  selector:
    app: redis-docker
  ports:
    - protocol: TCP
      port: 6379
      targetPort: 6379
  type: ClusterIP
```

In addition, to integrate with `self-hosted`, a Redis secret needs to be defined. The content is:

| `.data.<keys>` | Value | Description |
| --- | --- | --- |
| `url` | `redis://default@redis-docker:6379` | Redis connection string |

We name this secret `redis-docker`. You can connect to the Redis service via port forwarding. If you see several key-value pairs in the service, the migration is successful.

## 2\. Migrating PostgreSQL

  - Connection string: `postgresql://swanlab:1uuYNzZa4p@postgres-docker:5432/app?schema=public`

### 2.1 Package and Upload Compressed File

::: warning
Note regarding PG: its data directory is stored under `/var/lib/postgresql/data`, corresponding to the external data volume `/data/postgres/data`. However, PG does not allow any folders other than `data` in the `/var/lib/postgresql` directory. Therefore, for PG, the final compressed package should be the content of a `data` directory.
:::

Use the following command to package PG data:

```bash
tar -czvf postgres-data.tar.gz -C data/postgres/ .
```

Then upload it to object storage or any network storage service accessible by the cluster. In this example, we upload to Aliyun Object Storage. The link sample after uploading is:

```
https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/postgres-data.tar.gz
```

### 2.2 Copy Data to Storage Volume

Reference configuration is as follows:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: postgres-migrate
  labels:
    swanlab: postgres
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: postgres-migrate
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: postgres-volume
              mountPath: /data
          env:
            - name: FILE_URL
              value: "https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/postgres-data.tar.gz"
          command:
            - /bin/sh
            - -c
            - |
              wget $FILE_URL -O /tmp/postgres-data.tar.gz
              tar -xzvf /tmp/postgres-data.tar.gz -C /data
      volumes:
        - name: postgres-volume
          persistentVolumeClaim:
            claimName: postgres-docker-pvc
```

### 2.3 Define Secret and Start PG Service

| `.data.<key>` | Value | Description |
| --- | --- | --- |
| `database` | `app` | Database name |
| `username` | `swanlab` | Username |
| `password` | `1uuYNzZa4p` | Password |
| `port` | `5432` | Port |
| `primaryUrl` | `postgresql://swanlab:1uuYNzZa4p@postgres-docker:5432/app?schema=public` | Writable connection string |
| `replicaUrl` | `postgresql://swanlab:1uuYNzZa4p@postgres-docker:5432/app?schema=public` | Read-only connection string; since this example is a single-instance PG, read-only and writable connection strings share the same one. |

We name this secret `postgres-docker`.

Reference PG deployment configuration is as follows:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-docker
  labels:
    app: postgres-docker
spec:
  selector:
    matchLabels:
      app: postgres-docker
  strategy:
    type: Recreate
  template:
    metadata:
      name: postgres-docker
      labels:
        app: postgres-docker
    spec:
      restartPolicy: Always
      volumes:
        - name: postgres-volume
          persistentVolumeClaim:
            claimName: postgres-docker-pvc
      containers:
        - name: postgresql
          image: postgres:16.1
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 5432
              name: postgresql
          volumeMounts:
            # Do not mount directly to /var/lib/postgresql/data, because the lost+found directory on the network drive will cause postgres to fail starting
            - mountPath: /var/lib/postgresql
              name: postgres-volume
          env:
            - name: TZ
              value: "UTC"
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-docker
                  key: username

            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-docker
                  key: password

            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: postgres-docker
                  key: database

---

apiVersion: v1
kind: Service
metadata:
  name: postgres-docker
spec:
  selector:
    app: postgres-docker
  ports:
    - port: 5432
      name: postgresql
```

You can connect to the PG service via port forwarding. If you see that multiple tables have been generated in the service, the migration is successful.

## 3\. Migrating ClickHouse

### 3.1 Package and Upload Compressed File

  - Connection string: `tcp://swanlab:2jwnZiojEV@clickhouse-docker:9000/app`
  - HTTP Port: `8123`

Use the following command to package ClickHouse data:

```bash
tar -czvf clickhouse-data.tar.gz -C data/clickhouse/ .
```

Then upload it to object storage or any network storage service accessible by the cluster. In this example, we upload to Aliyun Object Storage. The link sample after uploading is:

```
https://xxxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/clickhouse-data.tar.gz
```

### 3.2 Copy Data to Storage Volume

Reference configuration is as follows:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: clickhouse-migrate
  labels:
    swanlab: clickhouse
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: clickhouse-migrate
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: clickhouse-volume
              mountPath: /data
          env:
            - name: FILE_URL
              value: "https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/clickhouse-data.tar.gz"
          command:
            - /bin/sh
            - -c
            - |
              wget $FILE_URL -O /tmp/clickhouse-data.tar.gz
              tar -xzvf /tmp/clickhouse-data.tar.gz -C /data
      volumes:
        - name: clickhouse-volume
          persistentVolumeClaim:
            claimName: clickhouse-docker-pvc
```

### 3.3 Define Secret and Start ClickHouse Service

| `.data.<key>` | Value | Description |
| --- | --- | --- |
| `database` | `app` | Database name |
| `username` | `swanlab` | Username |
| `password` | `2jwnZiojEV` | Password |
| `host` | `clickhouse-docker` | Host |
| `httpPort` | `8123` | HTTP Port |
| `tcpPort` | `9000` | TCP Port |

Reference ClickHouse deployment configuration is as follows:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clickhouse-docker
  labels:
    app: clickhouse-docker
spec:
  selector:
    matchLabels:
      app: clickhouse-docker
  template:
    metadata:
      name: clickhouse-docker
      labels:
        app: clickhouse-docker
    spec:
      volumes:
        - name: clickhouse-volume
          persistentVolumeClaim:
            claimName: clickhouse-docker-pvc
      containers:
        - name: clickhouse
          image: clickhouse-server:24.3
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /var/lib/clickhouse
              name: clickhouse-volume
          ports:
            - containerPort: 9000
              name: clickhouse-tcp
            - containerPort: 8123
              name: clickhouse-http
          env:
            - name: TZ
              value: "UTC"
            - name: CLICKHOUSE_USER
              valueFrom:
                secretKeyRef:
                  name: clickhouse-docker
                  key: username
            - name: CLICKHOUSE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: clickhouse-docker
                  key: password
            - name: CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT
              value: "1"
      restartPolicy: Always

---

apiVersion: v1
kind: Service
metadata:
  name: clickhouse-docker
spec:
  selector:
    app: clickhouse-docker
  ports:
    - port: 9000
      name: clickhouse-tcp
    - port: 8123
      name: clickhouse-http
```

You can connect to the ClickHouse service via port forwarding. If you see that multiple tables have been generated in the service, the migration is successful.

## 4\. Migrating MinIO

  - accessKey: `swanlab`
  - accessSecret: `qtllV4B9KZ`

### 4.1 Package and Upload Compressed File

Use the following command to package MinIO data:

```bash
tar -czvf minio-data.tar.gz -C data/minio/ .
```

Then upload it to object storage or any network storage service accessible by the cluster. In this example, we upload to Aliyun Object Storage. The link sample after uploading is:

```
https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/minio-data.tar.gz
```

### 4.2 Copy Data to Storage Volume

Reference configuration is as follows:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: minio-migrate
  labels:
    swanlab: minio
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: minio-migrate
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: minio-volume
              mountPath: /data
          env:
            - name: FILE_URL
              value: "https://xxx.oss-cn-beijing.aliyuncs.com/self-hosted/docker/minio-data.tar.gz"
          command:
            - /bin/sh
            - -c
            - |
              wget $FILE_URL -O /tmp/minio-data.tar.gz
              tar -xzvf /tmp/minio-data.tar.gz -C /data
      volumes:
        - name: minio-volume
          persistentVolumeClaim:
            claimName: minio-docker-pvc
```

### 4.3 Define Secret and Start MinIO Service

| `.data.<key>` | Value | Description |
| --- | --- | --- |
| `accessKey` | `swanlab` | Access Key |
| `secretKey` | `qtllV4B9KZ` | Secret Key |
| `endpoint` | `http://minio-docker:9000` | Endpoint |
| `privateBucket` | `swanlab-private` | Private bucket name |
| `publicBucket` | `swanlab-public` | Public bucket name |
| `region` | `local` | Region |

Reference MinIO deployment configuration is as follows:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio-docker
  labels:
    app: minio-docker
spec:
  selector:
    matchLabels:
      app: minio-docker
  template:
    metadata:
      name: minio-docker
      labels:
        app: minio-docker
    spec:
      volumes:
        - name: minio-volume
          persistentVolumeClaim:
            claimName: minio-docker-pvc
      restartPolicy: Always
      containers:
        - name: minio
          image: minio:RELEASE.2025-09-07T16-13-09Z
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /data
              name: minio-volume
          args:
            - "server"
            - "/data"
            - "--console-address"
            - ":9001"
          ports:
            - containerPort: 9000
              name: minio
            - containerPort: 9001
              name: minio-console
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio-docker
                  key: accessKey
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio-docker
                  key: secretKey
---


apiVersion: v1
kind: Service
metadata:
  name: minio-docker
spec:
  selector:
    app: minio-docker
  ports:
    - port: 9000
      name: minio
    - port: 9001
      name: minio-console
```

You can log in to the MinIO console to view the data in the `swanlab-private` and `swanlab-public` buckets. If data exists, the migration is successful.

### 4.4 Configure Routing Forwarding Rules

Your load balancer needs to configure routing forwarding rules for `/swanlab-private` and `/swanlab-public` to forward traffic to the deployed MinIO service, ensuring the frontend can access resources such as avatars and media charts.

## 5\. Deploying Service

Once you have completed the migration of the above 4 services, you can start deploying the SwanLab Kubernetes service.

> For basic operations of deploying Kubernetes services, see: [Deployment using Kubernetes](/en/guide_cloud/self_host/kubernetes-deploy.md).

You only need to modify the `integrations` section based on the original [values.yaml](https://github.com/SwanHubX/charts/blob/main/charts/self-hosted/values.yaml), setting `enabled` to `true`, and configuring `existingSecret` to the secret name of the corresponding service.

Reference values are as follows (please ensure your secrets are correct):

```yaml
integrations:
  postgres:
    enabled: true
    existingSecret: "postgres-docker"
  redis:
    enabled: true
    existingSecret: "redis-docker"
  clickhouse:
    enabled: true
    existingSecret: "clickhouse-docker"
  s3:
    enabled: true
    existingSecret: "minio-docker"
```

After completing the modifications, execute the following command to deploy the SwanLab Kubernetes service:

```bash
helm install swanlab-self-hosted swanlab/self-hosted
```

> After deployment, if you have already logged in to the **SwanLab Docker version** on your browser, and the domain name remains consistent before and after migration, you do not need to log in again.

For more detailed operations on Kubernetes deployment, please refer to: [Deployment using Kubernetes](/en/guide_cloud/self_host/kubernetes-deploy.md).